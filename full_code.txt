Directory Tree:
mobius-epigenomic-transformer/
│   ├── make_paper_figures.py
│   ├── .DS_Store
│   ├── requirements.txt
│   ├── Miniforge3-MacOSX-arm64.sh
│   ├── web_structure.txt
│   ├── README.md
│   ├── full_code.txt
│   ├── dashboard_config.json
│   ├── web/
│   │   ├── server.py
│   │   ├── index.html
│   │   ├── .DS_Store
│   │   ├── requirements.txt
│   │   ├── landing.html
│   │   ├── start.sh
│   │   ├── css/
│   │   │   ├── styles.css
│   │   ├── js/
│   │   │   ├── main.js
│   │   │   ├── connection.js
│   ├── __pycache__/ [EXCLUDED]
│   ├── docs/ [EXCLUDED]
│   ├── processed_data/ [EXCLUDED]
│   ├── results/ [EXCLUDED]
│   ├── .venv/ [EXCLUDED]
│   ├── .webenv/ [EXCLUDED]
│   ├── .git/ [EXCLUDED]
│   ├── data/
│   ├── helpers/
│   │   ├── summary_of_files.txt
│   │   ├── show_csv.py
│   ├── wandb/ [EXCLUDED]
│   ├── src/
│   │   ├── .DS_Store
│   │   ├── original/
│   │   │   ├── .DS_Store
│   │   │   ├── run_pipeline_master.R
│   │   │   ├── analysis/
│   │   │   │   ├── pca.R
│   │   │   │   ├── annotate_dmps.R
│   │   │   ├── other/
│   │   │   │   ├── automate_pipeline.py
│   │   │   │   ├── new_test.py
│   │   │   │   ├── .DS_Store
│   │   │   │   ├── epigenomic_animation.py
│   │   │   │   ├── interactive_dashboard.py
│   │   │   │   ├── upload_hf.py
│   │   │   │   ├── fractal.py
│   │   │   │   ├── check_pipeline_outputs.R
│   │   │   │   ├── random_forest_hyperparam_search.py
│   │   │   │   ├── shap_analysis.py
│   │   │   │   ├── gnn_integration.py
│   │   │   │   ├── snapshot.R
│   │   │   │   ├── __pycache__/ [EXCLUDED]
│   │   │   ├── exploring/
│   │   │   │   ├── process_idat.R
│   │   │   │   ├── view_merged.R
│   │   │   │   ├── dmap.R
│   │   │   ├── steps/
│   │   │   │   ├── 02_differential_methylation_analysis.R
│   │   │   │   ├── 14_visualize_results.py
│   │   │   │   ├── 05_preprocessing.py
│   │   │   │   ├── 12_evaluate_results.py
│   │   │   │   ├── 11_report_shared_distinct_dmps.R
│   │   │   │   ├── 06_feature_engineer.py
│   │   │   │   ├── 01_unify_IDATs_And_Transpose.R
│   │   │   │   ├── transformer_classifier.py
│   │   │   │   ├── 03_verify.R
│   │   │   │   ├── 07_ensemble_ml.py
│   │   │   │   ├── 08_prepare_final_data.R
│   │   │   │   ├── 04_qa.R
│   │   │   │   ├── 10_baseline_classification.py
│   │   │   │   ├── 09_debug_feature_selection.R
│   │   │   │   ├── 09_feature_selection.R
│   │   │   │   ├── 13_summarize_findings.R
│   │   │   │   ├── __pycache__/ [EXCLUDED]
│   │   │   │   ├── 450k/
│   │   │   │   │   ├── 450k_transformer.py
│   │   │   │   │   ├── extract_quarter.py
│   │   │   │   │   ├── view.py
│   │   │   │   │   ├── __pycache__/ [EXCLUDED]
│   │   │   ├── visuals/
│   │   │   │   ├── confusion_matrix.png
│   │   │   │   ├── volcano_plot.png
│   │   │   │   ├── network_diagram.png
│   │   │   │   ├── violin_feature.png
│   │   │   │   ├── tsne_plot_p20.png
│   │   │   │   ├── condition_count.png
│   │   │   │   ├── pca_plot.png
│   │   │   │   ├── pairplot.png
│   │   │   │   ├── tsne_plot.png
│   │   │   │   ├── hierarchical_dendrogram.png
│   │   │   │   ├── pairplot_subset.png
│   │   │   │   ├── scatter_cpg.png
│   │   │   │   ├── pca_biplot.png
│   │   │   │   ├── tsne_plot_p10.png
│   │   │   │   ├── tsne_3d_plot.png
│   │   │   │   ├── multi_panel_summary.png
│   │   │   │   ├── pca_scree_plot.png
│   │   │   │   ├── pca_3d_plot.png
│   │   │   │   ├── rf_feature_importances.png
│   │   │   │   ├── kde_plot.png
│   │   │   ├── create_controls/
│   │   │   │   ├── control_gsms.txt
│   │   │   │   ├── filter.py
│   │   │   │   ├── delete_patients.py




# ======================
# File: make_paper_figures.py
# ======================

#!/usr/bin/env python3
"""
EPIGENETICS ANALYSIS + VISUALS FOR PAPER (MODIFIED)
==================================================
This script reads your 'filtered_biomarker_matrix.csv' from EpiMECoV/processed_data,
performs:
  • Data inspection
  • Baseline comparisons (Logistic Regression, RandomForest)
  • Transformer-based classification
  • Robust cross-validation, hold-out set
  • Sensitivity analysis (RF n_estimators)

Figures & CSV outputs are placed in EpiMECoV/results/.

Usage:
  python make_paper_figures.py

Modified to include:
  1) A hold-out set (20%) for final evaluation,
  2) Stratified K-Fold CV on the training portion,
  3) Logistic Regression baseline,
  4) Sensitivity Analysis example,
  5) Extended clarity on diagnostic prints.
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.decomposition import PCA, TruncatedSVD

# ---------------------------------------------------------
# Resolve repository root dynamically to avoid hard-coded paths
# ---------------------------------------------------------
REPO_ROOT = os.path.dirname(os.path.abspath(__file__))
RESULTS_DIR = os.path.join(REPO_ROOT, "results")
PROCESSED_DATA_DIR = os.path.join(REPO_ROOT, "processed_data")
BETA_FILE = os.path.join(PROCESSED_DATA_DIR, "filtered_biomarker_matrix.csv")

# Optional DMP results (if they exist):
DMP_ME_CTRL = os.path.join(RESULTS_DIR, "DMP_ME_vs_Control.csv")
DMP_LC_CTRL = os.path.join(RESULTS_DIR, "DMP_LC_vs_Control.csv")
DMP_ME_LC   = os.path.join(RESULTS_DIR, "DMP_ME_vs_LC.csv")

# ---------------------------------------------------------
# Helper Classes / Functions
# ---------------------------------------------------------
class DnamDataset(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.int64)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class TransformerClassifier(nn.Module):
    """Lightweight transformer baseline classifier used for figure generation."""
    def __init__(self, seq_len=128, num_heads=4, ff_dim=256, num_classes=3, dropout=0.1):
        super(TransformerClassifier, self).__init__()
        self.embedding = nn.Linear(seq_len, seq_len)
        self.pos_encoding = nn.Parameter(torch.randn(1, seq_len, seq_len))

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=seq_len,
            nhead=num_heads,
            dim_feedforward=ff_dim,
            dropout=dropout,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.classifier = nn.Linear(seq_len, num_classes)

    def forward(self, x):
        # x shape: [batch_size, seq_len]
        x = x.unsqueeze(-1)          # [batch_size, seq_len, 1]
        x = x.transpose(1, 2)        # [batch_size, 1, seq_len]
        seq_len = x.size(-1)

        # Simple positional encoding
        pe = self.pos_encoding[:, :seq_len, :].transpose(1,2)
        x = x + pe[..., :1]

        enc = self.transformer_encoder(x)
        enc = enc.mean(dim=-1)  # global avg pooling
        out = self.classifier(enc)
        return out

def plot_confusion_matrix(cm, classes, out_path, title="Confusion Matrix"):
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
                xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()

# ---------------------------------------------------------
# RandomForest sensitivity analysis utility
# ---------------------------------------------------------
def random_forest_sensitivity_analysis(X_train, y_train, X_valid, y_valid):
    """Evaluate macro-F1 across different n_estimators settings for RandomForest."""
    results = []
    n_estimators_list = [50, 100, 150, 200, 300, 500]
    for ne in n_estimators_list:
        rf = RandomForestClassifier(n_estimators=ne, random_state=42)
        rf.fit(X_train, y_train)
        preds = rf.predict(X_valid)
        f1 = f1_score(y_valid, preds, average='macro')
        results.append((ne, f1))

    # Plot or log the results
    fig, ax = plt.subplots(figsize=(6,4))
    n_list = [r[0] for r in results]
    f1_list = [r[1] for r in results]
    ax.plot(n_list, f1_list, marker='o')
    ax.set_title("Sensitivity: #Estimators vs. Macro-F1")
    ax.set_xlabel("#Estimators")
    ax.set_ylabel("Macro-F1")
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, "rf_sensitivity_analysis.png"))
    plt.close()

    # Print the table
    for (ne, score) in results:
        print(f"  [RF sensitivity] n_estimators={ne}, Macro-F1={score:.3f}")

# ---------------------------------------------------------
# Main Script
# ---------------------------------------------------------
def main():
    # 1) LOAD & INSPECT
    if not os.path.exists(BETA_FILE):
        raise FileNotFoundError(f"Cannot find {BETA_FILE}; please confirm path.")
    df = pd.read_csv(BETA_FILE, index_col=0)
    if 'Condition' not in df.columns:
        raise ValueError("No 'Condition' column in the CSV. Cannot proceed.")
    print("Loaded data with shape:", df.shape)
    print("Unique conditions BEFORE fix:", df['Condition'].unique())

    # Example fix merging 'Noel ME' into 'ME' if relevant:
    df.loc[df['Condition'] == "Noel ME", "Condition"] = "ME"
    print("Unique conditions AFTER fix:", df['Condition'].unique())
    print("Head:\n", df.head())

    # Convert condition => numeric
    cond_map = {c: i for i, c in enumerate(sorted(df['Condition'].unique()))}
    y = df['Condition'].map(cond_map).values
    X = df.drop(columns=['Condition']).values

    class_labels = sorted(cond_map.keys())
    print("Label distribution (by numeric code):")
    for k in np.unique(y):
        print(f"  Class {k} => {np.sum(y==k)} samples  (Condition={class_labels[k]})")

    # 2) Simple Summaries + PCA
    summary_cols = min(X.shape[1], 10)
    summary_stats = pd.DataFrame(X[:, :summary_cols]).describe()
    summary_stats.to_csv(os.path.join(RESULTS_DIR, "summary_stats_sampleFeatures.csv"))
    print("Saved sample stats to summary_stats_sampleFeatures.csv")

    # Quick PCA for visualization
    pca = PCA(n_components=2)
    max_cols_for_pca = 2000
    X_sub = X if X.shape[1] <= max_cols_for_pca else X[:, :max_cols_for_pca]
    X_pca = pca.fit_transform(X_sub)
    plt.figure(figsize=(7,5))
    scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis', alpha=0.7)
    plt.title("PCA (Top 2 Components)")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    cbar = plt.colorbar(scatter)
    cbar.set_label("Class Label")
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, "pca_plot.png"))
    plt.close()

    # 3) SPLIT into HOLD-OUT (for final testing) + everything else
    X_trainval, X_holdout, y_trainval, y_holdout = train_test_split(
        X, y, test_size=0.20, random_state=42, stratify=y
    )
    print(f"Train/Val shape => {X_trainval.shape}, Hold-Out => {X_holdout.shape}")

    # 4) BASELINE COMPARISONS: (A) LOGISTIC REGRESSION, (B) RANDOM FOREST, (C) XGBoost
    #    Use STRATIFIED 10-FOLD on the training portion only (per paper)
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    # ---- (A) Logistic Regression
    log_f1_scores = []
    for fold_i, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):
        X_tr, X_va = X_trainval[tr_idx], X_trainval[va_idx]
        y_tr, y_va = y_trainval[tr_idx], y_trainval[va_idx]
        log_clf = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto', random_state=42)
        log_clf.fit(X_tr, y_tr)
        va_preds = log_clf.predict(X_va)
        f1_v = f1_score(y_va, va_preds, average='macro')
        log_f1_scores.append(f1_v)
        print(f"[LogReg fold {fold_i}] Macro-F1={f1_v:.3f}")
    avg_log_f1 = np.mean(log_f1_scores)
    print(f"Logistic Regression CV Macro-F1 => {avg_log_f1:.3f}")
    
    # ---- (B) Random Forest
    rf_f1_scores = []
    for fold_i, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):
        X_tr, X_va = X_trainval[tr_idx], X_trainval[va_idx]
        y_tr, y_va = y_trainval[tr_idx], y_trainval[va_idx]
        rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)
        rf_clf.fit(X_tr, y_tr)
        va_preds = rf_clf.predict(X_va)
        f1_v = f1_score(y_va, va_preds, average='macro')
        rf_f1_scores.append(f1_v)
        print(f"[RF fold {fold_i}] Macro-F1={f1_v:.3f}")
    avg_rf_f1 = np.mean(rf_f1_scores)
    print(f"Random Forest CV Macro-F1 => {avg_rf_f1:.3f}")

    # ---- (C) XGBoost (if available)
    try:
        from xgboost import XGBClassifier
        xgb_f1_scores = []
        for fold_i, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):
            X_tr, X_va = X_trainval[tr_idx], X_trainval[va_idx]
            y_tr, y_va = y_trainval[tr_idx], y_trainval[va_idx]
            xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
            xgb.fit(X_tr, y_tr)
            va_preds = xgb.predict(X_va)
            f1_v = f1_score(y_va, va_preds, average='macro')
            xgb_f1_scores.append(f1_v)
            print(f"[XGB fold {fold_i}] Macro-F1={f1_v:.3f}")
        avg_xgb_f1 = np.mean(xgb_f1_scores)
        print(f"XGBoost CV Macro-F1 => {avg_xgb_f1:.3f}")
    except Exception as e:
        print("[WARN] XGBoost not available:", e)

    # Train final LogReg on entire trainval, evaluate on hold-out
    final_log = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto', random_state=42)
    final_log.fit(X_trainval, y_trainval)
    hold_preds = final_log.predict(X_holdout)
    hold_f1 = f1_score(y_holdout, hold_preds, average='macro')
    try:
        hold_auc = roc_auc_score(pd.get_dummies(y_holdout),
                                 final_log.predict_proba(X_holdout),
                                 average='macro', multi_class='ovr')
    except:
        hold_auc = np.nan
    print(f"[LogReg] HOLD-OUT => Macro-F1={hold_f1:.3f}, AUC={hold_auc:.3f}")
    cm_log = confusion_matrix(y_holdout, hold_preds)
    plot_confusion_matrix(cm_log, class_labels,
                          os.path.join(RESULTS_DIR, "logreg_holdout_confusion.png"),
                          title="Logistic Regression (Hold-Out)")

    # ---- (B) Random Forest (100 trees, tuned depth optional)
    rf_f1_scores = []
    for fold_i, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):
        X_tr, X_va = X_trainval[tr_idx], X_trainval[va_idx]
        y_tr, y_va = y_trainval[tr_idx], y_trainval[va_idx]
        rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_clf.fit(X_tr, y_tr)
        va_preds = rf_clf.predict(X_va)
        f1_v = f1_score(y_va, va_preds, average='macro')
        rf_f1_scores.append(f1_v)
        print(f"[RF fold {fold_i}] Macro-F1={f1_v:.3f}")
    avg_rf_f1 = np.mean(rf_f1_scores)
    print(f"RandomForest CV Macro-F1 => {avg_rf_f1:.3f}")

    # Train final RF on entire trainval, evaluate on hold-out
    final_rf = RandomForestClassifier(n_estimators=100, random_state=42)
    final_rf.fit(X_trainval, y_trainval)
    hold_preds_rf = final_rf.predict(X_holdout)
    hold_f1_rf = f1_score(y_holdout, hold_preds_rf, average='macro')
    try:
        hold_auc_rf = roc_auc_score(pd.get_dummies(y_holdout),
                                    final_rf.predict_proba(X_holdout),
                                    average='macro', multi_class='ovr')
    except:
        hold_auc_rf = np.nan
    print(f"[RF] HOLD-OUT => Macro-F1={hold_f1_rf:.3f}, AUC={hold_auc_rf:.3f}")
    cm_rf = confusion_matrix(y_holdout, hold_preds_rf)
    plot_confusion_matrix(cm_rf, class_labels,
                          os.path.join(RESULTS_DIR, "rf_holdout_confusion.png"),
                          title="Random Forest (Hold-Out)")

    # (B.1) Optional Sensitivity Analysis for RandomForest
    # We'll do this using the train/val approach again
    # Here we do a single train/val split from the trainval set
    X_subtrain, X_subval, y_subtrain, y_subval = train_test_split(
        X_trainval, y_trainval, test_size=0.2, random_state=99, stratify=y_trainval
    )
    print("\n=== RandomForest Sensitivity Analysis ===")
    random_forest_sensitivity_analysis(X_subtrain, y_subtrain, X_subval, y_subval)

    # 5) Transformer baseline with Monte Carlo Dropout; train on trainval, eval on hold-out.
    from sklearn.decomposition import TruncatedSVD
    reduced_dim = 128
    # SVD to reduce dimensionality (if needed)
    if X_trainval.shape[1] > reduced_dim:
        svd = TruncatedSVD(n_components=reduced_dim, random_state=42)
        X_trainval_svd = svd.fit_transform(X_trainval)
        X_holdout_svd  = svd.transform(X_holdout)
        actual_seq_len = reduced_dim
    else:
        X_trainval_svd = X_trainval
        X_holdout_svd  = X_holdout
        actual_seq_len = X_trainval.shape[1]

    train_data = DnamDataset(X_trainval_svd, y_trainval)
    hold_data  = DnamDataset(X_holdout_svd,  y_holdout)
    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)
    hold_loader  = DataLoader(hold_data, batch_size=16, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_trans = TransformerClassifier(seq_len=actual_seq_len,
                                        num_classes=len(class_labels)).to(device)
    optimizer = optim.Adam(model_trans.parameters(), lr=5e-4)
    criterion = nn.CrossEntropyLoss()

    # Training
    epochs = 15
    for epoch in range(epochs):
        model_trans.train()
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            logits = model_trans(batch_x)
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(train_loader)
        print(f"[Transformer] Epoch {epoch+1}/{epochs}, Loss={avg_loss:.4f}")

    # MC-Dropout Inference
    model_trans.train()  # keep dropout in training mode
    mc_samples = 20
    hold_preds_list = []
    with torch.no_grad():
        for _ in range(mc_samples):
            tmp_preds = []
            for bx, by in hold_loader:
                bx = bx.to(device)
                out_logits = model_trans(bx)
                preds = torch.argmax(out_logits, dim=1).cpu().numpy()
                tmp_preds.append(preds)
            hold_preds_list.append(np.concatenate(tmp_preds))
    hold_preds_array = np.stack(hold_preds_list, axis=0)
    # majority vote
    final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=hold_preds_array)

    # Evaluate
    trans_f1 = f1_score(y_holdout, final_preds, average='macro')
    try:
        # approximate mean prob for AUC
        with torch.no_grad():
            mc_prob_sum = np.zeros((len(X_holdout_svd), len(class_labels)), dtype=np.float32)
            for _ in range(mc_samples):
                preds_prob = []
                start_idx = 0
                for bx, by in hold_loader:
                    bx = bx.to(device)
                    out_logits = model_trans(bx)
                    prob = nn.Softmax(dim=1)(out_logits).cpu().numpy()
                    batch_size = prob.shape[0]
                    mc_prob_sum[start_idx:start_idx+batch_size, :] += prob
                    start_idx += batch_size
            mc_prob_mean = mc_prob_sum / mc_samples
        trans_auc = roc_auc_score(pd.get_dummies(y_holdout), mc_prob_mean, average='macro', multi_class='ovr')
    except:
        trans_auc = np.nan
    print(f"[Transformer] HOLD-OUT => Macro-F1={trans_f1:.3f}, AUC={trans_auc:.3f}")
    cm_trans = confusion_matrix(y_holdout, final_preds)
    plot_confusion_matrix(cm_trans, class_labels,
                          os.path.join(RESULTS_DIR, "transformer_holdout_confusion.png"),
                          title="Transformer (Hold-Out)")

    # 6) If DMP results exist, optionally load them
    if os.path.exists(DMP_ME_CTRL):
        df_me_ctrl = pd.read_csv(DMP_ME_CTRL)
        print("Sample ME vs Control DMPs:\n", df_me_ctrl.head())
    if os.path.exists(DMP_LC_CTRL):
        df_lc_ctrl = pd.read_csv(DMP_LC_CTRL)
        print("Sample LC vs Control DMPs:\n", df_lc_ctrl.head())
    if os.path.exists(DMP_ME_LC):
        df_me_lc = pd.read_csv(DMP_ME_LC)
        print("Sample ME vs LC DMPs:\n", df_me_lc.head())

    print("All done. Results in EpiMECoV/results/ folder.")

if __name__ == "__main__":
    main()


# ======================
# File: Miniforge3-MacOSX-arm64.sh
# ======================


# Error reading file Miniforge3-MacOSX-arm64.sh: 'utf-8' codec can't decode byte 0xcf in position 19767: invalid continuation byte


# ======================
# File: web/server.py
# ======================

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import jwt
import os
import secrets
from functools import wraps
from datetime import datetime, timedelta

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# Configuration
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', secrets.token_hex(32))
app.config['JWT_EXPIRATION_HOURS'] = 24

# Store active connections and their tokens
active_connections = {}
api_keys = set()  # In production, this should be in a secure database

def generate_token():
    expiration = datetime.utcnow() + timedelta(hours=app.config['JWT_EXPIRATION_HOURS'])
    return jwt.encode(
        {'exp': expiration},
        app.config['SECRET_KEY'],
        algorithm='HS256'
    )

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None
        if 'Authorization' in request.headers:
            token = request.headers['Authorization'].split(' ')[1]
        
        if not token:
            return jsonify({'message': 'Token is missing'}), 401

        try:
            jwt.decode(token, app.config['SECRET_KEY'], algorithms=['HS256'])
        except:
            return jsonify({'message': 'Token is invalid'}), 401

        return f(*args, **kwargs)
    return decorated

# Serve static files
@app.route('/')
def serve_landing():
    # server.py is in the web directory; serve files from current directory
    return send_from_directory('.', 'landing.html')

@app.route('/dashboard')
def serve_dashboard():
    # Serve the main dashboard UI
    return send_from_directory('.', 'index.html')

@app.route('/<path:path>')
def serve_static(path):
    return send_from_directory('.', path)

# API Routes
@app.route('/api/auth', methods=['POST'])
def authenticate():
    data = request.json
    api_key = data.get('apiKey')
    
    if not api_key or api_key not in api_keys:
        return jsonify({'message': 'Invalid API key'}), 401
    
    token = generate_token()
    return jsonify({'token': token})

@app.route('/api/config', methods=['POST'])
def save_config():
    """Persist dashboard configuration for the R pipeline to consume."""
    try:
        config = request.json or {}
        # Stronger validation
        required = ['condition1', 'condition2', 'condition3']
        missing = [k for k in required if not config.get(k)]
        if missing:
            return jsonify({'message': f'Missing fields: {", ".join(missing)}'}), 400

        # Save for run_pipeline_master.R (reads dashboard_config.json in web_mode)
        import json
        with open('dashboard_config.json', 'w') as f:
            json.dump({
                'condition1': config['condition1'],
                'condition2': config['condition2'],
                'condition3': config['condition3'],
                'startStep': int(config.get('startStep', 0))
            }, f, indent=2)

        return jsonify({'message': 'Configuration saved'})
    except Exception as e:
        return jsonify({'message': f'Failed to save config: {e}'}), 500

@app.route('/api/pipeline/start', methods=['POST'])
def start_pipeline():
    """Start the R pipeline and stream progress to WebSocket clients."""
    try:
        # Run the pipeline in a subprocess
        import subprocess
        import threading
        import json

        payload = request.json or {}
        # Ensure config exists on disk for web_mode
        if not os.path.exists('dashboard_config.json'):
            with open('dashboard_config.json', 'w') as f:
                json.dump(payload, f, indent=2)

        cmd = ['Rscript', 'src/original/run_pipeline_master.R', str(payload.get('startStep', 0)), '--web_mode=TRUE']
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)

        def stream_output():
            try:
                for line in proc.stdout:  # type: ignore[attr-defined]
                    socketio.emit('output', {'type': 'output', 'message': line})
            finally:
                code = proc.wait()
                status = 'complete' if code == 0 else 'error'
                socketio.emit(status, {'type': status, 'message': f'Pipeline {status}', 'code': code})

        threading.Thread(target=stream_output, daemon=True).start()
        return jsonify({'message': 'Pipeline started'})
    except Exception as e:
        return jsonify({'message': f'Failed to start pipeline: {e}'}), 500

# WebSocket Events
@socketio.on('connect')
def handle_connect():
    print('Client connected')

@socketio.on('disconnect')
def handle_disconnect():
    print('Client disconnected')
    if request.sid in active_connections:
        del active_connections[request.sid]

@socketio.on('auth')
def handle_authentication(data):
    try:
        token = data.get('token')
        if token:
            jwt.decode(token, app.config['SECRET_KEY'], algorithms=['HS256'])
            active_connections[request.sid] = {'authenticated': True}
            emit('auth_success')
        else:
            emit('auth_failed', {'message': 'No token provided'})
    except jwt.ExpiredSignatureError:
        emit('auth_failed', {'message': 'Token expired'})
    except jwt.InvalidTokenError:
        emit('auth_failed', {'message': 'Invalid token'})

@socketio.on('pipeline_command')
def handle_pipeline_command(data):
    if not active_connections.get(request.sid, {}).get('authenticated'):
        emit('error', {'message': 'Not authenticated'})
        return
    
    command = data.get('command')
    # Handle pipeline commands here
    emit('command_response', {'status': 'success'})

def create_api_key():
    """Generate a new API key and add it to the set of valid keys"""
    api_key = secrets.token_urlsafe(32)
    api_keys.add(api_key)
    return api_key

if __name__ == '__main__':
    # Generate initial API key if none exist
    if not api_keys:
        initial_key = create_api_key()
        print(f"Initial API Key: {initial_key}")
    
    socketio.run(app, debug=True)

# ======================
# File: web/start.sh
# ======================

#!/bin/bash

# Function to check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Function to setup Python virtual environment
setup_venv() {
    if ! command_exists python3; then
        echo "Error: Python 3 is required but not installed."
        exit 1
    }

    if [ ! -d "venv" ]; then
        echo "Creating virtual environment..."
        python3 -m venv venv
    fi

    # Activate virtual environment
    source venv/bin/activate

    # Install requirements
    echo "Installing dependencies..."
    pip install -r requirements.txt
}

# Function to generate a secure secret key
generate_secret() {
    python3 -c 'import secrets; print(secrets.token_hex(32))'
}

# Parse command line arguments
MODE="local"
PORT=5000
HOST="127.0.0.1"

while [[ $# -gt 0 ]]; do
    case $1 in
        --remote)
            MODE="remote"
            shift
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Setup environment
setup_venv

# Generate secret key if not exists
if [ ! -f ".env" ]; then
    echo "Generating secret key..."
    echo "SECRET_KEY=$(generate_secret)" > .env
    echo "Environment file created with new secret key"
fi

# Start server based on mode
if [ "$MODE" = "remote" ]; then
    echo "Starting server in remote mode..."
    echo "Server will be accessible from: $HOST:$PORT"
    python server.py --host "$HOST" --port "$PORT"
else
    echo "Starting server in local mode..."
    echo "Server will be accessible at: http://localhost:$PORT"
    python server.py
fi

# ======================
# File: helpers/show_csv.py
# ======================

import os
import pandas as pd
import xml.etree.ElementTree as ET

try:
    import pyreadr
    PYREADR_AVAILABLE = True
except ImportError:
    PYREADR_AVAILABLE = False

root_folder = "/Volumes/T9/EpiMECoV/" 
output_file = "summary_of_files.txt"
script_dir = os.path.dirname(os.path.abspath(__file__))
output_path = os.path.join(script_dir, output_file)

def summarize_csv(file_path, f):
    df = pd.read_csv(file_path)
    f.write(f"  - Dimensions: {df.shape}\n")
    f.write(f"  - Headings: {list(df.columns)}\n")
    f.write(f"  - First 5 Rows:\n{df.head().to_string(index=False)}\n")

def summarize_rds(file_path, f):
    if not PYREADR_AVAILABLE:
        f.write("  - pyreadr not installed. Cannot parse RDS content.\n")
        return
    try:
        result = pyreadr.read_r(file_path)
        for key in result.keys():
            df = result[key]
            f.write(f"  - R object: {key}, shape = {df.shape}\n")
            f.write(f"    * Columns: {list(df.columns)}\n")
            if len(df) > 0:
                f.write(f"    * First 5:\n{df.head().to_string(index=False)}\n")
    except Exception as e:
        f.write(f"  - RDS Error: {e}\n")

def summarize_idat(file_path, f):
    size = os.path.getsize(file_path)
    f.write(f"  - File size: {size} bytes\n")
    with open(file_path, "rb") as idf:
        head = idf.read(8)
    f.write(f"  - First 8 bytes (hex): {head.hex()}\n")
    f.write("  - Likely an Illumina IDAT file.\n")

def summarize_xml(file_path, f):
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        c_count = len(list(root))
        f.write(f"  - Root tag: {root.tag}\n")
        f.write(f"  - # of children: {c_count}\n")
    except Exception as e:
        f.write(f"  - XML Parse Error: {e}\n")

def summarize_other(file_path, f):
    size = os.path.getsize(file_path)
    f.write(f"  - File size: {size} bytes\n")
    f.write("  - (Non-code data file: short summary only.)\n")

if __name__ == "__main__":
    all_files = []
    for subdir, _, files in os.walk(root_folder):
        for file in files:
            path = os.path.join(subdir, file)
            all_files.append(path)

    total_files = len(all_files)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("Summary of Files\n" + "="*60 + "\n\n")
        for idx, fp in enumerate(all_files, start=1):
            print(f"Processing file {idx} of {total_files}: {fp}")
            f.write(f"File {idx}/{total_files}: {fp}\n")
            ext = os.path.splitext(fp)[1].lower()
            try:
                if ext == ".csv":
                    summarize_csv(fp, f)
                elif ext == ".rds":
                    summarize_rds(fp, f)
                elif ext == ".idat":
                    summarize_idat(fp, f)
                elif ext == ".xml":
                    summarize_xml(fp, f)
                else:
                    summarize_other(fp, f)
            except Exception as e:
                f.write(f"  - Exception: {e}\n")
            f.write("\n" + "-"*40 + "\n\n")

    print(f"Summary saved to: {output_path}")
    with open(output_path, "r", encoding="utf-8") as f:
        content = f.read()
        tokens_approx = len(content.split())
    print(f"Approximate tokens in {output_file}: {tokens_approx}")

# ======================
# File: src/original/run_pipeline_master.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# run_pipeline_master.R (Updated)
#
# This script orchestrates the epigenetic biomarker discovery pipeline,
# now with an optional final step calling "SOTA_Transformer_Classifier.py"
# for mandatory self-supervised pretraining + classification.
#
# Additionally:
#  • If --web_mode=TRUE, we read from a config file that the new interactive
#    dashboard might generate, specifying the folder structure for the 3 conditions.
#
# Tradeoffs of using a Transformer:
#  - Transformers offer powerful sequence modeling but are computationally intensive
#    and may overfit on small datasets. To address these issues, we incorporated
#    self-supervised pretraining, dropout, ReZero connections, and LoRA modules.
#
# USAGE:
#   Rscript run_pipeline_master.R [startStep] [--web_mode=TRUE|FALSE]
###############################################################################

rm(list = ls())
cat("\014")  # Clear console

args <- commandArgs(trailingOnly = TRUE)
startStep <- 0
web_mode <- FALSE
if (length(args) > 0) {
  maybeNum <- suppressWarnings(as.numeric(args[1]))
  if (!is.na(maybeNum)) {
    startStep <- maybeNum
  }
  if (any(grepl("--web_mode=", args))) {
    wm_val <- gsub("--web_mode=", "", args[grepl("--web_mode=", args)])
    if (length(wm_val)==1 && tolower(wm_val)=="true") {
      web_mode <- TRUE
    }
  }
}

cat("=== Run Pipeline Master Script (Updated) ===\n")
cat("Will begin at step:", startStep, "\n")
cat("Web mode:", web_mode, "\n\n")

N <- 15  # total pipeline steps
library(progress)
pb <- progress_bar$new(format = "  Step :current/:total [:bar] :percent Elapsed: :elapsed ETA: :eta",
                       total = N, clear = FALSE, width = 60)

required_dirs <- c("processed_data","results")
for (dd in required_dirs) {
  if (!dir.exists(dd)) {
    dir.create(dd, recursive = TRUE)
  }
}

if (web_mode) {
  cat("[INFO] Web mode => reading config from 'dashboard_config.json' if it exists.\n")
  json_path <- "dashboard_config.json"
  if (file.exists(json_path)) {
    cat("Found dashboard_config.json => parsing.\n")
    conf_txt <- readLines(json_path)
    library(jsonlite)
    conf_list <- fromJSON(paste(conf_txt, collapse=""))
    Sys.setenv(PIPELINE_COND1=conf_list$condition1)
    Sys.setenv(PIPELINE_COND2=conf_list$condition2)
    Sys.setenv(PIPELINE_COND3=conf_list$condition3)
  } else {
    cat("[WARN] No dashboard_config.json found. Using default.\n")
  }
}

if (startStep <= 0) {
  cat("[STEP 0] IDAT Verification (optional)\n")
  # (Optional step)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 0\n")
}

if (startStep <= 1) {
  cat("\n[STEP 1] unify_IDATs_And_Transpose.R\n")
  system("Rscript src/original/steps/01_unify_IDATs_And_Transpose.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 1\n")
}

if (startStep <= 2) {
  cat("\n[STEP 2] 04_qa.R minimal QA\n")
  system("Rscript src/original/steps/04_qa.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 2\n")
}

if (startStep <= 3) {
  cat("\n[STEP 3] 02_differential_methylation_analysis.R\n")
  system("Rscript src/original/steps/02_differential_methylation_analysis.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 3\n")
}

if (startStep <= 4) {
  cat("\n[STEP 4] 08_prepare_final_data.R\n")
  system("Rscript src/original/steps/08_prepare_final_data.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 4\n")
}

if (startStep <= 5) {
  cat("\n[STEP 5] 09_feature_selection.R\n")
  system("Rscript src/original/steps/09_feature_selection.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 5\n")
}

if (startStep <= 6) {
  cat("\n[STEP 6] 05_preprocessing.py\n")
  cmd <- "python3 src/original/steps/05_preprocessing.py --csv ./processed_data/filtered_biomarker_matrix.csv --out ./processed_data/cleaned_data.csv --method auto"
  system(cmd, intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 6\n")
}

if (startStep <= 7) {
  cat("\n[STEP 7] 06_feature_engineer.py (VAE-based dimensionality reduction)\n")
  cmd <- "python3 src/original/steps/06_feature_engineer.py --csv ./processed_data/cleaned_data.csv --out ./processed_data/transformed_data.csv --latent_dim 64 --epochs 20 --batch_size 64 --lr 0.001 --dropout 0.1 --use_scale True"
  system(cmd, intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 7\n")
}

if (startStep <= 8) {
  cat("\n[STEP 8] 10_baseline_classification.py\n")
  system("python3 src/original/steps/10_baseline_classification.py", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 8\n")
}

if (startStep <= 9) {
  cat("\n[STEP 9] 11_transformer_classifier.py & 12_evaluate_results.py\n")
  system("python3 src/original/steps/transformer_classifier.py", intern = TRUE)
  system("python3 src/original/steps/12_evaluate_results.py", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 9\n")
}

if (startStep <= 10) {
  cat("\n[STEP 10] 13_summarize_findings.R\n")
  system("Rscript src/original/steps/13_summarize_findings.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 10\n")
}

if (startStep <= 10.5) {
  cat("\n[STEP 10.5] 11_report_shared_distinct_dmps.R\n")
  system("Rscript src/original/steps/11_report_shared_distinct_dmps.R", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 10.5\n")
}

if (startStep <= 11) {
  cat("\n[STEP 11] 14_visualize_results.py\n")
  system("python3 src/original/steps/14_visualize_results.py", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 11\n")
}

if (startStep <= 12) {
  cat("\n[STEP 12] Other advanced analyses (e.g., random forest hyperparam search, shap analysis)\n")
  # Optional steps
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 12\n")
}

if (startStep <= 13) {
  cat("\n[STEP 13] **New** SOTA_Transformer_Classifier (with convergence graphs) - mandatory self-supervised pretraining\n")
  system("python3 src/original/steps/transformer_classifier.py", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 13\n")
}

if (startStep <= 14) {
  cat("\n[STEP 14] 07_ensemble_ml.py (Ensemble Machine Learning)\n")
  system("python3 src/original/other/random_forest_hyperparam_search.py", intern = TRUE)
  try(pb$tick(), silent=TRUE)
} else {
  cat("Skipping Step 14\n")
}

cat("\n=== Pipeline complete! ===\n")

# ======================
# File: src/original/analysis/pca.R
# ======================

###############################################################################
# pca_analysis.R
#
# Purpose:
#   A more comprehensive PCA script for your epigenetic Beta matrix, leveraging
#   'irlba' for large-scale data. It:
#     1) Accepts user-defined arguments (via command line).
#     2) Loads your Beta matrix CSV (rows = samples, columns = probes, last col = Condition).
#     3) Performs PCA using 'irlba'.
#     4) Outputs a PCA plot (PC1 vs. PC2), coloring by Condition.
#     5) Saves numeric PCA results in CSV.
#     6) Saves the plot as a PNG file.
#
# Usage:
#   Rscript pca_analysis.R --csv=Merged_450K_EPIC_BetaValues_with_Condition_Updated.csv --pcs=10
###############################################################################

rm(list=ls())
cat("\014")

if (!requireNamespace("argparse", quietly = TRUE)) {
  install.packages("argparse", repos="https://cloud.r-project.org/")
}
library(argparse)

if (!requireNamespace("irlba", quietly = TRUE)) {
  install.packages("irlba", repos="https://cloud.r-project.org/")
}
if (!requireNamespace("ggplot2", quietly = TRUE)) {
  install.packages("ggplot2", repos="https://cloud.r-project.org/")
}
library(irlba)
library(ggplot2)

parser <- ArgumentParser(description="Perform PCA on a Beta matrix with optional subsampling.")
parser$add_argument("--csv", default="Merged_450K_EPIC_BetaValues_with_Condition_Updated.csv",
                    help="Path to the Beta matrix CSV.")
parser$add_argument("--pcs", type="integer", default=10,
                    help="Number of principal components (default=10).")
parser$add_argument("--subsample", type="integer", default=0,
                    help="Number of random probes to select. If 0, use all.")
parser$add_argument("--output", default="PCA_irlba",
                    help="Basename for output files.")
args <- parser$parse_args()

beta_csv   <- args$csv
n_pcs      <- args$pcs
subsample  <- args$subsample
output_tag <- args$output

cat("=== PCA Analysis ===\n")
cat("CSV file   =", beta_csv, "\n")
cat("PCs        =", n_pcs, "\n")
cat("Subsample  =", subsample, "\n")
cat("Output tag =", output_tag, "\n\n")

if (!file.exists(beta_csv)) {
  stop("CSV file not found: ", beta_csv)
}

beta_df <- read.csv(beta_csv, row.names=1, check.names=FALSE)
if (!("Condition" %in% colnames(beta_df))) {
  stop("No 'Condition' column found.")
}
Condition <- as.factor(beta_df$Condition)
beta_mat <- beta_df[, -ncol(beta_df), drop=FALSE]

cat("Samples =", nrow(beta_mat), "\nProbes  =", ncol(beta_mat), "\n\n")

if (subsample > 0 && subsample < ncol(beta_mat)) {
  set.seed(123)
  chosen_cols <- sample(colnames(beta_mat), subsample, replace=FALSE)
  beta_mat <- beta_mat[, chosen_cols, drop=FALSE]
  cat("Subsampled => new dimension =", nrow(beta_mat), "x", ncol(beta_mat), "\n")
}

beta_mat <- as.matrix(beta_mat)
cat("\nRunning PCA with IRLBA, #PCs =", n_pcs, "\n")
pca_res <- irlba::prcomp_irlba(beta_mat, n=n_pcs, center=TRUE, scale.=TRUE)

cat("PCA done. Summarizing variance:\n")
var_explained <- summary(pca_res)$importance[2, ]
print(var_explained)

pca_df <- data.frame(PC1=pca_res$x[,1], PC2=pca_res$x[,2], Condition=Condition)

pca_plot <- ggplot(pca_df, aes(x=PC1, y=PC2, color=Condition)) +
  geom_point(alpha=0.7, size=2) +
  theme_minimal() +
  labs(title=paste("PCA with IRLBA (", n_pcs, " PCs)", sep=""),
       x="PC1", y="PC2")

plot_file <- paste0(output_tag, "_PC1_PC2.png")
cat("Saving PCA plot =>", plot_file, "\n")
ggsave(filename=plot_file, plot=pca_plot, width=8, height=6, dpi=300)

pc_coords <- as.data.frame(pca_res$x)
pc_coords$Condition <- Condition
out_csv <- paste0(output_tag, "_PC_Coords.csv")
write.csv(pc_coords, file=out_csv, row.names=TRUE, quote=FALSE)
cat("Saved numeric PCA coords =>", out_csv, "\n")

cat("\n=== Script Complete ===\n")

# ======================
# File: src/original/analysis/annotate_dmps.R
# ======================

###############################################################################
# annotate_all_dmps.R
#
# Purpose:
#   Annotate your three DMP CSV files:
#     - DMP_ME_vs_Control.csv
#     - DMP_LC_vs_Control.csv
#     - DMP_ME_vs_LC.csv
#   Each file has row names for CpG IDs plus columns from limma's topTable.
#   We'll create a "CpG" column from row names, then run champ.annot().
#
# Usage:
#   Rscript annotate_all_dmps.R --array=450K  (or EPIC)
###############################################################################

rm(list=ls())
cat("\014")

if (!requireNamespace("argparse", quietly=TRUE)) {
  install.packages("argparse", repos="https://cloud.r-project.org/")
}
library(argparse)

# We need BiocManager for ChAMP + minfi if not installed
if (!requireNamespace("BiocManager", quietly=TRUE)) {
  install.packages("BiocManager", repos="https://cloud.r-project.org/")
}
if (!requireNamespace("ChAMP", quietly=TRUE)) {
  BiocManager::install("ChAMP")
}
if (!requireNamespace("minfi", quietly=TRUE)) {
  BiocManager::install("minfi")
}

library(ChAMP)
library(minfi)

parser <- ArgumentParser(description="Annotate the 3 main DMP CSV files.")
parser$add_argument("--array", default="450K",
                    help="Array type: 450K or EPIC.")
args <- parser$parse_args()
array_type <- args$array

cat("=== Bulk Annotation for 3 DMP Files ===\n")
cat("Array type =", array_type, "\n\n")

# We'll define the three expected DMP files
dmp_files <- c("DMP_ME_vs_Control.csv", "DMP_LC_vs_Control.csv", "DMP_ME_vs_LC.csv")

for (dmp_file in dmp_files) {
  if (!file.exists(dmp_file)) {
    cat("[WARNING] File not found:", dmp_file, "Skipping.\n")
    next
  }
  
  cat("\n--- Processing:", dmp_file, "---\n")
  dmp_data <- read.csv(dmp_file, row.names=1, stringsAsFactors=FALSE)
  cat("Read", dmp_file, "dimension:", nrow(dmp_data), "x", ncol(dmp_data), "\n")
  
  dmp_data$CpG <- rownames(dmp_data)
  
  # Minimal beta matrix scoped to CpG set (required by champ.annot)
  betaMatrix <- matrix(nrow=nrow(dmp_data), ncol=1)
  rownames(betaMatrix) <- dmp_data$CpG
  colnames(betaMatrix) <- "Sample_1"
  
  cat("Running champ.annot()...\n")
  annotRes <- champ.annot(beta=betaMatrix, arraytype=array_type)
  annotDF <- annotRes$bedFile
  
  if (!("probeID" %in% colnames(annotDF))) {
    annotDF$probeID <- rownames(annotDF)
  }
  
  final_annotated <- merge(dmp_data, annotDF, by.x="CpG", by.y="probeID", all.x=TRUE)
  
  outFile <- paste0("Annotated_", dmp_file)
  cat("Writing =>", outFile, "\nDimension:", nrow(final_annotated), "x", ncol(final_annotated), "\n")
  write.csv(final_annotated, outFile, row.names=FALSE, quote=FALSE)
}

cat("\n=== All annotation completed. Check Annotated_*.csv files. ===\n")


# ======================
# File: src/original/other/automate_pipeline.py
# ======================

#!/usr/bin/env python3
"""
epigenetic_transformer_analysis.py

Title: Epigenetic Profiling: A Transformer Approach

Steps:
  1) Loads data from "transformed_data.csv" (with columns AE_0..AE_n, Condition).
  2) Splits into train/val/test, trains a custom Transformer.
  3) Evaluates performance, confusion matrix, classification report.
  4) Stats-based feature analysis (ANOVA) => identifies differential features.

Outputs:
  - Plots + textual summary in "epigenetic_analysis_report.txt"
  - "epigenetic_stats.csv" with p-values and effect sizes
"""

import argparse
import math
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

CSV_PATH = "transformed_data.csv"
EPOCHS = 400
BATCH_SIZE = 32
LR = 1e-4
HID_DIM = 64
N_LAYERS = 2
N_HEADS = 4
DROP_PROB = 0.1
TEST_SIZE = 0.2
VAL_SIZE = 0.2
SEED = 42
REPORT_TXT = "epigenetic_analysis_report.txt"
REPORT_CSV = "epigenetic_stats.csv"


class SelfAttentionBlock(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x):
        residual = x
        out, _ = self.mha(x, x, x)
        out = self.dropout(out)
        out = self.norm(residual + out)
        return out


class FeedForwardBlock(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x):
        residual = x
        out = F.relu(self.linear1(x))
        out = self.dropout(out)
        out = self.linear2(out)
        out = self.dropout(out)
        out = self.norm(residual + out)
        return out


class TabularTransformer(nn.Module):
    def __init__(self, input_dim, d_model=64, n_layers=2, n_heads=4, d_ff=256, dropout=0.1, num_classes=3):
        super().__init__()
        self.input_dim = input_dim
        self.d_model = d_model
        self.embed = nn.Linear(1, d_model)
        self.pos_embed = nn.Parameter(torch.zeros(1, input_dim, d_model))

        self.blocks = nn.ModuleList([])
        for _ in range(n_layers):
            attn = SelfAttentionBlock(d_model, n_heads, dropout)
            ff = FeedForwardBlock(d_model, d_ff, dropout)
            self.blocks.append(nn.ModuleList([attn, ff]))

        self.pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, x):
        bsz, seq_len = x.shape
        x = x.unsqueeze(-1)
        x = self.embed(x)
        x = x + self.pos_embed[:, :seq_len, :]
        for attn, ff in self.blocks:
            x = attn(x)
            x = ff(x)
        x = x.permute(0, 2, 1)
        x = self.pool(x).squeeze(-1)
        out = self.classifier(x)
        return out


def main():
    np.random.seed(SEED)
    torch.manual_seed(SEED)

    if not os.path.exists(CSV_PATH):
        print(f"[ERROR] CSV not found: {CSV_PATH}")
        sys.exit(1)

    df = pd.read_csv(CSV_PATH)
    if "Condition" not in df.columns:
        print("[ERROR] Condition col missing.")
        sys.exit(1)

    feature_cols = [c for c in df.columns if c != "Condition"]
    X = df[feature_cols].values.astype(np.float32)
    conds = df["Condition"].values.astype(str)
    class_names = sorted(list(np.unique(conds)))
    c2i = {cname: i for i, cname in enumerate(class_names)}
    y = np.array([c2i[v] for v in conds], dtype=np.int64)

    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_trainval,
        y_trainval,
        test_size=VAL_SIZE,
        random_state=SEED,
        stratify=y_trainval,
    )

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_val_s = scaler.transform(X_val)
    X_test_s = scaler.transform(X_test)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TabularTransformer(
        input_dim=X_train_s.shape[1],
        d_model=HID_DIM,
        n_layers=N_LAYERS,
        n_heads=N_HEADS,
        d_ff=HID_DIM * 4,
        dropout=DROP_PROB,
        num_classes=len(class_names),
    ).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    def make_loader(Xf, yf):
        tX = torch.from_numpy(Xf)
        ty = torch.from_numpy(yf)
        ds = torch.utils.data.TensorDataset(tX, ty)
        return torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)

    train_loader = make_loader(X_train_s, y_train)
    val_loader   = make_loader(X_val_s, y_val)

    best_val_loss = float("inf")
    best_sd = None
    patience = 50
    no_improve = 0
    train_losses, val_losses = [], []

    for ep in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0.0
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            logits = model(bx)
            loss = criterion(logits, by)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train = total_loss / len(train_loader)

        model.eval()
        total_val = 0
        correct = 0
        total_samp = 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                v_log = model(bx)
                v_loss = criterion(v_log, by)
                total_val += v_loss.item()
                _, preds = torch.max(v_log, 1)
                correct += (preds == by).sum().item()
                total_samp += by.size(0)

        avg_val = total_val / len(val_loader)
        val_acc = correct / total_samp

        train_losses.append(avg_train)
        val_losses.append(avg_val)

        if ep % 50 == 0 or ep == 1 or ep == EPOCHS:
            print(f"[Epoch {ep}/{EPOCHS}] train_loss={avg_train:.4f}, val_loss={avg_val:.4f}, val_acc={val_acc:.4f}")

        if avg_val < best_val_loss:
            best_val_loss = avg_val
            best_sd = model.state_dict()
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"[EarlyStop] No improvement {patience} epochs => stop at ep={ep}")
                break

    if best_sd is not None:
        model.load_state_dict(best_sd)

    # Evaluate
    test_ds = make_loader(X_test_s, y_test)
    model.eval()
    preds_all = []
    truths_all = []
    for bx, by in test_ds:
        bx, by = bx.to(device), by.to(device)
        with torch.no_grad():
            out = model(bx)
            predicted = torch.argmax(out, dim=1)
            preds_all.append(predicted.cpu().numpy())
            truths_all.append(by.cpu().numpy())

    preds_all = np.concatenate(preds_all)
    truths_all = np.concatenate(truths_all)

    acc_test = np.mean(preds_all == truths_all)
    print(f"[TEST ACCURACY] => {acc_test:.4f}")

    cm = confusion_matrix(truths_all, preds_all)
    print("Confusion Matrix:\n", cm)

    cr = classification_report(truths_all, preds_all, target_names=class_names)
    print("Classification Report:\n", cr)

    # Confusion matrix plot
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=class_names, yticklabels=class_names)
    plt.title("Transformer Conf Matrix")
    plt.savefig("transformer_confusion_matrix.png", dpi=300)
    plt.close()

    # Stats-based feature diffs
    X_full_s = scaler.fit_transform(X)
    results = []
    for i, feat in enumerate(feature_cols):
        groups = []
        for cls_i in range(len(class_names)):
            groups.append(X_full_s[y==cls_i, i])
        try:
            fval, pval = stats.f_oneway(*groups)
            k = len(class_names)
            N = sum(len(g) for g in groups)
            eta_sq = (fval * (k - 1)) / ((fval * (k - 1)) + (N - k)) if fval > 0 else 0
            if pval < 1e-14:
                pval = 1e-14
            results.append((feat, pval, eta_sq))
        except:
            hval, pval = stats.kruskal(*groups)
            results.append((feat, pval, 0.0))

    res_sorted = sorted(results, key=lambda x: x[1])
    df_stats = pd.DataFrame(res_sorted, columns=["Feature","p_value","eta_sq_est"])
    df_stats.to_csv("epigenetic_stats.csv", index=False)
    print("[INFO] Wrote stats => epigenetic_stats.csv")

    p_thresh = 0.05
    differential = [r for r in res_sorted if r[1] < p_thresh]
    similar = [r for r in res_sorted if r[1] >= p_thresh]

    top_20 = differential[:20]
    if len(top_20) > 0:
        feats = [x[0] for x in top_20]
        pvals = [x[1] for x in top_20]
        plt.figure(figsize=(6,5))
        sns.barplot(x=-np.log10(pvals), y=feats, color="red")
        plt.title("Top 20 Diff Features")
        plt.savefig("top20_differential_features.png", dpi=300)
        plt.close()

    lines = []
    lines.append("=== Epigenetic Transformer Analysis ===")
    lines.append(f"Data shape => {X.shape}, #features={X.shape[1]}, classes={class_names}")
    lines.append(f"Transformer Test Accuracy => {acc_test:.4f}")
    lines.append("Confusion Matrix:\n" + str(cm))
    lines.append("Classification Report:\n" + cr)
    lines.append(f"\n=== Stats-based Feature Differences (ANOVA p<{p_thresh}) ===")
    lines.append(f"Found {len(differential)} features significantly differ.")
    lines.append(f"Found {len(similar)} features appear 'similar'.")
    lines.append("Sample top 10 diffs [feat, p_val, eta_sq]:")
    for x in differential[:10]:
        lines.append(f" - {x[0]} => p={x[1]:.4e}, eta^2~{x[2]:.3f}")

    lines.append("\n=== Potential Similarities ===")
    for x in similar[:10]:
        lines.append(f" - {x[0]} => p={x[1]:.4e}")

    with open("epigenetic_analysis_report.txt","w") as f:
        f.write("\n".join(lines))

    print("[DONE] Wrote epigenetic_analysis_report.txt\n")

if __name__ == "__main__":
    main()

# ======================
# File: src/original/other/new_test.py
# ======================

from manim import *

class SquareAnimation(Scene):
    def construct(self):
        square = Square(fill_opacity=1, fill_color=BLUE)
        self.play(Create(square))
        self.wait()
        self.play(square.animate.rotate(PI/4))
        self.wait()


# ======================
# File: src/original/other/epigenomic_animation.py
# ======================

#!/usr/bin/env python3
# A complete Manim script for explaining the Epigenomic Transformer

# First import manim
from manim import *

# Then import other libraries
import numpy as np
import os
import sys

# Set a consistent color scheme
METHYLATION_COLOR = "#3B4CC0"  # Blue for methylation
GENE_COLOR = "#B83A4D"  # Red for genes
TRANSFORMER_COLOR = "#38761D"  # Green for transformer components
ATTENTION_COLOR = "#F1C232"  # Yellow for attention
EXPERT_COLOR = "#8E7CC3"  # Purple for experts
BACKGROUND_COLOR = "#1E1E1E"  # Dark background

# Now config is available because it came from the manim import
config.background_color = BACKGROUND_COLOR
config.pixel_height = 1080
config.pixel_width = 1920
config.frame_height = 8
config.frame_width = 14.2

class Introduction(Scene):
    """Introduction to the Epigenomic Transformer concept."""
    
    def construct(self):
        # Title sequence
        title = Text("Epigenomic Transformers", font_size=72, color=WHITE)
        subtitle = Text("Mathematical Models for Disease Classification", font_size=42, color=WHITE)
        subtitle.next_to(title, DOWN, buff=0.5)
        
        self.play(Write(title, run_time=2))
        self.play(FadeIn(subtitle, shift=UP*0.5))
        self.wait(2)
        
        self.play(
            title.animate.scale(0.7).to_edge(UP),
            FadeOut(subtitle)
        )
        self.wait()
        
        # DNA visualization
        dna_helix = self.create_dna_helix()
        dna_helix.shift(LEFT * 3)
        
        self.play(Create(dna_helix, run_time=3))
        self.wait()
        
        # Introduce epigenetics concept
        epigenetics = Text("Epigenetics:", font_size=48, color=WHITE)
        epigenetics.to_edge(LEFT).shift(UP * 2)
        
        definition = Text(
            "How cells control gene expression\nwithout changing DNA sequence", 
            font_size=36, color=WHITE
        )
        definition.next_to(epigenetics, DOWN, aligned_edge=LEFT, buff=0.5)
        
        self.play(Write(epigenetics))
        self.play(Write(definition, run_time=2))
        self.wait()
        
        # Show methylation
        methylation_text = Text("DNA Methylation", font_size=42, color=METHYLATION_COLOR)
        methylation_text.next_to(definition, DOWN, aligned_edge=LEFT, buff=1)
        
        # Create methyl groups on the DNA
        methyl_groups = self.add_methyl_groups(dna_helix)
        
        self.play(Write(methylation_text))
        self.play(
            *[GrowFromCenter(methyl) for methyl in methyl_groups],
            run_time=2
        )
        self.wait()
        
        # Explain ME/CFS and Long COVID
        disease_text = Text("ME/CFS & Long COVID", font_size=42, color=GENE_COLOR)
        disease_text.to_edge(RIGHT).shift(UP * 2)
        
        disease_desc = Text(
            "Post-viral conditions with\nepigenetic alterations",
            font_size=36, color=WHITE
        )
        disease_desc.next_to(disease_text, DOWN, aligned_edge=LEFT, buff=0.5)
        
        self.play(Write(disease_text))
        self.play(Write(disease_desc, run_time=2))
        self.wait(2)
        
        # Transition to the mathematical challenge
        challenge = Text("The Mathematical Challenge", font_size=58, color=WHITE)
        challenge.center()
        
        self.play(
            FadeOut(dna_helix),
            FadeOut(methyl_groups),
            FadeOut(epigenetics),
            FadeOut(definition),
            FadeOut(methylation_text),
            FadeOut(disease_text),
            FadeOut(disease_desc),
            FadeOut(title),
            run_time=1.5
        )
        
        self.play(Write(challenge))
        self.wait(2)
        self.play(FadeOut(challenge))
    
    def create_dna_helix(self):
        """Create a stylized DNA double helix."""
        # Parameters for the helix
        t_range = np.linspace(0, 4*np.pi, 100)
        radius = 0.5
        frequency = 1
        
        # Create the two strands
        strand1_points = []
        strand2_points = []
        
        for t in t_range:
            x1 = radius * np.cos(frequency * t)
            y1 = t / 3  # Stretch along y-axis
            strand1_points.append([x1, y1, 0])
            
            x2 = radius * np.cos(frequency * t + np.pi)
            y2 = t / 3
            strand2_points.append([x2, y2, 0])
        
        # Create the strands as polygons
        strand1 = Polygon(*strand1_points, color=BLUE, stroke_width=3)
        strand2 = Polygon(*strand2_points, color=RED, stroke_width=3)
        
        # Create the base pairs (rungs)
        rungs = VGroup()
        for i in range(0, len(t_range), 10):
            p1 = strand1_points[i]
            p2 = strand2_points[i]
            line = Line(p1, p2, color=WHITE, stroke_width=2)
            rungs.add(line)
        
        return VGroup(strand1, strand2, rungs)
    
    def add_methyl_groups(self, dna_helix):
        """Add methyl groups to the DNA."""
        methyl_groups = VGroup()
        strand1 = dna_helix[0]
        
        # Add methyl groups at selected positions
        positions = [10, 30, 50, 70, 90]
        for i in positions:
            point = strand1.points[i]
            methyl = Circle(radius=0.15, color=METHYLATION_COLOR, fill_opacity=0.7)
            methyl.move_to(point)
            methyl_groups.add(methyl)
        
        return methyl_groups

class DataRepresentation(Scene):
    """Explaining how methylation data is represented mathematically."""
    
    def construct(self):
        # Title
        title = Text("Mathematical Representation of Methylation Data", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain what a CpG site is
        cpg = Text("CpG Site: Where cytosine can be methylated", font_size=36)
        cpg.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(cpg))
        self.wait()
        
        # Show the data format
        matrix_title = Text("Methylation Data Matrix:", font_size=36)
        matrix_title.next_to(cpg, DOWN, buff=0.8).to_edge(LEFT)
        
        self.play(Write(matrix_title))
        
        # Create a sample matrix of methylation values
        sample_matrix = self.create_methylation_matrix()
        sample_matrix.next_to(matrix_title, RIGHT, buff=0.5)
        
        self.play(Create(sample_matrix))
        self.wait()
        
        # Explain beta values
        beta_text = Text("β-values: 0 (unmethylated) to 1 (fully methylated)", font_size=32)
        beta_text.next_to(matrix_title, DOWN, aligned_edge=LEFT, buff=0.5)
        
        self.play(Write(beta_text))
        self.wait()
        
        # Highlight different patterns
        self.highlight_patterns(sample_matrix)
        
        # Show the mathematical challenge of dimensionality
        challenge = Text("Challenge: 1,280 CpG sites × 70 patients", font_size=36)
        challenge.next_to(beta_text, DOWN, buff=1)
        
        self.play(Write(challenge))
        self.wait()
        
        # Transition to mathematical embedding
        embedding_text = Text("We need to embed this high-dimensional data", font_size=36)
        embedding_text.next_to(challenge, DOWN, buff=0.5)
        
        self.play(Write(embedding_text))
        self.wait(2)
        
        # Transition to next scene
        self.play(
            FadeOut(title),
            FadeOut(cpg),
            FadeOut(matrix_title),
            FadeOut(sample_matrix),
            FadeOut(beta_text),
            FadeOut(challenge),
            FadeOut(embedding_text),
            run_time=1.5
        )
    
    def create_methylation_matrix(self):
        """Create a visualization of methylation data matrix."""
        # Create a 10x16 grid of squares to represent methylation values
        n_rows, n_cols = 5, 8
        square_size = 0.4
        
        grid = VGroup()
        values = np.random.rand(n_rows, n_cols)  # Random values between 0 and 1
        
        for i in range(n_rows):
            for j in range(n_cols):
                value = values[i, j]
                color = self.get_methylation_color(value)
                square = Square(side_length=square_size)
                square.set_fill(color, opacity=0.8)
                square.set_stroke(WHITE, width=1)
                square.move_to([j*square_size, -i*square_size, 0])
                grid.add(square)
                
                # Add value text
                value_text = Text(f"{value:.2f}", font_size=14)
                value_text.move_to(square.get_center())
                grid.add(value_text)
        
        # Add row and column labels
        row_labels = VGroup()
        for i in range(n_rows):
            label = Text(f"Patient {i+1}", font_size=18)
            label.next_to(grid[i*2*n_cols], LEFT, buff=0.3)
            row_labels.add(label)
        
        col_labels = VGroup()
        for j in range(n_cols):
            label = Text(f"CpG {j+1}", font_size=18)
            label.next_to(grid[j*2], UP, buff=0.3)
            col_labels.add(label)
        
        return VGroup(grid, row_labels, col_labels)
    
    def get_methylation_color(self, value):
        """Map methylation value to color (blue for low, red for high)."""
        r = value
        g = 0
        b = 1 - value
        return rgb_to_color([r, g, b])
    
    def highlight_patterns(self, matrix):
        """Highlight different methylation patterns in the matrix."""
        grid = matrix[0]
        
        # Highlight a column (CpG site across patients)
        col_highlight = SurroundingRectangle(
            VGroup(*[grid[i*16 + 4] for i in range(5)]), 
            color=YELLOW, 
            buff=0.05
        )
        col_text = Text("CpG site pattern", font_size=24, color=YELLOW)
        col_text.next_to(col_highlight, RIGHT)
        
        self.play(Create(col_highlight), Write(col_text))
        self.wait()
        
        # Highlight a row (patient's methylation profile)
        row_highlight = SurroundingRectangle(
            VGroup(*[grid[2*16 + j] for j in range(8)]), 
            color=GREEN, 
            buff=0.05
        )
        row_text = Text("Patient's methylation profile", font_size=24, color=GREEN)
        row_text.next_to(row_highlight, RIGHT)
        
        self.play(Create(row_highlight), Write(row_text))
        self.wait(2)
        
        self.play(
            FadeOut(col_highlight),
            FadeOut(col_text),
            FadeOut(row_highlight),
            FadeOut(row_text)
        )

class TransformerOverview(Scene):
    """Overview of the Transformer architecture."""
    
    def construct(self):
        # Title
        title = Text("The Transformer Architecture", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain why transformers are suitable
        explanation = Text(
            "Transformers excel at modeling complex patterns\nand interactions between features",
            font_size=36,
            line_spacing=1.2
        )
        explanation.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(explanation, run_time=2))
        self.wait()
        
        # Show the overall transformer block diagram
        transformer_diagram = self.create_transformer_diagram()
        transformer_diagram.scale(0.9).next_to(explanation, DOWN, buff=1)
        
        self.play(Create(transformer_diagram, run_time=3))
        self.wait(2)
        
        # Explain key innovations for epigenomic data
        self.play(
            FadeOut(explanation),
            transformer_diagram.animate.scale(0.8).to_edge(LEFT)
        )
        
        innovations = VGroup(
            Text("Key Innovations:", font_size=40, color=YELLOW),
            Text("1. Self-supervised Masked Pretraining", font_size=32),
            Text("2. Mixture-of-Experts Feed-Forward Network", font_size=32),
            Text("3. Adaptive Computation Time", font_size=32)
        )
        
        innovations.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        innovations.next_to(title, DOWN, buff=0.8).to_edge(RIGHT)
        
        for i in range(len(innovations)):
            self.play(FadeIn(innovations[i], shift=UP*0.2))
            self.wait(0.5)
        
        self.wait(2)
        
        # Transition to detailed explanation
        next_text = Text("Let's explore each component in detail", font_size=36)
        next_text.to_edge(DOWN, buff=1)
        
        self.play(Write(next_text))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(transformer_diagram),
            FadeOut(innovations),
            FadeOut(next_text),
            run_time=1.5
        )
    
    def create_transformer_diagram(self):
        """Create a simplified diagram of the transformer architecture."""
        # Main blocks
        input_block = Rectangle(width=4, height=0.8, color=WHITE)
        input_text = Text("Input Methylation Data", font_size=24)
        input_text.move_to(input_block.get_center())
        input_group = VGroup(input_block, input_text)
        
        embedding_block = Rectangle(width=4, height=0.8, color=BLUE)
        embedding_text = Text("Input Embedding", font_size=24)
        embedding_text.move_to(embedding_block.get_center())
        embedding_group = VGroup(embedding_block, embedding_text)
        embedding_group.next_to(input_group, DOWN, buff=0.5)
        
        # Transformer encoder block
        encoder_block = Rectangle(width=4, height=4, color=TRANSFORMER_COLOR)
        encoder_block.next_to(embedding_group, DOWN, buff=0.5)
        
        attn_block = Rectangle(width=3.5, height=1.2, color=ATTENTION_COLOR)
        attn_text = Text("Multi-Head Self-Attention", font_size=20)
        attn_text.move_to(attn_block.get_center())
        attn_group = VGroup(attn_block, attn_text)
        attn_group.move_to(encoder_block.get_center() + UP * 1)
        
        ffn_block = Rectangle(width=3.5, height=1.2, color=EXPERT_COLOR)
        ffn_text = Text("MoE Feed-Forward", font_size=20)
        ffn_text.move_to(ffn_block.get_center())
        ffn_group = VGroup(ffn_block, ffn_text)
        ffn_group.move_to(encoder_block.get_center() + DOWN * 1)
        
        encoder_text = Text("Transformer Encoder", font_size=24, color=TRANSFORMER_COLOR)
        encoder_text.next_to(encoder_block, LEFT, buff=0.3)
        
        # ACT mechanism
        act_block = Rectangle(width=4, height=0.8, color=RED)
        act_text = Text("Adaptive Computation Time", font_size=20)
        act_text.move_to(act_block.get_center())
        act_group = VGroup(act_block, act_text)
        act_group.next_to(encoder_block, DOWN, buff=0.5)
        
        # Final classification block
        output_block = Rectangle(width=4, height=0.8, color=GREEN)
        output_text = Text("Classification Output", font_size=24)
        output_text.move_to(output_block.get_center())
        output_group = VGroup(output_block, output_text)
        output_group.next_to(act_group, DOWN, buff=0.5)
        
        # Arrows
        arrows = VGroup(
            Arrow(input_group.get_bottom(), embedding_group.get_top()),
            Arrow(embedding_group.get_bottom(), encoder_block.get_top()),
            Arrow(encoder_block.get_bottom(), act_group.get_top()),
            Arrow(act_group.get_bottom(), output_group.get_top())
        )
        
        # x6 label for encoder blocks
        x6_text = Text("×6", font_size=28, color=TRANSFORMER_COLOR)
        x6_text.next_to(encoder_block, RIGHT, buff=0.3)
        
        # Create a loop arrow from ACT back to encoder
        loop_arrow = CurvedArrow(
            act_group.get_left() + LEFT * 0.2,
            encoder_block.get_left() + LEFT * 0.2,
            angle=-np.pi/2
        )
        loop_text = Text("If needed", font_size=18, color=RED)
        loop_text.next_to(loop_arrow, LEFT)
        
        return VGroup(
            input_group, embedding_group, encoder_block, attn_group, ffn_group,
            act_group, output_group, arrows, encoder_text, x6_text, loop_arrow, loop_text
        )

class InputEmbedding(Scene):
    """Explaining how methylation data is embedded for the transformer."""
    
    def construct(self):
        # Title
        title = Text("Input Embedding for Methylation Data", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Showing the methylation vector
        vector_title = Text("Original Methylation Values:", font_size=36)
        vector_title.next_to(title, DOWN, buff=0.8).to_edge(LEFT)
        
        self.play(Write(vector_title))
        
        # Create a visualization of the methylation vector
        values = np.random.rand(10)  # 10 example values
        meth_vector = self.create_feature_vector(values, "x")
        meth_vector.next_to(vector_title, DOWN, buff=0.4, aligned_edge=LEFT)
        
        self.play(Create(meth_vector))
        self.wait()
        
        # Show the embedding process
        embed_arrow = Arrow(
            meth_vector.get_right() + RIGHT * 0.5,
            meth_vector.get_right() + RIGHT * 2,
            buff=0
        )
        embed_text = MathTex(r"\mathbf{e} = W_e \mathbf{x} + \mathbf{b}_e", font_size=36)
        embed_text.next_to(embed_arrow, UP, buff=0.2)
        
        self.play(GrowArrow(embed_arrow), Write(embed_text))
        self.wait()
        
        # Show the embedded vector
        embedded_values = np.random.rand(10)  # Example embedded values
        embedded_vector = self.create_feature_vector(embedded_values, "e")
        embedded_vector.next_to(embed_arrow, RIGHT, buff=0.5)
        
        self.play(Create(embedded_vector))
        self.wait()
        
        # Explain chunking process
        self.play(
            meth_vector.animate.shift(UP * 1.5),
            embed_arrow.animate.shift(UP * 1.5),
            embed_text.animate.shift(UP * 1.5),
            embedded_vector.animate.shift(UP * 1.5)
        )
        
        chunking_title = Text("Chunking into L=4 Tokens:", font_size=36)
        chunking_title.next_to(meth_vector, DOWN, buff=1, aligned_edge=LEFT)
        
        self.play(Write(chunking_title))
        
        # Visualize the chunking
        chunks = self.create_chunks(embedded_values)
        chunks.next_to(chunking_title, DOWN, buff=0.5, aligned_edge=LEFT)
        
        self.play(Create(chunks))
        self.wait()
        
        # Show positional encoding
        pos_enc = Text("+ Positional Encoding", font_size=32)
        pos_enc.next_to(chunks, RIGHT, buff=1)
        
        pos_arrow = Arrow(chunks.get_right(), pos_enc.get_left(), buff=0.2)
        
        self.play(GrowArrow(pos_arrow), Write(pos_enc))
        self.wait()
        
        # Final form before transformer
        final_form = MathTex(r"Z^0 = E + P", font_size=42)
        final_form.next_to(chunks, DOWN, buff=1)
        
        self.play(Write(final_form))
        self.wait(2)
        
        # Summary of what we've done
        summary = Text(
            "We've transformed methylation data into a format\nsuitable for transformer processing",
            font_size=36,
            line_spacing=1.2
        )
        summary.to_edge(DOWN, buff=1)
        
        self.play(Write(summary))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(vector_title),
            FadeOut(meth_vector),
            FadeOut(embed_arrow),
            FadeOut(embed_text),
            FadeOut(embedded_vector),
            FadeOut(chunking_title),
            FadeOut(chunks),
            FadeOut(pos_enc),
            FadeOut(pos_arrow),
            FadeOut(final_form),
            FadeOut(summary),
            run_time=1.5
        )
    
    def create_feature_vector(self, values, symbol):
        """Create a visualization of a feature vector."""
        n_values = len(values)
        square_size = 0.4
        
        vector = VGroup()
        
        # Create squares with values
        for i, val in enumerate(values):
            color = self.get_value_color(val)
            square = Square(side_length=square_size)
            square.set_fill(color, opacity=0.8)
            square.set_stroke(WHITE, width=1)
            square.move_to([i*square_size, 0, 0])
            
            # Add value text
            value_text = Text(f"{val:.2f}", font_size=14)
            value_text.move_to(square.get_center())
            
            vector.add(VGroup(square, value_text))
        
        # Add brackets and labels
        left_bracket = Text("[", font_size=36)
        left_bracket.next_to(vector[0], LEFT, buff=0.1)
        
        right_bracket = Text("]", font_size=36)
        right_bracket.next_to(vector[-1], RIGHT, buff=0.1)
        
        vector_label = MathTex(f"\\mathbf{{{symbol}}}", font_size=36)
        vector_label.next_to(left_bracket, LEFT, buff=0.2)
        
        return VGroup(vector, left_bracket, right_bracket, vector_label)
    
    def create_chunks(self, values):
        """Create a visualization of chunked tokens."""
        n_chunks = 4
        chunk_size = len(values) // n_chunks
        square_size = 0.4
        
        chunks = VGroup()
        
        for c in range(n_chunks):
            chunk = VGroup()
            for i in range(chunk_size):
                idx = c * chunk_size + i
                if idx < len(values):
                    val = values[idx]
                    color = self.get_value_color(val)
                    square = Square(side_length=square_size)
                    square.set_fill(color, opacity=0.8)
                    square.set_stroke(WHITE, width=1)
                    square.move_to([i*square_size, 0, 0])
                    chunk.add(square)
            
            # Add bracket and label
            chunk_label = Text(f"Token {c+1}", font_size=24)
            chunk_label.next_to(chunk, DOWN, buff=0.2)
            
            chunk_group = VGroup(chunk, chunk_label)
            if c > 0:
                chunk_group.next_to(chunks[-1], RIGHT, buff=0.5)
            
            chunks.add(chunk_group)
        
        return chunks
    
    def get_value_color(self, value):
        """Map value to color (blue to red gradient)."""
        r = value
        g = 0.2
        b = 1 - value
        return rgb_to_color([r, g, b])

class SelfAttentionMechanism(Scene):
    """Explaining the self-attention mechanism in detail."""
    
    def construct(self):
        # Title
        title = Text("Self-Attention Mechanism", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain the key insight
        insight = Text(
            "Key insight: Let the model learn which features\nshould pay attention to which other features",
            font_size=36,
            line_spacing=1.2
        )
        insight.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(insight, run_time=2))
        self.wait(2)
        
        # Transition to detailed explanation
        self.play(
            insight.animate.scale(0.8).to_edge(UP, buff=1.5),
            title.animate.shift(UP * 0.5)
        )
        
        # Explain the Query, Key, Value concept
        qkv_title = Text("Query, Key, Value Projections:", font_size=36)
        qkv_title.next_to(insight, DOWN, buff=0.8)
        
        self.play(Write(qkv_title))
        
        # Create visualization for QKV
        qkv_diagram = self.create_qkv_diagram()
        qkv_diagram.next_to(qkv_title, DOWN, buff=0.5)
        
        self.play(Create(qkv_diagram, run_time=3))
        self.wait()
        
        # Show the mathematical formula
        self.play(
            FadeOut(qkv_title),
            qkv_diagram.animate.scale(0.8).to_edge(LEFT)
        )
        
        formulas = VGroup(
            MathTex(r"Q = Z^{l-1}W_Q", font_size=36),
            MathTex(r"K = Z^{l-1}W_K", font_size=36),
            MathTex(r"V = Z^{l-1}W_V", font_size=36)
        )
        
        formulas.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        formulas.next_to(insight, DOWN, buff=0.8).to_edge(RIGHT)
        
        self.play(Write(formulas[0]))
        self.wait(0.5)
        self.play(Write(formulas[1]))
        self.wait(0.5)
        self.play(Write(formulas[2]))
        self.wait()
        
        # Transition to attention score calculation
        self.play(
            FadeOut(qkv_diagram),
            FadeOut(formulas)
        )
        
        # Show attention score calculation
        attn_title = Text("Calculating Attention Scores:", font_size=36)
        attn_title.next_to(insight, DOWN, buff=0.8).to_edge(LEFT)
        
        self.play(Write(attn_title))
        
        # Show matrix multiplication
        matrix_mult = self.create_matrix_multiplication_visual()
        matrix_mult.next_to(attn_title, DOWN, buff=0.5)
        
        self.play(Create(matrix_mult, run_time=2))
        self.wait()
        
        # Show scaling and softmax
        scaling_text = MathTex(r"\text{Scaled Attention} = \frac{QK^T}{\sqrt{d}}", font_size=36)
        scaling_text.next_to(matrix_mult, DOWN, buff=0.8)
        
        self.play(Write(scaling_text))
        self.wait()
        
        softmax_text = MathTex(r"A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)", font_size=36)
        softmax_text.next_to(scaling_text, DOWN, buff=0.5)
        
        self.play(Write(softmax_text))
        self.wait()
        
        # Visualize softmax transformation
        softmax_viz = self.create_softmax_visualization()
        softmax_viz.next_to(softmax_text, DOWN, buff=0.5)
        
        self.play(Create(softmax_viz))
        self.wait(2)
        
        # Show final attention output calculation
        self.play(
            FadeOut(matrix_mult),
            FadeOut(scaling_text),
            FadeOut(softmax_text),
            FadeOut(softmax_viz)
        )
        
        # Final attention output
        output_title = Text("Final Attention Output:", font_size=36)
        output_title.next_to(attn_title, DOWN, buff=0.8)
        
        self.play(Write(output_title))
        
        output_formula = MathTex(r"\text{Attention}(Q,K,V) = AV", font_size=36)
        output_formula.next_to(output_title, DOWN, buff=0.5)
        
        self.play(Write(output_formula))
        self.wait()
        
        # Multi-head attention
        multihead_title = Text("Multi-Head Attention:", font_size=36)
        multihead_title.next_to(output_formula, DOWN, buff=0.8)
        
        self.play(Write(multihead_title))
        
        multihead_formula = MathTex(r"\text{MultiHead}(Z) = \text{Concat}(\text{head}_1,...,\text{head}_8)W_O", font_size=32)
        multihead_formula.next_to(multihead_title, DOWN, buff=0.5)
        
        self.play(Write(multihead_formula))
        
        # Explain multi-head concept
        multihead_explanation = Text(
            "8 separate attention mechanisms run in parallel\neach focusing on different patterns",
            font_size=28,
            line_spacing=1.2
        )
        multihead_explanation.next_to(multihead_formula, DOWN, buff=0.5)
        
        self.play(Write(multihead_explanation))
        self.wait(2)
        
        # Explanation of biological significance
        biological_box = SurroundingRectangle(multihead_explanation, buff=0.5, color=YELLOW)
        biological_text = Text(
            "This allows the model to focus on different biological pathways:\nimmune genes, stress response genes, etc.",
            font_size=28,
            color=YELLOW,
            line_spacing=1.2
        )
        biological_text.next_to(biological_box, DOWN, buff=0.5)
        
        self.play(Create(biological_box), Write(biological_text))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(insight),
            FadeOut(attn_title),
            FadeOut(output_title),
            FadeOut(output_formula),
            FadeOut(multihead_title),
            FadeOut(multihead_formula),
            FadeOut(multihead_explanation),
            FadeOut(biological_box),
            FadeOut(biological_text),
            run_time=1.5
        )
    
    def create_qkv_diagram(self):
        """Create a visualization of the Query, Key, Value concept."""
        # Input tensor
        input_rect = Rectangle(width=4, height=0.8, color=WHITE)
        input_text = Text("Input Z", font_size=24)
        input_text.move_to(input_rect.get_center())
        input_group = VGroup(input_rect, input_text)
        
        # Projection matrices
        wq_rect = Rectangle(width=1.2, height=1.2, color=RED)
        wq_text = Text("WQ", font_size=24)
        wq_text.move_to(wq_rect.get_center())
        wq_group = VGroup(wq_rect, wq_text)
        wq_group.next_to(input_group, DOWN+RIGHT, buff=1)
        
        wk_rect = Rectangle(width=1.2, height=1.2, color=GREEN)
        wk_text = Text("WK", font_size=24)
        wk_text.move_to(wk_rect.get_center())
        wk_group = VGroup(wk_rect, wk_text)
        wk_group.next_to(input_group, DOWN, buff=1)
        
        wv_rect = Rectangle(width=1.2, height=1.2, color=BLUE)
        wv_text = Text("WV", font_size=24)
        wv_text.move_to(wv_rect.get_center())
        wv_group = VGroup(wv_rect, wv_text)
        wv_group.next_to(input_group, DOWN+LEFT, buff=1)
        
        # Output tensors
        q_rect = Rectangle(width=1.2, height=0.8, color=RED)
        q_text = Text("Q", font_size=24)
        q_text.move_to(q_rect.get_center())
        q_group = VGroup(q_rect, q_text)
        q_group.next_to(wq_group, DOWN, buff=0.7)
        
        k_rect = Rectangle(width=1.2, height=0.8, color=GREEN)
        k_text = Text("K", font_size=24)
        k_text.move_to(k_rect.get_center())
        k_group = VGroup(k_rect, k_text)
        k_group.next_to(wk_group, DOWN, buff=0.7)
        
        v_rect = Rectangle(width=1.2, height=0.8, color=BLUE)
        v_text = Text("V", font_size=24)
        v_text.move_to(v_rect.get_center())
        v_group = VGroup(v_rect, v_text)
        v_group.next_to(wv_group, DOWN, buff=0.7)
        
        # Arrows
        arrows = VGroup(
            Arrow(input_group.get_bottom(), wq_group.get_top(), buff=0.1),
            Arrow(input_group.get_bottom(), wk_group.get_top(), buff=0.1),
            Arrow(input_group.get_bottom(), wv_group.get_top(), buff=0.1),
            Arrow(wq_group.get_bottom(), q_group.get_top(), buff=0.1),
            Arrow(wk_group.get_bottom(), k_group.get_top(), buff=0.1),
            Arrow(wv_group.get_bottom(), v_group.get_top(), buff=0.1)
        )
        
        # Labels
        query_label = Text("Query: 'What am I looking for?'", font_size=20, color=RED)
        query_label.next_to(q_group, RIGHT, buff=0.5)
        
        key_label = Text("Key: 'What do I contain?'", font_size=20, color=GREEN)
        key_label.next_to(k_group, RIGHT, buff=0.5)
        
        value_label = Text("Value: 'What info do I carry?'", font_size=20, color=BLUE)
        value_label.next_to(v_group, RIGHT, buff=0.5)
        
        return VGroup(
            input_group, wq_group, wk_group, wv_group, q_group, k_group, v_group,
            arrows, query_label, key_label, value_label
        )
    
    def create_matrix_multiplication_visual(self):
        """Create a visualization of QK^T matrix multiplication."""
        # Q and K matrices
        q_matrix = self.create_matrix("Q", 4, 3, RED)
        k_matrix = self.create_matrix("K", 4, 3, GREEN)
        k_transpose = self.create_matrix("K^T", 3, 4, GREEN)
        
        # Position matrices
        q_matrix.to_edge(LEFT, buff=1)
        k_transpose.next_to(q_matrix, RIGHT, buff=0.5)
        
        # Multiplication operator
        mult_symbol = MathTex(r"\times", font_size=36)
        mult_symbol.move_to((q_matrix.get_right() + k_transpose.get_left()) / 2)
        
        # Result matrix
        result_matrix = self.create_matrix("QK^T", 4, 4, YELLOW)
        result_matrix.next_to(k_transpose, RIGHT, buff=1)
        
        # Equal sign
        equal_sign = MathTex(r"=", font_size=36)
        equal_sign.move_to((k_transpose.get_right() + result_matrix.get_left()) / 2)
        
        # Show matrix dimensions
        q_dim = Text("(4×3)", font_size=20, color=RED)
        q_dim.next_to(q_matrix, DOWN, buff=0.2)
        
        k_dim = Text("(3×4)", font_size=20, color=GREEN)
        k_dim.next_to(k_transpose, DOWN, buff=0.2)
        
        result_dim = Text("(4×4)", font_size=20, color=YELLOW)
        result_dim.next_to(result_matrix, DOWN, buff=0.2)
        
        # Explanation
        explanation = Text(
            "QK^T measures similarity between tokens\nHigher values = more attention",
            font_size=24,
            line_spacing=1.2
        )
        explanation.next_to(VGroup(q_matrix, k_transpose, result_matrix), DOWN, buff=0.8)
        
        return VGroup(
            q_matrix, k_transpose, mult_symbol, result_matrix, equal_sign,
            q_dim, k_dim, result_dim, explanation
        )
    
    def create_matrix(self, label, rows, cols, color=WHITE):
        """Create a visualization of a matrix."""
        cell_size = 0.3
        matrix = VGroup()
        
        # Create grid of cells
        for i in range(rows):
            for j in range(cols):
                cell = Square(side_length=cell_size)
                cell.set_stroke(color, width=1)
                cell.move_to([j*cell_size, -i*cell_size, 0])
                matrix.add(cell)
        
        # Add matrix label
        matrix_label = Text(label, font_size=24, color=color)
        matrix_label.next_to(matrix, UP, buff=0.2)
        
        return VGroup(matrix, matrix_label)
    
    def create_softmax_visualization(self):
        """Create a visualization of the softmax transformation."""
        # Input values
        input_values = [2.5, 1.2, 0.8, 3.1]
        
        # Calculate softmax
        exp_values = [np.exp(x) for x in input_values]
        sum_exp = sum(exp_values)
        softmax_values = [e / sum_exp for e in exp_values]
        
        # Input array
        input_array = self.create_array(input_values, "Input")
        
        # Exponential array
        exp_array = self.create_array(exp_values, "exp(Input)")
        exp_array.next_to(input_array, RIGHT, buff=1)
        
        # Softmax array
        softmax_array = self.create_array(softmax_values, "Softmax")
        softmax_array.next_to(exp_array, RIGHT, buff=1)
        
        # Arrows
        arrows = VGroup(
            Arrow(input_array.get_right(), exp_array.get_left(), buff=0.2),
            Arrow(exp_array.get_right(), softmax_array.get_left(), buff=0.2)
        )
        
        # Arrow labels
        exp_label = Text("Take exp", font_size=20)
        exp_label.next_to(arrows[0], UP, buff=0.1)
        
        normalize_label = Text("Normalize", font_size=20)
        normalize_label.next_to(arrows[1], UP, buff=0.1)
        
        # Sum is 1 annotation
        sum_label = Text("Sum = 1.0", font_size=20, color=YELLOW)
        sum_label.next_to(softmax_array, DOWN, buff=0.3)
        
        return VGroup(
            input_array, exp_array, softmax_array, arrows, exp_label, normalize_label, sum_label
        )
    
    def create_array(self, values, label):
        """Create a visualization of a 1D array with values."""
        cell_size = 0.5
        array = VGroup()
        
        # Create cells with values
        for i, val in enumerate(values):
            cell = Square(side_length=cell_size)
            cell.set_stroke(WHITE, width=1)
            cell.move_to([0, -i*cell_size, 0])
            
            # Add value text
            if abs(val) < 0.01:
                val_text = "≈0"
            elif abs(val) > 999:
                val_text = f"{val:.1e}"
            else:
                val_text = f"{val:.2f}"
                
            value_text = Text(val_text, font_size=16)
            value_text.move_to(cell.get_center())
            
            array.add(VGroup(cell, value_text))
        
        # Add array label
        array_label = Text(label, font_size=24)
        array_label.next_to(array, UP, buff=0.2)
        
        return VGroup(array, array_label)

class MixtureOfExperts(Scene):
    """Explaining the Mixture-of-Experts feed-forward network."""
    
    def construct(self):
        # Title
        title = Text("Mixture-of-Experts Feed-Forward Network", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain the concept
        concept = Text(
            "Instead of a single feed-forward network,\nuse multiple specialized 'expert' networks",
            font_size=36,
            line_spacing=1.2
        )
        concept.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(concept, run_time=2))
        self.wait()
        
        # Create a standard FFN for comparison
        standard_title = Text("Standard Feed-Forward Network:", font_size=32)
        standard_title.next_to(concept, DOWN, buff=0.8).to_edge(LEFT)
        
        self.play(Write(standard_title))
        
        standard_ffn = self.create_standard_ffn()
        standard_ffn.next_to(standard_title, DOWN, buff=0.4)
        
        self.play(Create(standard_ffn, run_time=2))
        self.wait()
        
        standard_formula = MathTex(
            r"\text{FFN}(x) = W_2(\text{GELU}(W_1 x + b_1)) + b_2",
            font_size=32
        )
        standard_formula.next_to(standard_ffn, DOWN, buff=0.5)
        
        self.play(Write(standard_formula))
        self.wait(2)
        
        # Transition to MoE
        self.play(
            FadeOut(standard_ffn),
            FadeOut(standard_formula)
        )
        
        moe_title = Text("Mixture-of-Experts Approach:", font_size=32)
        moe_title.next_to(standard_title, RIGHT, buff=3)
        
        self.play(Write(moe_title))
        
        moe_diagram = self.create_moe_diagram()
        moe_diagram.next_to(moe_title, DOWN, buff=0.4)
        
        self.play(Create(moe_diagram, run_time=3))
        self.wait()
        
        # Show MoE formula
        moe_formula = MathTex(
            r"\text{MoE-FFN}(x) = \sum_{e=1}^{4} g_e(x) \cdot \text{FFN}_e(x)",
            font_size=32
        )
        moe_formula.next_to(moe_diagram, DOWN, buff=0.5)
        
        self.play(Write(moe_formula))
        self.wait()
        
        # Show gating formula
        gating_formula = MathTex(
            r"g(x) = \text{softmax}(W_g x)",
            font_size=32
        )
        gating_formula.next_to(moe_formula, DOWN, buff=0.5)
        
        self.play(Write(gating_formula))
        self.wait(2)
        
        # Explain biological significance
        self.play(
            FadeOut(standard_title),
            FadeOut(moe_title),
            FadeOut(moe_diagram),
            FadeOut(concept)
        )
        
        bio_title = Text("Biological Significance of Mixture-of-Experts", font_size=40)
        bio_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(bio_title))
        
        # Create visuals for specialized experts
        experts = self.create_expert_specialization()
        experts.next_to(bio_title, DOWN, buff=0.8)
        
        self.play(Create(experts, run_time=3))
        self.wait()
        
        # Show example of gating
        example_title = Text("Example: How experts process different patients", font_size=32)
        example_title.next_to(experts, DOWN, buff=0.8)
        
        self.play(Write(example_title))
        
        patient_example = self.create_patient_example()
        patient_example.next_to(example_title, DOWN, buff=0.5)
        
        self.play(Create(patient_example, run_time=2))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(bio_title),
            FadeOut(experts),
            FadeOut(example_title),
            FadeOut(patient_example),
            FadeOut(moe_formula),
            FadeOut(gating_formula),
            run_time=1.5
        )
    
    def create_standard_ffn(self):
        """Create a visualization of a standard feed-forward network."""
        # Input node
        input_circle = Circle(radius=0.3, color=WHITE)
        input_text = Text("x", font_size=24)
        input_text.move_to(input_circle.get_center())
        input_node = VGroup(input_circle, input_text)
        
        # Hidden layer nodes
        hidden_nodes = VGroup()
        for i in range(4):
            circle = Circle(radius=0.3, color=BLUE)
            circle.move_to([0, -i*0.8 - 1.5, 0])
            text = Text(f"h{i+1}", font_size=20)
            text.move_to(circle.get_center())
            hidden_nodes.add(VGroup(circle, text))
        
        # Output node
        output_circle = Circle(radius=0.3, color=GREEN)
        output_circle.move_to([0, -5, 0])
        output_text = Text("y", font_size=24)
        output_text.move_to(output_circle.get_center())
        output_node = VGroup(output_circle, output_text)
        
        # Connect with arrows
        arrows1 = VGroup()
        for node in hidden_nodes:
            arrow = Arrow(input_node.get_bottom(), node.get_top(), buff=0.1)
            arrows1.add(arrow)
        
        arrows2 = VGroup()
        for node in hidden_nodes:
            arrow = Arrow(node.get_bottom(), output_node.get_top(), buff=0.1)
            arrows2.add(arrow)
        
        # Layer labels
        input_label = Text("Input", font_size=20)
        input_label.next_to(input_node, LEFT, buff=0.5)
        
        hidden_label = Text("Hidden Layer\n(GELU Activation)", font_size=20, line_spacing=1)
        hidden_label.next_to(hidden_nodes, LEFT, buff=0.5)
        
        output_label = Text("Output", font_size=20)
        output_label.next_to(output_node, LEFT, buff=0.5)
        
        # Weight labels
        w1_label = Text("W1", font_size=18, color=BLUE)
        w1_label.next_to(arrows1[1], RIGHT, buff=0.1)
        
        w2_label = Text("W2", font_size=18, color=GREEN)
        w2_label.next_to(arrows2[1], RIGHT, buff=0.1)
        
        return VGroup(
            input_node, hidden_nodes, output_node, arrows1, arrows2,
            input_label, hidden_label, output_label, w1_label, w2_label
        )
    
    def create_moe_diagram(self):
        """Create a visualization of a mixture-of-experts feed-forward network."""
        # Input
        input_circle = Circle(radius=0.3, color=WHITE)
        input_text = Text("x", font_size=24)
        input_text.move_to(input_circle.get_center())
        input_node = VGroup(input_circle, input_text)
        
        # Gating network
        gate_rect = Rectangle(width=1.5, height=0.8, color=YELLOW)
        gate_text = Text("Gating", font_size=20)
        gate_text.move_to(gate_rect.get_center())
        gate_network = VGroup(gate_rect, gate_text)
        gate_network.next_to(input_node, DOWN, buff=1)
        
        # Expert networks
        experts = VGroup()
        expert_colors = [RED, GREEN, BLUE, PURPLE]
        
        for i in range(4):
            expert = self.create_mini_ffn(expert_colors[i])
            experts.add(expert)
        
        # Position experts
        for i, expert in enumerate(experts):
            expert.move_to([i*2 - 3, -3, 0])
        
        # Arrows from input to gate
        input_gate_arrow = Arrow(input_node.get_bottom(), gate_network.get_top(), buff=0.1)
        
        # Gating output (softmax weights)
        weights_text = VGroup()
        weight_arrows = VGroup()
        
        for i in range(4):
            text = MathTex(f"g_{i+1}", font_size=24, color=YELLOW)
            text.next_to(experts[i], UP, buff=0.8)
            weights_text.add(text)
            
            arrow = Arrow(gate_network.get_bottom(), text.get_top(), buff=0.1, color=YELLOW)
            weight_arrows.add(arrow)
        
        # Input to experts
        input_expert_arrows = VGroup()
        
        for expert in experts:
            arrow = Arrow(input_node.get_center(), expert.get_top(), buff=0.1)
            input_expert_arrows.add(arrow)
        
        # Weighted combination
        output_circle = Circle(radius=0.4, color=WHITE)
        output_text = Text("Σ", font_size=30)
        output_text.move_to(output_circle.get_center())
        output_node = VGroup(output_circle, output_text)
        output_node.next_to(experts, DOWN, buff=1.5)
        
        # Arrows from experts to output
        expert_output_arrows = VGroup()
        
        for i, expert in enumerate(experts):
            arrow = Arrow(expert.get_bottom(), output_node.get_top(), buff=0.1, color=expert_colors[i])
            expert_output_arrows.add(arrow)
        
        # Labels
        gate_label = Text("Determines which\nexperts to use", font_size=18, line_spacing=1)
        gate_label.next_to(gate_network, LEFT, buff=0.5)
        
        output_label = Text("Weighted sum\nof outputs", font_size=18, line_spacing=1)
        output_label.next_to(output_node, RIGHT, buff=0.5)
        
        expert_labels = VGroup()
        for i, expert in enumerate(experts):
            label = Text(f"Expert {i+1}", font_size=18, color=expert_colors[i])
            label.next_to(expert, DOWN, buff=0.2)
            expert_labels.add(label)
        
        return VGroup(
            input_node, gate_network, experts, output_node,
            input_gate_arrow, weight_arrows, input_expert_arrows, expert_output_arrows,
            weights_text, gate_label, output_label, expert_labels
        )
    
    def create_mini_ffn(self, color=BLUE):
        """Create a small feed-forward network for visualization."""
        # Two-layer network
        nodes = VGroup()
        
        # Input node
        input_node = Circle(radius=0.15, color=WHITE)
        
        # Hidden nodes
        for i in range(3):
            node = Circle(radius=0.15, color=color)
            node.move_to([i*0.4 - 0.4, -0.4, 0])
            nodes.add(node)
        
        # Output node
        output_node = Circle(radius=0.15, color=color)
        output_node.move_to([0, -0.8, 0])
        
        # Connect with lines
        lines = VGroup()
        
        for node in nodes:
            line = Line(input_node.get_center(), node.get_center(), color=color, stroke_width=1)
            lines.add(line)
        
        for node in nodes:
            line = Line(node.get_center(), output_node.get_center(), color=color, stroke_width=1)
            lines.add(line)
        
        mini_ffn = VGroup(input_node, nodes, output_node, lines)
        
        return mini_ffn
    
    def create_expert_specialization(self):
        """Create a visualization of expert specializations."""
        # Create four specialized "experts"
        specializations = VGroup()
        
        # Expert 1: Immune genes
        immune_rect = Rectangle(width=3, height=1.5, color=RED)
        immune_title = Text("Expert 1: Immune Genes", font_size=24, color=RED)
        immune_title.next_to(immune_rect, UP, buff=0.2)
        immune_content = Text("HLA-DRB1, IFNG, TNF", font_size=20)
        immune_content.move_to(immune_rect.get_center())
        immune_group = VGroup(immune_rect, immune_title, immune_content)
        
        # Expert 2: Stress response
        stress_rect = Rectangle(width=3, height=1.5, color=GREEN)
        stress_title = Text("Expert 2: Stress Response", font_size=24, color=GREEN)
        stress_title.next_to(stress_rect, UP, buff=0.2)
        stress_content = Text("NR3C1, CRH, FKBP5", font_size=20)
        stress_content.move_to(stress_rect.get_center())
        stress_group = VGroup(stress_rect, stress_title, stress_content)
        stress_group.next_to(immune_group, RIGHT, buff=1)
        
        # Expert 3: Metabolic pathways
        metabolic_rect = Rectangle(width=3, height=1.5, color=BLUE)
        metabolic_title = Text("Expert 3: Metabolism", font_size=24, color=BLUE)
        metabolic_title.next_to(metabolic_rect, UP, buff=0.2)
        metabolic_content = Text("PDK2, AMPK, MTOR", font_size=20)
        metabolic_content.move_to(metabolic_rect.get_center())
        metabolic_group = VGroup(metabolic_rect, metabolic_title, metabolic_content)
        metabolic_group.next_to(immune_group, DOWN, buff=1)
        
        # Expert 4: Viral response
        viral_rect = Rectangle(width=3, height=1.5, color=PURPLE)
        viral_title = Text("Expert 4: Viral Response", font_size=24, color=PURPLE)
        viral_title.next_to(viral_rect, UP, buff=0.2)
        viral_content = Text("IFITM3, ISG15, OAS1", font_size=20)
        viral_content.move_to(viral_rect.get_center())
        viral_group = VGroup(viral_rect, viral_title, viral_content)
        viral_group.next_to(stress_group, DOWN, buff=1)
        
        specializations.add(immune_group, stress_group, metabolic_group, viral_group)
        
        return specializations
    
    def create_patient_example(self):
        """Create an example of different gating for different patients."""
        # Create a table showing expert weights for different patients
        table = VGroup()
        
        # Header row
        header = VGroup()
        header_texts = ["Patient Type", "Expert 1\n(Immune)", "Expert 2\n(Stress)", "Expert 3\n(Metabolism)", "Expert 4\n(Viral)"]
        
        for i, text in enumerate(header_texts):
            cell = Rectangle(width=2, height=0.8)
            cell.set_stroke(WHITE, width=1)
            if i > 0:
                cell.next_to(header[i-1], RIGHT, buff=0)
            
            cell_text = Text(text, font_size=16, line_spacing=0.8)
            cell_text.move_to(cell.get_center())
            
            header.add(VGroup(cell, cell_text))
        
        table.add(header)
        
        # Data rows
        patient_types = ["ME/CFS", "Long COVID", "Control"]
        
        # Expert weights for different patients (normalized to sum to 1)
        weights = [
            [0.45, 0.30, 0.20, 0.05],  # ME/CFS - high immune & stress
            [0.30, 0.15, 0.15, 0.40],  # Long COVID - high immune & viral
            [0.20, 0.25, 0.40, 0.15]   # Control - more balanced, higher metabolism
        ]
        
        for i, patient in enumerate(patient_types):
            row = VGroup()
            
            # Patient cell
            patient_cell = Rectangle(width=2, height=0.8)
            patient_cell.set_stroke(WHITE, width=1)
            patient_cell.next_to(header[0], DOWN, buff=0) if i == 0 else patient_cell.next_to(table[-1][0], DOWN, buff=0)
            
            patient_text = Text(patient, font_size=16)
            patient_text.move_to(patient_cell.get_center())
            
            row.add(VGroup(patient_cell, patient_text))
            
            # Weight cells
            for j, weight in enumerate(weights[i]):
                cell = Rectangle(width=2, height=0.8)
                cell.set_stroke(WHITE, width=1)
                cell.next_to(row[j], RIGHT, buff=0)
                
                # Color-coded weight
                color = self.get_weight_color(weight)
                weight_text = Text(f"{weight:.2f}", font_size=16, color=color)
                weight_text.move_to(cell.get_center())
                
                row.add(VGroup(cell, weight_text))
            
            table.add(row)
        
        # Highlight the highest weight in each row
        highlights = VGroup()
        
        for i, row in enumerate(table[1:]):
            max_idx = weights[i].index(max(weights[i]))
            cell = row[max_idx + 1][0]  # +1 to skip patient name cell
            highlight = SurroundingRectangle(cell, color=YELLOW, buff=0.05)
            highlights.add(highlight)
        
        # Add explanation
        explanation = Text(
            "The gating mechanism routes each sample to the most relevant experts",
            font_size=24
        )
        explanation.next_to(table, DOWN, buff=0.8)
        
        return VGroup(table, highlights, explanation)
    
    def get_weight_color(self, weight):
        """Color code for weights (higher = more saturated)."""
        if weight < 0.2:
            return GRAY
        elif weight < 0.3:
            return BLUE_C
        elif weight < 0.4:
            return GREEN_C
        else:
            return RED_C

class AdaptiveComputationTime(Scene):
    """Explaining the Adaptive Computation Time mechanism."""
    
    def construct(self):
        # Title
        title = Text("Adaptive Computation Time (ACT)", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain the concept
        concept = Text(
            "Key idea: Allow the model to decide how many\nprocessing steps to use for each sample",
            font_size=36,
            line_spacing=1.2
        )
        concept.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(concept, run_time=2))
        self.wait()
        
        # Visualize the ACT mechanism
        act_diagram = self.create_act_diagram()
        act_diagram.next_to(concept, DOWN, buff=0.8)
        
        self.play(Create(act_diagram, run_time=3))
        self.wait()
        
        # Show the halting probability calculation
        halt_title = Text("Halting Probability Calculation:", font_size=32)
        halt_title.next_to(act_diagram, DOWN, buff=0.8)
        
        self.play(Write(halt_title))
        
        halt_formula = MathTex(r"p = \sigma(w^T x_{\text{mean}})", font_size=36)
        halt_formula.next_to(halt_title, DOWN, buff=0.4)
        
        self.play(Write(halt_formula))
        self.wait()
        
        # Show the halting mechanism
        halting_mechanism = self.create_halting_mechanism()
        halting_mechanism.next_to(halt_formula, DOWN, buff=0.8)
        
        self.play(Create(halting_mechanism, run_time=2))
        self.wait(2)
        
        # Explain biological relevance
        self.play(
            FadeOut(act_diagram),
            FadeOut(halt_title),
            FadeOut(halt_formula),
            FadeOut(halting_mechanism)
        )
        
        relevance_title = Text("Clinical Relevance:", font_size=36)
        relevance_title.next_to(concept, DOWN, buff=0.8)
        
        self.play(Write(relevance_title))
        
        relevance = self.create_relevance_viz()
        relevance.next_to(relevance_title, DOWN, buff=0.4)
        
        self.play(Create(relevance, run_time=2))
        self.wait()
        
        # Show distribution of iterations
        distribution_title = Text("Distribution of Iterations in the Model:", font_size=32)
        distribution_title.next_to(relevance, DOWN, buff=0.8)
        
        self.play(Write(distribution_title))
        
        distribution = self.create_iteration_distribution()
        distribution.next_to(distribution_title, DOWN, buff=0.4)
        
        self.play(Create(distribution))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(concept),
            FadeOut(relevance_title),
            FadeOut(relevance),
            FadeOut(distribution_title),
            FadeOut(distribution),
            run_time=1.5
        )
    
    def create_act_diagram(self):
        """Create a visualization of the Adaptive Computation Time mechanism."""
        # Input
        input_rect = Rectangle(width=3, height=0.8, color=WHITE)
        input_text = Text("Sample Features", font_size=24)
        input_text.move_to(input_rect.get_center())
        input_group = VGroup(input_rect, input_text)
        
        # Transformer encoder
        encoder_rect = Rectangle(width=3, height=1.5, color=TRANSFORMER_COLOR)
        encoder_text = Text("Transformer\nEncoder", font_size=24, line_spacing=1)
        encoder_text.move_to(encoder_rect.get_center())
        encoder_group = VGroup(encoder_rect, encoder_text)
        encoder_group.next_to(input_group, DOWN, buff=0.8)
        
        # Halting unit
        halt_rect = Rectangle(width=2, height=0.8, color=RED)
        halt_text = Text("Halting Unit", font_size=20)
        halt_text.move_to(halt_rect.get_center())
        halt_group = VGroup(halt_rect, halt_text)
        halt_group.next_to(encoder_group, RIGHT, buff=1.5)
        
        # Output
        output_rect = Rectangle(width=3, height=0.8, color=GREEN)
        output_text = Text("Output", font_size=24)
        output_text.move_to(output_rect.get_center())
        output_group = VGroup(output_rect, output_text)
        output_group.next_to(encoder_group, DOWN, buff=0.8)
        
        # Arrows
        input_encoder = Arrow(input_group.get_bottom(), encoder_group.get_top(), buff=0.1)
        encoder_output = Arrow(encoder_group.get_bottom(), output_group.get_top(), buff=0.1)
        encoder_halt = Arrow(encoder_group.get_right(), halt_group.get_left(), buff=0.1)
        
        # Loop back arrow
        loop_arrow = CurvedArrow(
            halt_group.get_bottom(),
            encoder_group.get_top() + RIGHT * 0.5,
            angle=-np.pi/2
        )
        
        # Decision diamond
        decision = Polygon(
            UP * 0.4, RIGHT * 0.4, DOWN * 0.4, LEFT * 0.4,
            color=YELLOW
        )
        decision.next_to(halt_group, DOWN, buff=0.5)
        decision_text = Text("p>0.99?", font_size=18)
        decision_text.move_to(decision.get_center())
        decision_group = VGroup(decision, decision_text)
        
        halt_to_decision = Arrow(halt_group.get_bottom(), decision.get_top(), buff=0.1)
        
        yes_arrow = Arrow(decision.get_right(), decision.get_right() + RIGHT * 0.8, buff=0.1)
        yes_text = Text("Yes", font_size=18, color=GREEN)
        yes_text.next_to(yes_arrow, UP, buff=0.1)
        
        no_arrow = Arrow(decision.get_bottom(), loop_arrow.get_start(), buff=0.1)
        no_text = Text("No", font_size=18, color=RED)
        no_text.next_to(no_arrow, RIGHT, buff=0.1)
        
        # Add labels
        max_iter = Text("Max iterations = 3", font_size=20, color=RED)
        max_iter.next_to(loop_arrow, LEFT, buff=0.5)
        
        return VGroup(
            input_group, encoder_group, halt_group, output_group,
            input_encoder, encoder_output, encoder_halt,
            loop_arrow, decision_group, halt_to_decision,
            yes_arrow, yes_text, no_arrow, no_text, max_iter
        )
    
    def create_halting_mechanism(self):
        """Create a visualization of the halting mechanism details."""
        # Table showing halting probability accumulation
        table = VGroup()
        
        # Header
        header_texts = ["Iteration", "p", "H (accumulated)", "Action"]
        cells = VGroup()
        
        for i, text in enumerate(header_texts):
            cell = Rectangle(width=2, height=0.6)
            cell.set_stroke(WHITE, width=1)
            if i > 0:
                cell.next_to(cells[-1], RIGHT, buff=0)
            else:
                cell.to_edge(LEFT, buff=1)
            
            cell_text = Text(text, font_size=20)
            cell_text.move_to(cell.get_center())
            
            cells.add(VGroup(cell, cell_text))
        
        table.add(cells)
        
        # Example rows
        example_data = [
            ["1", "0.3", "0.3", "Continue"],
            ["2", "0.6", "0.9", "Continue"],
            ["3", "0.2", "1.0", "Halt (H ≥ 1)"]
        ]
        
        for data in example_data:
            row = VGroup()
            
            for i, value in enumerate(data):
                cell = Rectangle(width=2, height=0.6)
                cell.set_stroke(WHITE, width=1)
                
                if i == 0:  # First cell in row
                    prev_row = table[-1] if len(table) > 1 else table[0]
                    cell.next_to(prev_row[0], DOWN, buff=0)
                else:
                    cell.next_to(row[-1], RIGHT, buff=0)
                
                color = WHITE
                if i == 3:  # Action column
                    color = GREEN if "Halt" in value else BLUE
                
                cell_text = Text(value, font_size=18, color=color)
                cell_text.move_to(cell.get_center())
                
                row.add(VGroup(cell, cell_text))
            
            table.add(row)
        
        # Add explanation
        explanation = Text(
            "Halting probability accumulates until reaching 1.0\nOr maximum iterations is reached",
            font_size=24,
            line_spacing=1
        )
        explanation.next_to(table, DOWN, buff=0.5)
        
        # Add loss penalty term
        penalty = MathTex(
            r"R = \sum_{\text{sample } j} \sum_{t=1}^{T_j} \big( p_j^{(t)} \prod_{s< t}(1 - p_j^{(s)}) \big) \cdot t",
            font_size=28
        )
        penalty.next_to(explanation, DOWN, buff=0.5)
        
        penalty_explanation = Text(
            "Loss term penalizes excessive computation",
            font_size=20,
            color=RED
        )
        penalty_explanation.next_to(penalty, DOWN, buff=0.3)
        
        return VGroup(table, explanation, penalty, penalty_explanation)
    
    def create_relevance_viz(self):
        """Create a visualization of the clinical relevance of ACT."""
        # Create two patient examples
        example = VGroup()
        
        # Patient 1: Clear case
        clear_rect = Rectangle(width=4, height=2, color=BLUE)
        clear_title = Text("Clear Case Patient", font_size=28, color=BLUE)
        clear_title.next_to(clear_rect, UP, buff=0.2)
        
        clear_content = VGroup(
            Text("• Strong methylation signature", font_size=20),
            Text("• Clear disease patterns", font_size=20),
            Text("• Halts after 1 iteration (p=0.95)", font_size=20, color=GREEN)
        )
        clear_content.arrange(DOWN, aligned_edge=LEFT, buff=0.2)
        clear_content.move_to(clear_rect.get_center())
        
        clear_group = VGroup(clear_rect, clear_title, clear_content)
        
        # Patient 2: Ambiguous case
        ambig_rect = Rectangle(width=4, height=2, color=RED)
        ambig_title = Text("Ambiguous Case Patient", font_size=28, color=RED)
        ambig_title.next_to(ambig_rect, UP, buff=0.2)
        
        ambig_content = VGroup(
            Text("• Mixed methylation patterns", font_size=20),
            Text("• Borderline between conditions", font_size=20),
            Text("• Needs 3 iterations (p=0.3, 0.4, 0.3)", font_size=20, color=YELLOW)
        )
        ambig_content.arrange(DOWN, aligned_edge=LEFT, buff=0.2)
        ambig_content.move_to(ambig_rect.get_center())
        
        ambig_group = VGroup(ambig_rect, ambig_title, ambig_content)
        ambig_group.next_to(clear_group, RIGHT, buff=1)
        
        example.add(clear_group, ambig_group)
        
        # Add explanation
        explanation = Text(
            "More computation is allocated to difficult or ambiguous cases,\nmaking the model efficient and accurate.",
            font_size=24,
            line_spacing=1
        )
        explanation.next_to(VGroup(clear_group, ambig_group), DOWN, buff=0.5)
        
        return VGroup(example, explanation)
    
    def create_iteration_distribution(self):
        """Create a bar chart showing distribution of iterations."""
        # Create axes
        axes = Axes(
            x_range=[0, 4, 1],
            y_range=[0, 80, 20],
            x_length=6,
            y_length=3,
            axis_config={"color": WHITE}
        )
        
        # Labels
        x_label = Text("Number of Iterations", font_size=20)
        x_label.next_to(axes, DOWN, buff=0.3)
        
        y_label = Text("% of Samples", font_size=20)
        y_label.next_to(axes, LEFT, buff=0.3).rotate(PI/2)
        
        # Data
        data = [0, 70, 25, 5]  # 0%, 70%, 25%, 5% for 0, 1, 2, 3 iterations
        
        # Create bars
        bars = VGroup()
        bar_colors = [BLUE_E, BLUE, YELLOW, RED]
        
        for i, value in enumerate(data):
            if i > 0:  # Skip the 0 iteration case
                bar = Rectangle(
                    width=0.5,
                    height=value * 3 / 100,  # Scale to match y-axis
                    fill_opacity=0.8,
                    fill_color=bar_colors[i],
                    stroke_color=WHITE
                )
                bar.move_to(axes.c2p(i, value/2), aligned_edge=DOWN)
                bars.add(bar)
                
                # Add value label
                value_label = Text(f"{value}%", font_size=16)
                value_label.next_to(bar, UP, buff=0.1)
                bars.add(value_label)
        
        # Add legend
        legend = VGroup()
        
        for i in range(1, 4):
            color = bar_colors[i]
            square = Square(side_length=0.2, fill_color=color, fill_opacity=0.8)
            label = Text(f"{i} iteration{'s' if i > 1 else ''}", font_size=16)
            label.next_to(square, RIGHT, buff=0.2)
            group = VGroup(square, label)
            
            if i > 1:
                group.next_to(legend[-1], RIGHT, buff=0.5)
            
            legend.add(group)
        
        legend.arrange(RIGHT, buff=0.5)
        legend.next_to(axes, UP, buff=0.5)
        
        return VGroup(axes, x_label, y_label, bars, legend)

class MaskedPretraining(Scene):
    """Explaining the masked pretraining technique."""
    
    def construct(self):
        # Title
        title = Text("Self-Supervised Masked Pretraining", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Explain the concept
        concept = Text(
            "Train the model to reconstruct masked methylation values\nbefore fine-tuning for classification",
            font_size=36,
            line_spacing=1.2
        )
        concept.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(concept, run_time=2))
        self.wait()
        
        # Show the masking process
        masking_title = Text("Methylation Value Masking Process:", font_size=32)
        masking_title.next_to(concept, DOWN, buff=0.8)
        
        self.play(Write(masking_title))
        
        masking_viz = self.create_masking_visualization()
        masking_viz.next_to(masking_title, DOWN, buff=0.4)
        
        self.play(Create(masking_viz, run_time=2))
        self.wait()
        
        # Show the loss function
        loss_title = Text("Pretraining Loss Function:", font_size=32)
        loss_title.next_to(masking_viz, DOWN, buff=0.8)
        
        self.play(Write(loss_title))
        
        loss_formula = MathTex(
            r"\mathcal{L}_{\text{MSE}} = \frac{1}{\sum_{i,j} M_{ij}} \sum_{i,j} M_{ij} (\hat{x}_{ij} - x_{ij})^2",
            font_size=32
        )
        loss_formula.next_to(loss_title, DOWN, buff=0.4)
        
        self.play(Write(loss_formula))
        self.wait()
        
        # Show pretraining curve
        curve_title = Text("Pretraining Convergence:", font_size=32)
        curve_title.next_to(loss_formula, DOWN, buff=0.8)
        
        self.play(Write(curve_title))
        
        pretrain_curve = self.create_pretrain_curve()
        pretrain_curve.next_to(curve_title, DOWN, buff=0.4)
        
        self.play(Create(pretrain_curve, run_time=2))
        self.wait(2)
        
        # Explain benefits
        self.play(
            FadeOut(masking_viz),
            FadeOut(loss_title),
            FadeOut(loss_formula),
            FadeOut(curve_title),
            FadeOut(pretrain_curve)
        )
        
        benefits_title = Text("Benefits of Masked Pretraining:", font_size=36)
        benefits_title.next_to(masking_title, DOWN, buff=0.8)
        
        self.play(Write(benefits_title))
        
        benefits = self.create_benefits_viz()
        benefits.next_to(benefits_title, DOWN, buff=0.4)
        
        self.play(Create(benefits, run_time=2))
        self.wait()
        
        # Results comparing with and without pretraining
        results_title = Text("Impact on Classification Performance:", font_size=32)
        results_title.next_to(benefits, DOWN, buff=0.8)
        
        self.play(Write(results_title))
        
        results = self.create_results_comparison()
        results.next_to(results_title, DOWN, buff=0.4)
        
        self.play(Create(results))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(concept),
            FadeOut(masking_title),
            FadeOut(benefits_title),
            FadeOut(benefits),
            FadeOut(results_title),
            FadeOut(results),
            run_time=1.5
        )
    
    def create_masking_visualization(self):
        """Create a visualization of the masking process."""
        # Original methylation values
        original_values = np.random.rand(10)
        original_vec = self.create_methylation_vector(original_values, "Original")
        
        # Masked values (replace 15% with special token)
        masked_values = original_values.copy()
        mask_indices = [1, 5, 8]  # 30% just for visualization clarity
        for i in mask_indices:
            masked_values[i] = 0
        
        # Arrow
        arrow = Arrow(UP * 0.5, DOWN * 0.5, buff=0.3)
        arrow.next_to(original_vec, DOWN, buff=0.5)
        
        arrow_label = Text("Mask 15% of values", font_size=24)
        arrow_label.next_to(arrow, RIGHT, buff=0.3)
        
        # Masked vector
        masked_vec = self.create_methylation_vector(masked_values, "Masked", mask_indices)
        masked_vec.next_to(arrow, DOWN, buff=0.5)
        
        # Transformer prediction
        prediction_arrow = Arrow(UP * 0.5, DOWN * 0.5, buff=0.3)
        prediction_arrow.next_to(masked_vec, DOWN, buff=0.5)
        
        arrow_label2 = Text("Transformer predicts", font_size=24)
        arrow_label2.next_to(prediction_arrow, RIGHT, buff=0.3)
        
        # Predicted vector
        predicted_values = original_values.copy()
        # Add some error to predictions
        for i in mask_indices:
            predicted_values[i] = original_values[i] + (np.random.rand() - 0.5) * 0.2
            predicted_values[i] = max(0, min(1, predicted_values[i]))  # Clamp to [0,1]
        
        predicted_vec = self.create_methylation_vector(predicted_values, "Predicted", mask_indices, prediction=True)
        predicted_vec.next_to(prediction_arrow, DOWN, buff=0.5)
        
        return VGroup(
            original_vec, arrow, arrow_label, masked_vec,
            prediction_arrow, arrow_label2, predicted_vec
        )
    
    def create_methylation_vector(self, values, label_text, mask_indices=None, prediction=False):
        """Create a visualization of a methylation vector."""
        n_values = len(values)
        square_size = 0.4
        gap = 0.05
        
        vector = VGroup()
        
        # Create squares with values
        for i, val in enumerate(values):
            color = self.get_methylation_color(val)
            square = Square(side_length=square_size)
            
            if mask_indices is not None and i in mask_indices:
                if prediction:
                    square.set_fill(color, opacity=0.8)
                    square.set_stroke(YELLOW, width=3)  # Highlight predictions
                else:
                    square.set_fill(GRAY, opacity=0.5)
                    square.set_stroke(WHITE, width=1)
            else:
                square.set_fill(color, opacity=0.8)
                square.set_stroke(WHITE, width=1)
            
            square.move_to([i * (square_size + gap), 0, 0])
            
            # Add value text
            if mask_indices is not None and i in mask_indices and not prediction:
                value_text = Text("?", font_size=18)
            else:
                value_text = Text(f"{val:.2f}", font_size=12)
            
            value_text.move_to(square.get_center())
            
            vector.add(VGroup(square, value_text))
        
        # Add vector label
        vector_label = Text(label_text, font_size=24)
        vector_label.next_to(vector, LEFT, buff=0.5)
        
        return VGroup(vector, vector_label)
    
    def get_methylation_color(self, value):
        """Map methylation value to color (blue to red gradient)."""
        r = value
        g = 0.2
        b = 1 - value
        return rgb_to_color([r, g, b])
    
    def create_pretrain_curve(self):
        """Create a visualization of the pretraining convergence curve."""
        # Create axes
        axes = Axes(
            x_range=[0, 30, 5],
            y_range=[0, 1.2, 0.2],
            x_length=6,
            y_length=3,
            axis_config={"color": WHITE}
        )
        
        # Labels
        x_label = Text("Epochs", font_size=24)
        x_label.next_to(axes, DOWN, buff=0.3)
        
        y_label = Text("MSE Loss", font_size=24)
        y_label.next_to(axes, LEFT, buff=0.3).rotate(PI/2)
        
        # Create curve
        def loss_function(x):
            return 1.1 * np.exp(-0.15 * x) + 0.08
        
        x_vals = np.linspace(0, 30, 100)
        y_vals = [loss_function(x) for x in x_vals]
        
        curve = VGroup()
        
        for i in range(len(x_vals) - 1):
            line = Line(
                axes.c2p(x_vals[i], y_vals[i]),
                axes.c2p(x_vals[i+1], y_vals[i+1]),
                color=BLUE,
                stroke_width=3
            )
            curve.add(line)
        
        # Add point showing convergence
        converge_point = Dot(axes.c2p(30, y_vals[-1]), color=YELLOW, radius=0.08)
        
        converge_label = Text("Final Loss: 0.00009", font_size=20, color=YELLOW)
        converge_label.next_to(converge_point, UP, buff=0.2)
        
        # Add title
        title = Text("Reconstruction MSE during Pretraining", font_size=28)
        title.to_edge(UP, buff=0.2)
        
        return VGroup(axes, x_label, y_label, curve, converge_point, converge_label)
    
    def create_benefits_viz(self):
        """Create a visualization of the benefits of masked pretraining."""
        # Create a list of benefits
        benefits = VGroup(
            Text("1. Learns biologically meaningful patterns without labels", font_size=28),
            Text("2. Initializes weights to better starting point", font_size=28),
            Text("3. Learns co-methylation patterns between CpG sites", font_size=28),
            Text("4. Can leverage unlabeled methylation data", font_size=28)
        )
        
        benefits.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        
        # Highlight key words
        highlights = VGroup()
        
        keywords = [
            ["biologically", "patterns"],
            ["better", "starting point"],
            ["co-methylation", "patterns"],
            ["unlabeled", "data"]
        ]
        
        for i, txt in enumerate(benefits):
            for keyword in keywords[i]:
                # Find the keyword in the text
                txt_str = txt.text
                start_idx = txt_str.find(keyword)
                if start_idx != -1:
                    # Calculate the position of the keyword in the text
                    char_indices = [j for j, char in enumerate(txt_str) if char == keyword[0]]
                    closest_idx = min(char_indices, key=lambda x: abs(x - start_idx))
                    
                    # Create a highlight rectangle
                    highlight = Rectangle(
                        width=len(keyword) * 0.18,  # Approximate width per character
                        height=0.3,
                        color=YELLOW,
                        fill_opacity=0.3
                    )
                    
                    # Position the highlight over the keyword
                    highlight.move_to(txt.submobjects[closest_idx:closest_idx+len(keyword)])
                    # Position the highlight over the keyword
                    highlight.move_to(txt[closest_idx:closest_idx+len(keyword)])
                    highlights.add(highlight)
        
        return VGroup(benefits, highlights)
    
    def create_results_comparison(self):
        """Create a bar chart comparing results with and without pretraining."""
        # Create axes
        axes = Axes(
            x_range=[0, 3, 1],
            y_range=[0, 100, 20],
            x_length=8,
            y_length=4,
            axis_config={"color": WHITE}
        )
        
        # Labels
        x_label = Text("Model Variant", font_size=24)
        x_label.next_to(axes, DOWN, buff=0.3)
        
        y_label = Text("Accuracy (%)", font_size=24)
        y_label.next_to(axes, LEFT, buff=0.3).rotate(PI/2)
        
        # Data
        data = [89.0, 92.0, 97.06]  # No MoE/ACT, No Pretraining, Full Model
        bar_labels = ["Standard\nTransformer", "Without\nPretraining", "With\nPretraining"]
        
        # Create bars
        bars = VGroup()
        bar_colors = [BLUE_E, BLUE, GREEN]
        
        for i, (value, label) in enumerate(zip(data, bar_labels)):
            bar = Rectangle(
                width=0.6,
                height=value * 4 / 100,  # Scale to match y-axis
                fill_opacity=0.8,
                fill_color=bar_colors[i],
                stroke_color=WHITE
            )
            bar.move_to(axes.c2p(i+0.5, value/2), aligned_edge=DOWN)
            bars.add(bar)
            
            # Add value label
            value_label = Text(f"{value}%", font_size=20)
            value_label.next_to(bar, UP, buff=0.1)
            
            # Add bar label
            bar_label = Text(label, font_size=20, line_spacing=0.8)
            bar_label.next_to(bar, DOWN, buff=0.5)
            
            bars.add(value_label, bar_label)
        
        # Highlight the improvement
        improvement = Text("+5.06% from pretraining", font_size=24, color=GREEN)
        improvement.next_to(bars, UP, buff=0.5)
        
        # Add arrow showing improvement
        improvement_arrow = Arrow(
            bars[5].get_top() + UP * 0.3,  # Third bar's label
            bars[8].get_top() + UP * 0.3,  # Last bar's label
            color=GREEN,
            buff=0.1
        )
        improvement_arrow.next_to(improvement, DOWN, buff=0.2)
        
        return VGroup(axes, x_label, y_label, bars, improvement, improvement_arrow)

class ResultsAndConclusion(Scene):
    """Presenting the results and conclusion of the research."""
    
    def construct(self):
        # Title
        title = Text("Results and Biological Insights", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Performance results
        results_title = Text("Classification Performance", font_size=36)
        results_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(results_title))
        
        results_table = self.create_results_table()
        results_table.next_to(results_title, DOWN, buff=0.5)
        
        self.play(Create(results_table, run_time=2))
        self.wait()
        
        # Show the confusion matrix
        matrix_title = Text("Confusion Matrix", font_size=32)
        matrix_title.next_to(results_table, DOWN, buff=0.8)
        
        self.play(Write(matrix_title))
        
        confusion_matrix = self.create_confusion_matrix()
        confusion_matrix.next_to(matrix_title, DOWN, buff=0.4)
        
        self.play(Create(confusion_matrix))
        self.wait(2)
        
        # Transition to biological insights
        self.play(
            FadeOut(results_title),
            FadeOut(results_table),
            FadeOut(matrix_title),
            FadeOut(confusion_matrix)
        )
        
        # Biological insights
        insights_title = Text("Key Epigenetic Biomarkers Discovered", font_size=36)
        insights_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(insights_title))
        
        gene_insights = self.create_gene_insights()
        gene_insights.next_to(insights_title, DOWN, buff=0.5)
        
        self.play(Create(gene_insights, run_time=2))
        self.wait(2)
        
        # Attention visualization
        attention_title = Text("Attention Map Visualization", font_size=32)
        attention_title.next_to(gene_insights, DOWN, buff=0.8)
        
        self.play(Write(attention_title))
        
        attention_map = self.create_attention_map()
        attention_map.next_to(attention_title, DOWN, buff=0.4)
        
        self.play(Create(attention_map, run_time=2))
        self.wait(2)
        
        # Transition to conclusion
        self.play(
            FadeOut(insights_title),
            FadeOut(gene_insights),
            FadeOut(attention_title),
            FadeOut(attention_map)
        )
        
        # Conclusion
        conclusion_title = Text("Conclusions and Impact", font_size=36)
        conclusion_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(conclusion_title))
        
        conclusions = self.create_conclusions()
        conclusions.next_to(conclusion_title, DOWN, buff=0.5)
        
        self.play(Create(conclusions, run_time=2))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(conclusion_title),
            FadeOut(conclusions),
            run_time=1.5
        )
    
    def create_results_table(self):
        """Create a table of classification results."""
        # Headers
        headers = ["Model", "Accuracy", "Macro F1", "Macro AUROC"]
        models = [
            "Logistic Regression",
            "Random Forest",
            "XGBoost",
            "Transformer (ours)"
        ]
        accuracy = ["75.0%", "78.3%", "80.0%", "97.06%"]
        f1 = ["0.73", "0.76", "0.78", "0.95"]
        auroc = ["0.80", "0.82", "0.85", "0.98"]
        
        # Create table
        table = VGroup()
        
        # Header row
        header_row = VGroup()
        for i, text in enumerate(headers):
            cell = Rectangle(width=3, height=0.8)
            cell.set_stroke(WHITE, width=1)
            
            if i > 0:
                cell.next_to(header_row[-1], RIGHT, buff=0)
            
            cell_text = Text(text, font_size=24)
            cell_text.move_to(cell.get_center())
            
            header_row.add(VGroup(cell, cell_text))
        
        table.add(header_row)
        
        # Data rows
        for i in range(len(models)):
            row_data = [models[i], accuracy[i], f1[i], auroc[i]]
            row = VGroup()
            
            for j, text in enumerate(row_data):
                cell = Rectangle(width=3, height=0.8)
                cell.set_stroke(WHITE, width=1)
                
                if j == 0:
                    cell.next_to(table[-1][0], DOWN, buff=0)
                else:
                    cell.next_to(row[-1], RIGHT, buff=0)
                
                # Color the best result
                if i == len(models) - 1 and j > 0:
                    cell_text = Text(text, font_size=24, color=GREEN)
                else:
                    cell_text = Text(text, font_size=24)
                
                cell_text.move_to(cell.get_center())
                
                row.add(VGroup(cell, cell_text))
            
            table.add(row)
        
        return table
    
    def create_confusion_matrix(self):
        """Create a visualization of the confusion matrix."""
        # Matrix data (rows: true class, columns: predicted class)
        # Format: [[MECFS->MECFS, MECFS->LC, MECFS->Control],
        #          [LC->MECFS, LC->LC, LC->Control],
        #          [Control->MECFS, Control->LC, Control->Control]]
        matrix_data = [
            [4, 1, 0],
            [1, 4, 0],
            [0, 0, 10]
        ]
        
        # Class labels
        class_labels = ["ME/CFS", "Long COVID", "Control"]
        
        # Create the matrix
        matrix = VGroup()
        
        # Add row and column headers
        row_headers = VGroup()
        for i, label in enumerate(class_labels):
            header = Text(f"True: {label}", font_size=20)
            header.to_edge(LEFT, buff=1)
            if i > 0:
                header.next_to(row_headers[-1], DOWN, buff=0.8)
            row_headers.add(header)
        
        col_headers = VGroup()
        for i, label in enumerate(class_labels):
            header = Text(f"Predicted: {label}", font_size=20)
            if i > 0:
                header.next_to(col_headers[-1], RIGHT, buff=1.2)
            col_headers.add(header)
        col_headers.arrange(RIGHT, buff=1.2)
        col_headers.next_to(row_headers, UP, buff=0.8)
        
        # Create cells
        cells = VGroup()
        for i in range(len(class_labels)):
            row = VGroup()
            for j in range(len(class_labels)):
                # Set cell color based on correct/incorrect prediction
                if i == j:
                    color = GREEN_E  # Correct prediction
                    opacity = 0.7
                else:
                    color = RED_E  # Incorrect prediction
                    opacity = 0.5
                
                cell = Square(side_length=1)
                cell.set_fill(color, opacity=opacity)
                cell.set_stroke(WHITE, width=1)
                
                # Position cell
                if j > 0:
                    cell.next_to(row[-1], RIGHT, buff=0.2)
                else:
                    cell.next_to(row_headers[i], RIGHT, buff=0.5)
                
                # Add count text
                count_text = Text(str(matrix_data[i][j]), font_size=28)
                count_text.move_to(cell.get_center())
                
                row.add(VGroup(cell, count_text))
            
            cells.add(row)
        
        # Add a note about the confusion matrix
        note = Text("Note: No controls misclassified as patients (high specificity)", 
                   font_size=20, color=GREEN)
        note.next_to(cells, DOWN, buff=0.8)
        
        return VGroup(row_headers, col_headers, cells, note)
    
    def create_gene_insights(self):
        """Create a visualization of the gene/biological insights."""
        # Create a table of top genes and their methylation patterns
        table = VGroup()
        
        # Headers
        headers = ["Gene", "Function", "ME/CFS Pattern", "Long COVID Pattern"]
        header_row = VGroup()
        
        for i, text in enumerate(headers):
            cell = Rectangle(width=3, height=0.8)
            cell.set_stroke(WHITE, width=1)
            
            if i > 0:
                cell.next_to(header_row[-1], RIGHT, buff=0)
            
            cell_text = Text(text, font_size=20)
            cell_text.move_to(cell.get_center())
            
            header_row.add(VGroup(cell, cell_text))
        
        table.add(header_row)
        
        # Gene data
        genes = [
            "HLA-DRB1",
            "NR3C1",
            "IFNG",
            "IFITM3",
            "PDK2"
        ]
        
        functions = [
            "Immune regulation",
            "Stress response",
            "Interferon signaling",
            "Viral defense",
            "Metabolism"
        ]
        
        mecfs_patterns = [
            "Hypomethylated",
            "Hypomethylated (β=0.55)",
            "Differentially methylated",
            "No significant change",
            "Hypermethylated"
        ]
        
        lc_patterns = [
            "Mixed pattern",
            "Variable methylation",
            "Hypomethylated",
            "Hypomethylated (β=0.48)",
            "No significant change"
        ]
        
        # Create rows
        for i in range(len(genes)):
            row_data = [genes[i], functions[i], mecfs_patterns[i], lc_patterns[i]]
            row = VGroup()
            
            for j, text in enumerate(row_data):
                cell = Rectangle(width=3, height=0.8)
                cell.set_stroke(WHITE, width=1)
                
                if j == 0:
                    cell.next_to(table[-1][0], DOWN, buff=0)
                else:
                    cell.next_to(row[-1], RIGHT, buff=0)
                
                # Color the gene names
                if j == 0:
                    cell_text = Text(text, font_size=20, color=GENE_COLOR)
                else:
                    cell_text = Text(text, font_size=18)
                
                cell_text.move_to(cell.get_center())
                
                row.add(VGroup(cell, cell_text))
            
            table.add(row)
        
        # Add interpretation
        interpretation = Text(
            "The model identified key methylation differences between ME/CFS and Long COVID",
            font_size=24
        )
        interpretation.next_to(table, DOWN, buff=0.5)
        
        return VGroup(table, interpretation)
    
    def create_attention_map(self):
        """Create a visualization of attention maps."""
        # Create a heatmap-like grid for attention visualization
        grid_size = 8
        cell_size = 0.3
        
        # Generate some sample attention patterns
        # Higher number = stronger attention
        attention_data = np.zeros((grid_size, grid_size))
        
        # Pattern 1: Focused attention on specific regions (for head 1)
        attention_data[1:3, 2:4] = np.random.uniform(0.7, 0.9, (2, 2))
        
        # Pattern 2: Diffuse attention (for head 5)
        attention_data[3:5, :] = np.random.uniform(0.3, 0.5, (2, grid_size))
        
        # Pattern 3: Some strong connections between distant regions (for head 2)
        attention_data[6, 1] = 0.8
        attention_data[6, 5] = 0.9
        attention_data[2, 7] = 0.85
        
        # Create the attention map grid
        attention_grid = VGroup()
        
        for i in range(grid_size):
            for j in range(grid_size):
                opacity = attention_data[i, j]
                cell = Square(side_length=cell_size)
                cell.set_fill(YELLOW, opacity=opacity)
                cell.set_stroke(WHITE, width=0.5)
                cell.move_to([j*cell_size, -i*cell_size, 0])
                
                attention_grid.add(cell)
        
        # Add labels for attention heads
        head_labels = VGroup()
        for i in range(grid_size):
            label = Text(f"Head {i+1}", font_size=16)
            label.next_to(attention_grid[i*grid_size], LEFT, buff=0.3)
            head_labels.add(label)
        
        # Add feature indices
        feature_labels = VGroup()
        for j in range(grid_size):
            label = Text(f"F{j+1}", font_size=16)
            label.next_to(attention_grid[j], UP, buff=0.3)
            feature_labels.add(label)
        
        # Add explanations for specific patterns
        explanations = VGroup()
        
        explanation1 = Text("Head 2: Focuses on immune gene clusters", font_size=20, color=GREEN)
        explanation1.to_edge(RIGHT, buff=1)
        explanation1.shift(UP * 1.5)
        
        explanation2 = Text("Head 5: Broad scanning for anomalies", font_size=20, color=BLUE)
        explanation2.next_to(explanation1, DOWN, buff=0.5)
        
        explanation3 = Text("Head 7: Stress-response gene focus", font_size=20, color=RED)
        explanation3.next_to(explanation2, DOWN, buff=0.5)
        
        explanations.add(explanation1, explanation2, explanation3)
        
        # Connect explanations to relevant parts of the grid
        arrows = VGroup()
        
        arrow1 = Arrow(explanation1.get_left(), attention_grid[6*grid_size + 1].get_center(), buff=0.1)
        arrow2 = Arrow(explanation2.get_left(), attention_grid[4*grid_size + 3].get_center(), buff=0.1)
        arrow3 = Arrow(explanation3.get_left(), attention_grid[7*grid_size + 5].get_center(), buff=0.1)
        
        arrows.add(arrow1, arrow2, arrow3)
        
        # Add title
        title = Text("Attention Map for ME/CFS Sample", font_size=24)
        title.next_to(feature_labels, UP, buff=0.5)
        
        return VGroup(attention_grid, head_labels, feature_labels, explanations, arrows, title)
    
    def create_conclusions(self):
        """Create a visualization summarizing the conclusions."""
        # Create a list of conclusions
        conclusions = VGroup(
            Text("1. Transformer architecture effectively models methylation patterns", font_size=28),
            Text("2. 97.06% accuracy in distinguishing ME/CFS, Long COVID, and controls", font_size=28),
            Text("3. Identified distinct epigenetic signatures for each condition", font_size=28),
            Text("4. Interpretable results provide mechanistic insights", font_size=28),
            Text("5. Correlation with clinical metrics suggests potential diagnostic use", font_size=28)
        )
        
        conclusions.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        
        # Highlight key points
        highlights = VGroup()
        
        keywords = [
            ["Transformer"],
            ["97.06%", "accuracy"],
            ["distinct", "epigenetic signatures"],
            ["Interpretable", "mechanistic insights"],
            ["diagnostic"]
        ]
        
        for i, txt in enumerate(conclusions):
            for keyword in keywords[i]:
                # Find the keyword in the text
                txt_str = txt.text
                start_idx = txt_str.find(keyword)
                if start_idx != -1:
                    # Create a highlight rectangle
                    highlight = Rectangle(
                        width=len(keyword) * 0.18,  # Approximate width per character
                        height=0.3,
                        color=YELLOW,
                        fill_opacity=0.3
                    )
                    
                    # Find the position of the submobject
                    # This is an approximation, in actual code you'd need to locate the exact submobjects
                    highlight.move_to(txt.get_center())
                    highlight.shift(RIGHT * (start_idx - len(txt_str)/2) * 0.1)
                    
                    highlights.add(highlight)
        
        # Add a visual summary
        summary_rect = Rectangle(width=8, height=1.5, color=GREEN)
        summary_rect.next_to(conclusions, DOWN, buff=0.8)
        
        summary_text = Text(
            "This work demonstrates how advanced mathematics can extract\n" +
            "meaningful biological patterns from complex epigenetic data",
            font_size=24,
            line_spacing=1.2
        )
        summary_text.move_to(summary_rect.get_center())
        
        return VGroup(conclusions, highlights, summary_rect, summary_text)

class MathematicalInsights(Scene):
    """Connecting the mathematics to biological insights."""
    
    def construct(self):
        # Title
        title = Text("From Mathematical Models to Biological Insights", font_size=48)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Show the mathematical pipeline
        pipeline_title = Text("Mathematical Pipeline:", font_size=36)
        pipeline_title.next_to(title, DOWN, buff=0.8).to_edge(LEFT)
        
        self.play(Write(pipeline_title))
        
        pipeline = self.create_math_pipeline()
        pipeline.next_to(pipeline_title, DOWN, buff=0.5)
        
        self.play(Create(pipeline, run_time=3))
        self.wait()
        
        # Key mathematical insights
        insights_title = Text("Key Mathematical Insights:", font_size=36)
        insights_title.next_to(pipeline, DOWN, buff=0.8)
        
        self.play(Write(insights_title))
        
        insights = self.create_math_insights()
        insights.next_to(insights_title, DOWN, buff=0.5)
        
        self.play(Create(insights, run_time=2))
        self.wait(2)
        
        # Biological interpretation
        self.play(
            FadeOut(pipeline_title),
            FadeOut(pipeline),
            FadeOut(insights_title),
            FadeOut(insights)
        )
        
        bio_title = Text("Biological Interpretation:", font_size=36)
        bio_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(bio_title))
        
        bio_interpretation = self.create_bio_interpretation()
        bio_interpretation.next_to(bio_title, DOWN, buff=0.5)
        
        self.play(Create(bio_interpretation, run_time=2))
        self.wait(2)
        
        # Final thoughts
        final_title = Text("The Power of Mathematical Modeling:", font_size=36)
        final_title.next_to(bio_interpretation, DOWN, buff=0.8)
        
        self.play(Write(final_title))
        
        final_thoughts = self.create_final_thoughts()
        final_thoughts.next_to(final_title, DOWN, buff=0.5)
        
        self.play(Write(final_thoughts))
        self.wait(2)
        
        self.play(
            FadeOut(title),
            FadeOut(bio_title),
            FadeOut(bio_interpretation),
            FadeOut(final_title),
            FadeOut(final_thoughts),
            run_time=1.5
        )
    
    def create_math_pipeline(self):
        """Create a visualization of the mathematical pipeline."""
        # Create a flowchart of mathematical steps
        flowchart = VGroup()
        
        # Blocks
        block1 = Rectangle(width=4, height=0.8, color=BLUE)
        block1_text = Text("Methylation Data Embedding", font_size=20)
        block1_text.move_to(block1.get_center())
        block1_group = VGroup(block1, block1_text)
        
        block2 = Rectangle(width=4, height=0.8, color=ATTENTION_COLOR)
        block2_text = Text("Self-Attention Mechanism", font_size=20)
        block2_text.move_to(block2.get_center())
        block2_group = VGroup(block2, block2_text)
        block2_group.next_to(block1_group, DOWN, buff=0.5)
        
        block3 = Rectangle(width=4, height=0.8, color=EXPERT_COLOR)
        block3_text = Text("Mixture-of-Experts FFN", font_size=20)
        block3_text.move_to(block3.get_center())
        block3_group = VGroup(block3, block3_text)
        block3_group.next_to(block2_group, DOWN, buff=0.5)
        
        block4 = Rectangle(width=4, height=0.8, color=RED)
        block4_text = Text("Adaptive Computation Time", font_size=20)
        block4_text.move_to(block4.get_center())
        block4_group = VGroup(block4, block4_text)
        block4_group.next_to(block3_group, DOWN, buff=0.5)
        
        block5 = Rectangle(width=4, height=0.8, color=GREEN)
        block5_text = Text("Classification Output", font_size=20)
        block5_text.move_to(block5.get_center())
        block5_group = VGroup(block5, block5_text)
        block5_group.next_to(block4_group, DOWN, buff=0.5)
        
        # Connect with arrows
        arrows = VGroup()
        for i in range(4):
            block_pairs = [
                (block1_group, block2_group),
                (block2_group, block3_group),
                (block3_group, block4_group),
                (block4_group, block5_group)
            ]
            arrow = Arrow(block_pairs[i][0].get_bottom(), block_pairs[i][1].get_top(), buff=0.1)
            arrows.add(arrow)
        
        # Add mathematical formulas next to blocks
        formulas = VGroup()
        
        formula1 = MathTex(r"Z^0 = E + P", font_size=24)
        formula1.next_to(block1_group, RIGHT, buff=1)
        
        formula2 = MathTex(r"A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)", font_size=24)
        formula2.next_to(block2_group, RIGHT, buff=1)
        
        formula3 = MathTex(r"\text{MoE-FFN}(x) = \sum_{e=1}^{4} g_e(x) \cdot \text{FFN}_e(x)", font_size=24)
        formula3.next_to(block3_group, RIGHT, buff=1)
        
        formula4 = MathTex(r"p = \sigma(w^T x_{\text{mean}})", font_size=24)
        formula4.next_to(block4_group, RIGHT, buff=1)
        
        formula5 = MathTex(r"y = \text{softmax}(W_{\text{cls}_2}\text{GELU}(W_{\text{cls}_1} x_{\text{mean}}))", font_size=22)
        formula5.next_to(block5_group, RIGHT, buff=1)
        
        formulas.add(formula1, formula2, formula3, formula4, formula5)
        
        flowchart.add(block1_group, block2_group, block3_group, block4_group, block5_group, arrows, formulas)
        
        return flowchart
    
    def create_math_insights(self):
        """Create a visualization highlighting key mathematical insights."""
        # Create a list of insights
        insights = VGroup(
            Text("1. Self-attention creates a similarity matrix between CpG sites", font_size=24),
            Text("2. Multi-head attention allows parallel focus on diverse patterns", font_size=24),
            Text("3. Mixture-of-Experts enables specialization in different pathways", font_size=24),
            Text("4. Adaptive Computation allocates more processing to ambiguous cases", font_size=24)
        )
        
        insights.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        
        # Add mathematical representations next to each insight
        math_viz = VGroup()
        
        # 1. Self-attention similarity matrix
        matrix_grid = VGroup()
        for i in range(4):
            for j in range(4):
                # Higher similarity for nearby positions
                opacity = 1 - 0.2 * abs(i - j)
                cell = Square(side_length=0.3)
                cell.set_fill(YELLOW, opacity=opacity)
                cell.set_stroke(WHITE, width=0.5)
                cell.move_to([j*0.3, -i*0.3, 0])
                matrix_grid.add(cell)
        
        matrix_grid.next_to(insights[0], RIGHT, buff=1)
        math_viz.add(matrix_grid)
        
        # 2. Multi-head attention
        heads = VGroup()
        for i in range(2):
            for j in range(4):
                head = Circle(radius=0.15)
                head.set_fill(BLUE, opacity=0.8)
                head.set_stroke(WHITE, width=0.5)
                head.move_to([j*0.4, -i*0.4, 0])
                heads.add(head)
        
        heads.next_to(insights[1], RIGHT, buff=1)
        math_viz.add(heads)
        
        # 3. Mixture-of-Experts
        experts = VGroup()
        for i in range(4):
            expert = Rectangle(width=0.3, height=0.6)
            expert.set_fill(EXPERT_COLOR, opacity=0.8)
            expert.set_stroke(WHITE, width=0.5)
            expert.move_to([i*0.4, 0, 0])
            experts.add(expert)
        
        gate = Triangle().scale(0.2)
        gate.set_fill(RED, opacity=0.8)
        gate.set_stroke(WHITE, width=0.5)
        gate.next_to(experts, UP, buff=0.3)
        
        moe_group = VGroup(experts, gate)
        moe_group.next_to(insights[2], RIGHT, buff=1)
        math_viz.add(moe_group)
        
        # 4. Adaptive Computation
        act_blocks = VGroup()
        n_blocks = [1, 2, 3]  # Different iterations
        for i, n in enumerate(n_blocks):
            blocks = VGroup()
            for j in range(n):
                block = Rectangle(width=0.3, height=0.3)
                block.set_fill(GREEN, opacity=0.8 - j*0.2)
                block.set_stroke(WHITE, width=0.5)
                block.move_to([j*0.4, 0, 0])
                blocks.add(block)
            
            blocks.next_to(insights[3], RIGHT, buff=1 + i*0.8)
            act_blocks.add(blocks)
        
        math_viz.add(act_blocks)
        
        return VGroup(insights, math_viz)
    
    def create_bio_interpretation(self):
        """Create a visualization linking mathematical models to biological interpretation."""
        # Create two columns: Math Model vs Biological Meaning
        table = VGroup()
        
        # Headers
        header1 = Rectangle(width=5, height=0.8, color=BLUE)
        header1_text = Text("Mathematical Component", font_size=24)
        header1_text.move_to(header1.get_center())
        header1_group = VGroup(header1, header1_text)
        
        header2 = Rectangle(width=5, height=0.8, color=GREEN)
        header2_text = Text("Biological Interpretation", font_size=24)
        header2_text.move_to(header2.get_center())
        header2_group = VGroup(header2, header2_text)
        header2_group.next_to(header1_group, RIGHT, buff=0)
        
        table.add(VGroup(header1_group, header2_group))
        
        # Row data
        math_components = [
            "Self-Attention Weights",
            "Multi-Head Attention",
            "Mixture-of-Experts",
            "Adaptive Computation"
        ]
        
        bio_interpretations = [
            "Correlation between CpG sites in regulatory regions",
            "Parallel processing of different gene pathways",
            "Specialized handling of immune vs metabolic patterns",
            "More analysis for borderline cases between conditions"
        ]
        
        # Create rows
        for i in range(len(math_components)):
            # Math component cell
            cell1 = Rectangle(width=5, height=0.8)
            cell1.set_stroke(WHITE, width=1)
            cell1.next_to(table[-1][0], DOWN, buff=0)
            
            cell1_text = Text(math_components[i], font_size=20)
            cell1_text.move_to(cell1.get_center())
            cell1_group = VGroup(cell1, cell1_text)
            
            # Bio interpretation cell
            cell2 = Rectangle(width=5, height=0.8)
            cell2.set_stroke(WHITE, width=1)
            cell2.next_to(cell1, RIGHT, buff=0)
            
            cell2_text = Text(bio_interpretations[i], font_size=18)
            cell2_text.move_to(cell2.get_center())
            cell2_group = VGroup(cell2, cell2_text)
            
            table.add(VGroup(cell1_group, cell2_group))
        
        # Add visualization of a specific example
        example_title = Text("Example: Attention to NR3C1 Gene", font_size=28)
        example_title.next_to(table, DOWN, buff=0.8)
        
        # Create a simple visualization of attention to a gene
        gene_viz = Rectangle(width=6, height=1, color=GENE_COLOR, fill_opacity=0.2)
        gene_viz.next_to(example_title, DOWN, buff=0.4)
        
        gene_label = Text("NR3C1 Gene (Stress Response)", font_size=20, color=GENE_COLOR)
        gene_label.move_to(gene_viz.get_center())
        
        # Add CpG sites with attention
        cpg_sites = VGroup()
        attentions = [0.3, 0.8, 0.5, 0.9, 0.4]
        
        for i, att in enumerate(attentions):
            cpg = Circle(radius=0.2)
            cpg.set_fill(ATTENTION_COLOR, opacity=att)
            cpg.set_stroke(WHITE, width=1)
            cpg.move_to(gene_viz.get_center() + RIGHT * (i - 2) * 0.8)
            
            label = Text(f"CpG {i+1}", font_size=16)
            label.next_to(cpg, DOWN, buff=0.2)
            
            att_label = Text(f"{att:.1f}", font_size=16, color=ATTENTION_COLOR)
            att_label.next_to(cpg, UP, buff=0.2)
            
            cpg_sites.add(VGroup(cpg, label, att_label))
        
        explanation = Text(
            "The model pays highest attention to CpG sites in the promoter region,\n" +
            "which control gene expression of this stress hormone receptor",
            font_size=20,
            line_spacing=1
        )
        explanation.next_to(cpg_sites, DOWN, buff=0.5)
        
        return VGroup(table, example_title, gene_viz, gene_label, cpg_sites, explanation)
    
    def create_final_thoughts(self):
        """Create a visualization for final thoughts."""
        final_text = Text(
            "The transformer architecture creates a mathematical 'bridge'\n" +
            "between complex epigenetic patterns and clinical classifications,\n" +
            "demonstrating how advanced mathematics can unlock biological insights\n" +
            "that traditional statistical methods might miss.",
            font_size=28,
            line_spacing=1.2
        )
        
        return final_text

class Conclusion(Scene):
    """Final conclusion and takeaways."""
    
    def construct(self):
        # Title
        title = Text("Understanding the Epigenomic Transformer", font_size=56)
        title.to_edge(UP)
        self.play(Write(title))
        self.wait()
        
        # Key learnings
        learnings_title = Text("Key Mathematical Concepts", font_size=40)
        learnings_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(learnings_title))
        
        # List the key mathematical concepts
        concepts = VGroup(
            Text("1. Self-attention as a measure of feature similarity", font_size=32),
            Text("2. Multi-head mechanisms for parallel pattern recognition", font_size=32),
            Text("3. Mixture-of-Experts for specialized processing", font_size=32),
            Text("4. Adaptive computation for variable-depth processing", font_size=32),
            Text("5. Self-supervised pretraining for representation learning", font_size=32)
        )
        
        concepts.arrange(DOWN, aligned_edge=LEFT, buff=0.4)
        concepts.next_to(learnings_title, DOWN, buff=0.5)
        
        for concept in concepts:
            self.play(FadeIn(concept, shift=UP*0.2))
            self.wait(0.3)
        
        self.wait()
        
        # Final thoughts
        self.play(
            FadeOut(learnings_title),
            FadeOut(concepts)
        )
        
        final_title = Text("The Power of Mathematics in Medicine", font_size=44)
        final_title.next_to(title, DOWN, buff=0.8)
        
        self.play(Write(final_title))
        
        final_message = Text(
            "By modeling complex epigenetic patterns with transformer architectures,\n" +
            "we can achieve clinical-grade classification accuracy while gaining\n" +
            "interpretable insights into disease mechanisms. This approach\n" +
            "demonstrates how mathematical innovation drives medical discovery.",
            font_size=32,
            line_spacing=1.2
        )
        final_message.next_to(final_title, DOWN, buff=0.5)
        
        self.play(Write(final_message, run_time=2))
        self.wait(2)
        
        # 3Blue1Brown-style ending
        ending = Text("Thanks for watching", font_size=48)
        ending.to_edge(DOWN, buff=1)
        
        self.play(Write(ending))
        self.wait(3)
        
        self.play(
            FadeOut(title),
            FadeOut(final_title),
            FadeOut(final_message),
            FadeOut(ending),
            run_time=1.5
        )

# Main function to run the animation
if __name__ == "__main__":
    # Uncomment the scene you want to render
    # Note: For a full video, you would render all scenes and combine them
    
    # Introduction
    # os.system("manim -pqh manim_script.py Introduction")
    
    # Data representation
    # os.system("manim -pqh manim_script.py DataRepresentation")
    
    # Transformer overview
    # os.system("manim -pqh manim_script.py TransformerOverview")
    
    # Input embedding
    # os.system("manim -pqh manim_script.py InputEmbedding")
    
    # Self-attention
    # os.system("manim -pqh manim_script.py SelfAttentionMechanism")
    
    # Mixture of experts
    # os.system("manim -pqh manim_script.py MixtureOfExperts")
    
    # Adaptive computation time
    # os.system("manim -pqh manim_script.py AdaptiveComputationTime")
    
    # Masked pretraining
    # os.system("manim -pqh manim_script.py MaskedPretraining")
    
    # Results and conclusion
    # os.system("manim -pqh manim_script.py ResultsAndConclusion")
    
    # Mathematical insights
    # os.system("manim -pqh manim_script.py MathematicalInsights")
    
    # Final conclusion
    # os.system("manim -pqh manim_script.py Conclusion")
    
    # For rendering all scenes
    scenes = [
        "Introduction",
        "DataRepresentation",
        "TransformerOverview",
        "InputEmbedding",
        "SelfAttentionMechanism",
        "MixtureOfExperts",
        "AdaptiveComputationTime",
        "MaskedPretraining",
        "ResultsAndConclusion",
        "MathematicalInsights",
        "Conclusion"
    ]
    
    # Generate a full command to render all scenes
    full_command = "manim -pqh manim_script.py " + " ".join(scenes)
    print(f"To render all scenes, run: {full_command}")
    
    # Alternatively, render a specific scene
    # Choose which scene to render by uncommenting one of these:
    scene_to_render = "SelfAttentionMechanism"  # Example: just render self-attention
    # scene_to_render = "MixtureOfExperts"
    # scene_to_render = "AdaptiveComputationTime"
    
    render_command = f"manim -pqh {__file__} {scene_to_render}"
    print(f"Rendering scene: {scene_to_render}")
    print(f"Command: {render_command}")
    
    # Execute the command
    import subprocess
    subprocess.run(render_command, shell=True)

# ======================
# File: src/original/other/interactive_dashboard.py
# ======================

#!/usr/bin/env python3
"""
Dark-themed Dash dashboard for Mobius epigenomic pipeline.

Ensure you have installed:
  pip install dash dash-bootstrap-components dash-extensions plotly
Place custom CSS in assets/ for styling/animations if desired.
"""

import dash
from dash import dcc, html, Input, Output, State, ctx
import dash_bootstrap_components as dbc
import plotly.express as px
import os, time, json, base64, subprocess
import pandas as pd
from dash_extensions import EventListener  # for advanced scrolling events, etc.

##############################################
# Configuration
##############################################

"""Resolve repository root dynamically to avoid hard-coded paths."""
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, "..", "..", ".."))
RUN_PIPELINE_R = os.path.join(BASE_DIR, "src", "original", "run_pipeline_master.R")
RESULTS_DIR = os.path.join(BASE_DIR, "results")

# Store the last pipeline run output in-memory so the Logs page can display it
PIPELINE_OUTPUT = ""

def load_image_as_base64(path: str) -> str | None:
    try:
        if os.path.exists(path):
            with open(path, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
    except Exception:
        return None
    return None

# The theme is dark, we use LUX, MINTY, SLATE, or custom. We'll pick a dark theme:
external_stylesheets = [dbc.themes.SLATE]

app = dash.Dash(__name__, external_stylesheets=external_stylesheets, suppress_callback_exceptions=True)
server = app.server

##############################################
# Place a large custom CSS in assets/custom.css
# e.g. advanced animations, parallax effect:
#
#   .parallax-section {
#     height: 80vh;
#     background-attachment: fixed;
#     background-size: cover;
#     background-position: center;
#     background-repeat: no-repeat;
#   }
#
#   .hero-text {
#     font-size: 4rem;
#     color: #ccc;
#     margin-top: 30vh;
#     text-align: center;
#     animation: fadeInUp 2s ease forwards;
#     opacity: 0;
#   }
#
#   @keyframes fadeInUp {
#     to { opacity: 1; transform: translateY(0); }
#     from { opacity: 0; transform: translateY(50px); }
#   }
#
#   .transition-container {
#     transition: all 0.4s ease-in-out;
#   }
#
# etc.
##############################################

##############################################
# NAVBAR
##############################################
navbar = dbc.Navbar(
    dbc.Container([
        dbc.NavbarBrand("Epigenetics Dashboard", className="ms-2 text-uppercase", style={"fontWeight":"600"}),
        dbc.Button(
            html.Span(className="navbar-toggler-icon"),
            color="dark",
            outline=False,
            className="ms-auto d-md-none me-2",
            id="sidebar-toggle"
        )
    ]),
    dark=True,
    color="black",
    sticky="top",
    className="shadow-sm"
)

##############################################
# COLLAPSIBLE SIDEBAR
##############################################
sidebar = html.Div(
    id="sidebar",
    className="transition-container",  # we rely on custom.css for transitions
    style={
        "position":"fixed",
        "left":"0",
        "top":"56px",  # navbar height
        "bottom":"0",
        "width":"300px",
        "background":"#111",
        "color":"#aaa",
        "padding":"1rem",
        "overflowY":"auto",
        "zIndex":"9999"
    },
    children=[
        html.H4("Analysis Tools", style={"fontWeight":"bold", "marginBottom":"1rem"}),
        dbc.Button("Run Pipeline", id="run-pipeline-btn", color="danger", className="w-100 mb-2"),
        html.Div(id="run-pipeline-status", style={"color":"#f66","fontWeight":"bold","marginBottom":"1rem"}),
        html.Hr(style={"borderColor":"#333"}),
        dbc.Nav([
            dbc.NavLink("Home", href="/", id="nav-link-home", active=False),
            dbc.NavLink("Epigenetics", href="/epigenetics", id="nav-link-epi", active=False),
            dbc.NavLink("ME vs LC", href="/me-lc", id="nav-link-me-lc", active=False),
            dbc.NavLink("Logs", href="/logs", id="nav-link-logs", active=False),
        ], vertical=True, pills=True),
        html.Hr(style={"borderColor":"#333","marginTop":"1rem","marginBottom":"1rem"}),
        # Example config area
        html.H4("Config", style={"fontWeight":"bold","fontSize":"1rem","marginTop":"1rem"}),
        dbc.Label("Condition 1 Folder", style={"marginTop":"0.5rem"}),
        dbc.Input(id="cond1-input", type="text", placeholder="/path/to/ME"),
        dbc.Label("Condition 2 Folder", style={"marginTop":"0.5rem"}),
        dbc.Input(id="cond2-input", type="text", placeholder="/path/to/LC"),
        dbc.Label("Condition 3 Folder", style={"marginTop":"0.5rem"}),
        dbc.Input(id="cond3-input", type="text", placeholder="/path/to/Controls"),
        dbc.Button("Save Config", id="save-config-btn", color="secondary", className="w-100 mt-2"),
        html.Div(id="save-config-status", style={"color":"#afa","fontWeight":"bold","marginTop":"0.5rem"}),
    ]
)

##############################################
# PAGE CONTENTS
##############################################
def layout_home():
    # Parallax hero section
    hero = html.Div(
        className="parallax-section",
        style={
            "height":"100vh",
            "backgroundImage":"url('/assets/hero_bg.jpg')",  # place hero_bg.jpg in assets
            "backgroundAttachment":"fixed",
            "backgroundSize":"cover",
            "backgroundPosition":"center",
            "display":"flex",
            "alignItems":"center",
            "justifyContent":"center"
        },
        children=[
            html.Div("Epigenetics + Transformers = Future", className="hero-text", style={"fontSize":"3rem"})
        ]
    )
    # second content
    about = dbc.Container([
        html.H2("Welcome to the Epigenetics Analysis Dashboard", className="mt-4 text-light"),
        html.P("Explore advanced analysis bridging ME/CFS and Long COVID through an advanced Transformer pipeline. Enjoy interactive data visualizations, logs, and a sleek UI!", style={"color":"#ccc","fontSize":"1.1rem"}),
        html.Div([
            html.H4("Features:", style={"color":"#ddd"}),
            html.Ul([
                html.Li("Mandatory self-supervised pretraining for the transformer pipeline"),
                html.Li("Interactive logs & robust config system"),
                html.Li("Dark mode, parallax hero sections, collapsible sidebar, fully responsive"),
                html.Li("Cutting-edge epigenetics pipeline bridging ME/CFS & Long COVID")
            ], style={"color":"#bbb"})
        ], className="mt-4 mb-5")
    ], fluid=True, className="page-content")

    return html.Div([hero, about])

def layout_epigenetics():
    pca_path = os.path.join(RESULTS_DIR, "pca_plot.png")
    cm_path = os.path.join(RESULTS_DIR, "transformer_holdout_confusion.png")
    pca_b64 = load_image_as_base64(pca_path)
    cm_b64 = load_image_as_base64(cm_path)

    children = [
        html.H2("Epigenetics & Pipeline", className="mt-4 text-light"),
        html.P(
            "Pipeline outputs including PCA visualization and classifier confusion matrix.",
            style={"color": "#ccc"},
        ),
    ]

    if pca_b64:
        children.append(
            html.Div([
                html.H4("PCA Plot", className="text-light"),
                html.Img(src=f"data:image/png;base64,{pca_b64}", style={"maxWidth": "100%"}),
            ], style={"marginTop": "1rem"})
        )
    if cm_b64:
        children.append(
            html.Div([
                html.H4("Confusion Matrix", className="text-light"),
                html.Img(src=f"data:image/png;base64,{cm_b64}", style={"maxWidth": "100%"}),
            ], style={"marginTop": "1rem"})
        )

    if len(children) == 2:
        children.append(html.P("No figures found yet in results/. Run the pipeline to populate outputs.", style={"color": "#aaa"}))

    return dbc.Container(children, fluid=True, className="page-content")

def layout_me_lc():
    section1 = html.Div(
        className="parallax-section",
        style={
            "height":"60vh",
            "backgroundImage":"url('/assets/me_lc_bg.jpg')",
            "backgroundSize":"cover",
            "backgroundPosition":"center",
            "display":"flex",
            "alignItems":"center",
            "justifyContent":"center"
        },
        children=[
            html.Div("ME/CFS vs Long COVID Comparison", style={"fontSize":"2.5rem","color":"#fdfdfd","textAlign":"center"})
        ]
    )
    # Show available DMP/network visuals if present
    volcano_path = os.path.join(RESULTS_DIR, "volcano_plot.png")
    network_path = os.path.join(RESULTS_DIR, "network_diagram.png")
    volcano_b64 = load_image_as_base64(volcano_path)
    network_b64 = load_image_as_base64(network_path)

    content_children = [html.H3("Differential Methylation", className="text-light mt-4")]
    if volcano_b64:
        content_children.append(
            html.Div([
                html.H4("Volcano Plot", className="text-light"),
                html.Img(src=f"data:image/png;base64,{volcano_b64}", style={"maxWidth": "100%"}),
            ], style={"marginTop": "1rem", "marginBottom": "1rem"})
        )
    if network_b64:
        content_children.append(
            html.Div([
                html.H4("Co-annotation Network", className="text-light"),
                html.Img(src=f"data:image/png;base64,{network_b64}", style={"maxWidth": "100%"}),
            ], style={"marginTop": "1rem", "marginBottom": "1rem"})
        )
    if len(content_children) == 1:
        content_children.append(html.P("Run visualization step to generate volcano/network figures.", style={"color":"#aaa"}))

    content = dbc.Container(content_children, fluid=True, className="page-content")

    return html.Div([section1, content])

def layout_logs():
    return dbc.Container([
        html.H2("Pipeline Logs", className="text-light mt-4"),
        dbc.Textarea(
            id="pipeline-logs",
            placeholder="Logs appear here...",
            style={"width":"100%","height":"400px","backgroundColor":"#222","color":"#ccc","marginTop":"1rem"}
        )
    ], fluid=True, className="page-content")

##############################################
# MAIN LAYOUT
##############################################
app.layout = html.Div([
    dcc.Location(id="url"),
    dcc.Store(id="sidebar-state", data={"open":True}),
    navbar,
    sidebar,
    html.Div(id="page-content", style={"marginLeft":"300px","transition":"margin-left 0.3s ease-in-out"})
])

##############################################
# CALLBACKS
##############################################

# Routing
@app.callback(
    Output("page-content", "children"),
    [Input("url","pathname")]
)
def display_page(pathname):
    if pathname == "/epigenetics":
        return layout_epigenetics()
    elif pathname == "/me-lc":
        return layout_me_lc()
    elif pathname == "/logs":
        return layout_logs()
    else:
        return layout_home()

# Collapsible sidebar
@app.callback(
    [Output("sidebar","style"), Output("page-content","style"), Output("sidebar-state","data")],
    [Input("sidebar-toggle","n_clicks")],
    [State("sidebar-state","data"), State("page-content","style")]
)
def toggle_sidebar(n_clicks, sidebar_state, content_style):
    if not n_clicks:
        return dash.no_update, dash.no_update, dash.no_update
    is_open = sidebar_state["open"]
    new_open = not is_open
    if new_open:
        return ({"position":"fixed","left":"0","top":"56px","bottom":"0","width":"300px","background":"#111","color":"#aaa","padding":"1rem","overflowY":"auto","zIndex":"9999"},
                {"marginLeft":"300px","transition":"margin-left 0.3s ease-in-out"},
                {"open":True})
    else:
        return ({"position":"fixed","left":"-300px","top":"56px","bottom":"0","width":"300px","background":"#111","color":"#aaa","padding":"1rem","overflowY":"auto","zIndex":"9999","transition":"left 0.3s ease-in-out"},
                {"marginLeft":"0px","transition":"margin-left 0.3s ease-in-out"},
                {"open":False})

# Save config
@app.callback(
    Output("save-config-status","children"),
    [Input("save-config-btn","n_clicks")],
    [State("cond1-input","value"),
     State("cond2-input","value"),
     State("cond3-input","value")]
)
def save_config(n_clicks, c1, c2, c3):
    if not n_clicks:
        return ""
    if not c1 or not c2 or not c3:
        return "Please fill all 3 condition paths!"
    data = {"condition1": c1, "condition2": c2, "condition3": c3}
    with open("dashboard_config.json","w") as f:
        json.dump(data, f, indent=2)
    return "Configuration saved!"

# Run pipeline
@app.callback(
    Output("run-pipeline-status","children"),
    [Input("run-pipeline-btn","n_clicks")]
)
def run_pipeline(n_clicks):
    global PIPELINE_OUTPUT
    if not n_clicks:
        return ""
    try:
        # Execute the real pipeline and capture its output
        cmd = ["Rscript", RUN_PIPELINE_R, "0", "--web_mode=TRUE"]
        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        lines = []
        for line in proc.stdout:  # type: ignore[attr-defined]
            lines.append(line)
        proc.wait()
        PIPELINE_OUTPUT = "".join(lines)
        if proc.returncode == 0:
            return "Pipeline completed successfully."
        return "Pipeline finished with errors. Check Logs tab for details."
    except Exception as ex:
        PIPELINE_OUTPUT = f"Error running pipeline: {str(ex)}"
        return f"Error: {str(ex)}"

# Show logs
@app.callback(
    Output("pipeline-logs","value"),
    [Input("run-pipeline-btn","n_clicks")]
)
def update_logs(n_clicks):
    if not n_clicks:
        return "No logs yet."
    return PIPELINE_OUTPUT or "No logs yet."

##############################################

if __name__=="__main__":
    app.run_server(debug=True, port=8050)

# ======================
# File: src/original/other/upload_hf.py
# ======================

#!/opt/homebrew/bin/python3.12
from huggingface_hub import HfApi

def main():
    # Initialize the API client.
    api = HfApi()
    
    # Use upload_large_folder for a resilient upload of a large folder.
    # This method splits the upload into multiple commits, making it more robust.
    api.upload_large_folder(
        repo_id="VerisimilitudeX/EpiMECoV",
        repo_type="dataset",
        folder_path="/Volumes/T9/EpiMECoV1"
    )

if __name__ == "__main__":
    main()

# ======================
# File: src/original/other/fractal.py
# ======================

#!/usr/bin/env python3
"""
This script recursively processes IDAT files from a specified base directory,
extracts beta values using methylprep, and computes the fractal dimension of
each sample using a box-counting algorithm. The results (including log–log plots)
are saved for further analysis.

If required packages are not installed, the script attempts to install them automatically.
"""

import sys
import subprocess

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# List of required packages
required_packages = ["methylprep", "numpy", "matplotlib", "pandas"]

# Check for each package and install if missing
for package in required_packages:
    try:
        __import__(package)
    except ModuleNotFoundError:
        print(f"Package '{package}' not found. Installing {package}...")
        install(package)

import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from methylprep import run_pipeline  # For processing IDAT files

def box_count(points, box_size, x_min, x_max, y_min, y_max):
    """
    Count the number of boxes of side length `box_size` needed to cover the set of points.
    Uses a 2D histogram to count boxes with at least one point.
    
    Parameters:
        points (np.ndarray): Array of shape (N,2) with (x,y) coordinates.
        box_size (float): The side length of the square box.
        x_min, x_max, y_min, y_max (float): Bounds of the data.
    
    Returns:
        count (int): Number of boxes that contain at least one point.
    """
    x_bins = np.arange(x_min, x_max + box_size, box_size)
    y_bins = np.arange(y_min, y_max + box_size, box_size)
    H, _, _ = np.histogram2d(points[:, 0], points[:, 1], bins=[x_bins, y_bins])
    count = np.sum(H > 0)
    return count

def compute_fractal_dimension(points, box_sizes):
    """
    Estimate the fractal dimension of a set of points using the box-counting method.
    
    Parameters:
        points (np.ndarray): Array of shape (N,2) of points in the normalized domain.
        box_sizes (np.ndarray): Array of box sizes to use for the analysis.
    
    Returns:
        fractal_dim (float): Estimated fractal dimension (slope from log–log regression).
        bs_used (np.ndarray): The box sizes used.
        counts (list): List of box counts corresponding to each box size.
        coeffs (np.ndarray): Coefficients of the linear fit (slope and intercept).
    """
    x_min, x_max = np.min(points[:, 0]), np.max(points[:, 0])
    y_min, y_max = np.min(points[:, 1]), np.max(points[:, 1])
    
    counts = []
    for box_size in box_sizes:
        count = box_count(points, box_size, x_min, x_max, y_min, y_max)
        counts.append(count)
    
    counts = np.array(counts)
    valid = counts > 0
    log_counts = np.log(counts[valid])
    log_inv_box_sizes = np.log(1 / box_sizes[valid])
    
    coeffs = np.polyfit(log_inv_box_sizes, log_counts, 1)
    fractal_dim = coeffs[0]
    
    return fractal_dim, box_sizes, counts, coeffs

def main():
    # Define the base directory containing the IDAT files (including subfolders)
    base_dir = "/Volumes/T9/EpiMECoV/data"
    print("Running methylprep pipeline on directory:", base_dir)
    
    # Process all IDAT files; returns a DataFrame with beta values where
    # rows correspond to probes and columns correspond to samples.
    df = run_pipeline(base_dir, export=False)
    print("Loaded beta values with shape:", df.shape)
    
    results = {}
    # Define a range of box sizes for the analysis (domain is [0,1]x[0,1])
    bs = np.linspace(0.01, 0.1, 20)
    
    # Process each sample (each column in the DataFrame)
    for sample in df.columns:
        beta_values = df[sample].values
        # x coordinates: equally spaced indices normalized to [0,1]
        x = np.linspace(0, 1, len(beta_values))
        # y coordinates are the beta values (assumed to be in [0,1])
        y = beta_values
        points = np.column_stack((x, y))
        
        # Compute fractal dimension using the box-counting method
        fractal_dim, bs_used, counts, coeffs = compute_fractal_dimension(points, bs)
        results[sample] = fractal_dim
        
        # Generate and save a log–log plot for diagnostic purposes
        plt.figure(figsize=(8, 6))
        plt.plot(np.log(1 / bs_used), np.log(counts), 'o-', label=f'Fractal dim = {fractal_dim:.3f}')
        plt.xlabel('log(1/box size)')
        plt.ylabel('log(box count)')
        plt.title(f'Box-Counting Analysis for Sample {sample}')
        plt.legend()
        plot_filename = f'box_count_{sample}.png'
        plt.savefig(plot_filename)
        plt.close()
        print(f"Sample '{sample}' fractal dimension: {fractal_dim:.3f} (plot saved as {plot_filename})")
    
    # Save fractal dimension results to a text file
    with open("fractal_dimensions.txt", "w") as f:
        for sample, fd in results.items():
            f.write(f"{sample}\t{fd:.4f}\n")
    print("Fractal dimension analysis complete. Results saved to 'fractal_dimensions.txt'.")

if __name__ == '__main__':
    main()

# ======================
# File: src/original/other/check_pipeline_outputs.R
# ======================

#!/usr/bin/env Rscript

################################################################################
# check_pipeline_outputs.R
# Purpose: Automated checks to confirm the pipeline ran successfully and
#          generated all expected outputs.
#
# Usage: Rscript check_pipeline_outputs.R
################################################################################

library(minfi)

expected_files <- c(
  "450K_BetaValues.csv",
  "450K_Combined_RGChannelSet.rds",
  "450K_Final_GenomicMethylSet.rds",
  "EPIC_BetaValues.csv",
  "EPIC_Combined_RGChannelSet.rds",
  "EPIC_Final_GenomicMethylSet.rds",
  "Merged_450K_EPIC_BetaValues_with_Condition.csv",
  "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds"
)

missing_files <- character()
cat("Checking for expected output files...\n")
for (f in expected_files) {
  if (!file.exists(f)) {
    cat(sprintf("[MISSING] %s\n", f))
    missing_files <- c(missing_files, f)
  } else {
    cat(sprintf("[OK]      %s\n", f))
  }
}

if (length(missing_files) > 0) {
  cat("\nWARNING: Some expected files are missing!\n")
  print(missing_files)
  cat("Please verify your pipeline.\n\n")
} else {
  cat("\nAll expected files found.\n\n")
}

safeCheckRDS <- function(filepath) {
  cat(sprintf("   Checking RDS file: %s\n", filepath))
  obj <- readRDS(filepath)
  if (inherits(obj, "RGChannelSet")) {
    cat(sprintf("   --> RGChannelSet found. Dimensions: %d features x %d samples\n",
                nrow(obj), ncol(obj)))
  } else if (inherits(obj, "GenomicMethylSet")) {
    cat(sprintf("   --> GenomicMethylSet found. Dimensions: %d features x %d samples\n",
                nrow(obj), ncol(obj)))
    col_info <- colData(obj)
    if ("Condition" %in% colnames(col_info)) {
      cat("   --> 'Condition' column detected in colData.\n")
    } else {
      cat("   --> WARNING: 'Condition' column not found in colData!\n")
    }
  } else {
    cat("   --> Unrecognized object type.\n")
}

safeCheckCSV <- function(filepath, check_header = TRUE) {
  cat(sprintf("   Checking CSV file: %s\n", filepath))
  df <- read.csv(filepath, header = check_header, row.names = 1)
  cat(sprintf("   --> CSV read success. Dimensions: %d rows x %d cols\n",
              nrow(df), ncol(df)))
  cat("   --> First few row names:\n")
  print(head(rownames(df)))
  cat("\n")
}

for (f in expected_files) {
  if (!file.exists(f)) next
  ext <- tools::file_ext(f)
  if (tolower(ext) == "rds") {
    safeCheckRDS(f)
  } else if (tolower(ext) == "csv") {
    safeCheckCSV(f, check_header = TRUE)
  }
}

cat("Done. If no major warnings, your outputs look good!\n")

# ======================
# File: src/original/other/random_forest_hyperparam_search.py
# ======================

#!/usr/bin/env python3
"""
random_forest_hyperparam_search.py

Purpose:
  1) Load your epigenetic CSV and parse Condition => y.
  2) Use Optuna for Bayesian hyperparameter optimization of RandomForestClassifier.
  3) Evaluate the best model on a hold-out test set.
  4) Save confusion matrix & top-20 feature importances.

Usage:
  python random_forest_hyperparam_search.py
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

def objective(trial, X_train, y_train, X_val, y_val):
    n_estimators = trial.suggest_int("n_estimators", 50, 300)
    max_depth = trial.suggest_int("max_depth", 5, 20)
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)
    bootstrap = trial.suggest_categorical("bootstrap", [True, False])
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        bootstrap=bootstrap,
        random_state=42
    )
    clf.fit(X_train, y_train)
    preds = clf.predict(X_val)
    acc = accuracy_score(y_val, preds)
    return acc

def plot_confusion_matrix(cm, class_names, filename="rf_confusion_matrix.png"):
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names, yticklabels=class_names)
    plt.title("Confusion Matrix (XGB Hyperparam Search)")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

def plot_feature_importances(feature_names, importances, top_n=20, filename="rf_feature_importances.png"):
    idxs = np.argsort(importances)[::-1]
    top_idx = idxs[:top_n]
    plt.figure(figsize=(8, 6))
    sns.barplot(x=importances[top_idx], y=[feature_names[i] for i in top_idx], orient="h", palette="viridis")
    plt.title("RF Feature Importances (Optuna Optimized)")
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.close()

def main():
    CSV_DATA = "transformed_data.csv"
    if not os.path.exists(CSV_DATA):
        raise FileNotFoundError(f"[ERROR] Could not find {CSV_DATA}")
    df = pd.read_csv(CSV_DATA)
    if "Condition" not in df.columns:
        raise ValueError("[ERROR] Condition column not found.")
    feature_names = [c for c in df.columns if c != "Condition"]
    X = df[feature_names].values
    y_str = df["Condition"].values
    classes = sorted(np.unique(y_str))
    class_map = {c: i for i, c in enumerate(classes)}
    y = np.array([class_map[v] for v in y_str], dtype=np.int64)
    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval
    )
    study = optuna.create_study(direction="maximize")
    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)
    print("Best Params:", study.best_trial.params)
    best_rf = RandomForestClassifier(
        n_estimators=int(study.best_trial.params["n_estimators"]),
        max_depth=study.best_trial.params["max_depth"],
        min_samples_split=study.best_trial.params["min_samples_split"],
        min_samples_leaf=study.best_trial.params["min_samples_leaf"],
        bootstrap=study.best_trial.params["bootstrap"],
        random_state=42
    )
    best_rf.fit(X_trainval, y_trainval)
    preds_test = best_rf.predict(X_test)
    acc = accuracy_score(y_test, preds_test)
    print(f"Test Accuracy: {acc:.4f}")
    cm = confusion_matrix(y_test, preds_test)
    print("Confusion Matrix:\n", cm)
    print("Classification Report:\n", classification_report(y_test, preds_test, target_names=classes))
    plot_confusion_matrix(cm, classes, filename="rf_confusion_matrix.png")
    # Feature importance using permutation importance could be used,
    # but here we use the built-in feature_importances_
    importances = best_rf.feature_importances_
    plot_feature_importances(feature_names, importances, filename="rf_feature_importances.png")
    print("\nDone with Random Forest hyperparam search. See:")
    print(" • rf_confusion_matrix.png")
    print(" • rf_feature_importances.png")

if __name__ == "__main__":
    main()

# ======================
# File: src/original/other/shap_analysis.py
# ======================

#!/usr/bin/env python3
"""
shap_analysis.py

Purpose:
  - Loads or trains a multi-class RandomForest model.
  - Evaluates on test set.
  - Uses shap.Explainer for multi-class RandomForest.
  - Additionally applies Integrated Gradients (via Captum) for model interpretability.
  - Saves separate SHAP beeswarm plots and an overall bar chart, plus integrated gradients plots.

Usage:
  python shap_analysis.py
"""

import os
import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Optional: Captum for Integrated Gradients
try:
    from captum.attr import IntegratedGradients
    HAVE_CAPTUM = True
except ImportError:
    HAVE_CAPTUM = False

CSV_DATA = "transformed_data.csv"
RF_MODEL_PATH = "best_rf.joblib"
OUTPUT_PREFIX = "shap_summary"
CLASS_NAMES = ["Control", "LC", "ME"]  # adjust as needed

def load_data(csv_path):
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"[ERROR] CSV file not found: {csv_path}")
    df = pd.read_csv(csv_path)
    if "Condition" not in df.columns:
        raise ValueError("[ERROR] 'Condition' column not found.")
    feat_cols = [c for c in df.columns if c != "Condition"]
    X = df[feat_cols].values
    conds = df["Condition"].values
    classes = sorted(np.unique(conds))
    c_map = {c: i for i, c in enumerate(classes)}
    y = np.array([c_map[v] for v in conds], dtype=int)
    return X, y, feat_cols, classes

def train_and_save_rf(X, y, out_path=RF_MODEL_PATH):
    print(f"[INFO] No existing RF => training new RandomForest.")
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    joblib.dump(rf, out_path)
    print("[INFO] Done training & saved model as", out_path)

def main():
    X, y, feat_cols, detected_classes = load_data(CSV_DATA)
    if CLASS_NAMES and len(CLASS_NAMES) == len(detected_classes):
        class_names = CLASS_NAMES
    else:
        class_names = detected_classes
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    if os.path.exists(RF_MODEL_PATH):
        print(f"[INFO] Found existing RF model. Loading it.")
        rf = joblib.load(RF_MODEL_PATH)
    else:
        train_and_save_rf(X_train_s, y_train, RF_MODEL_PATH)
        rf = joblib.load(RF_MODEL_PATH)
    preds_test = rf.predict(X_test_s)
    acc = np.mean(preds_test == y_test)
    print(f"[INFO] Test Accuracy => {acc:.4f}")
    cm = confusion_matrix(y_test, preds_test)
    print("Confusion Matrix:")
    print(cm)
    print("\nClassification Report:")
    print(classification_report(y_test, preds_test, target_names=class_names))
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=class_names, yticklabels=class_names)
    plt.title("RF Confusion Matrix")
    plt.savefig("rf_confusion_matrix.png", dpi=300)
    plt.close()
    # SHAP analysis
    explainer = shap.Explainer(rf, X_train_s)
    shap_values = explainer(X_test_s)
    # Generate beeswarm plots for each class
    n_classes = shap_values.values.shape[1]
    for c_idx in range(n_classes):
        class_label = class_names[c_idx] if c_idx < len(class_names) else f"Class{c_idx}"
        shap.plots.beeswarm(shap_values[:, c_idx, :], max_display=20, show=False)
        plt.title(f"SHAP Beeswarm: {class_label}")
        out_beeswarm = f"{OUTPUT_PREFIX}_beeswarm_class_{c_idx}.png"
        plt.savefig(out_beeswarm, dpi=300, bbox_inches="tight")
        plt.close()
    shap_abs_mean = np.abs(shap_values.values).mean(axis=0)
    plt.figure(figsize=(8,6))
    sorted_idx = np.argsort(shap_abs_mean)[::-1][:20]
    sns.barplot(x=shap_abs_mean[sorted_idx], y=np.array(feat_cols)[sorted_idx], palette="viridis")
    plt.title("SHAP Bar (Average |SHAP|)")
    out_bar = f"{OUTPUT_PREFIX}_bar_overall.png"
    plt.savefig(out_bar, dpi=300, bbox_inches="tight")
    plt.close()
    # Integrated Gradients analysis (if Captum available)
    if HAVE_CAPTUM:
        print("[INFO] Running Integrated Gradients analysis using Captum.")
        # Wrap the RF model in a simple PyTorch model
        # Uses a simple linear approximation.
        class SimpleRFWrapper(nn.Module):
            def __init__(self, model):
                super().__init__()
                self.model = model
            def forward(self, x):
                # x: [batch, features]
                # Using model.predict_proba as a proxy; here we simply convert input to tensor.
                x_np = x.detach().cpu().numpy()
                preds = self.model.predict_proba(x_np)
                return torch.tensor(preds, dtype=torch.float32, device=x.device)
        rf_wrapper = SimpleRFWrapper(rf)
        ig = IntegratedGradients(rf_wrapper)
        attributions, delta = ig.attribute(torch.tensor(X_test_s, dtype=torch.float32), 
                                           target=1, return_convergence_delta=True)
        plt.figure(figsize=(10,4))
        plt.bar(range(len(feat_cols)), attributions.mean(dim=0).cpu().numpy())
        plt.xticks(range(len(feat_cols)), feat_cols, rotation=90)
        plt.title("Integrated Gradients (Average Attribution for Class 1)")
        plt.tight_layout()
        plt.savefig("integrated_gradients_class1.png", dpi=300)
        plt.close()
        print("Integrated Gradients plot saved as integrated_gradients_class1.png")
    else:
        print("[INFO] Captum not installed; skipping Integrated Gradients analysis.")
    print("=== SHAP analysis complete. ===")

if __name__ == "__main__":
    main()


# ======================
# File: src/original/other/gnn_integration.py
# ======================

#!/usr/bin/env python3
"""
gnn_integration.py

Purpose:
  Provides a compact Graph Convolutional Network (GCN) building block intended
  for import and use by training scripts.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleGCNLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super(SimpleGCNLayer, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
    def forward(self, X, A):
        # X: Node features [num_nodes, in_features]
        # A: Adjacency matrix [num_nodes, num_nodes]
        support = self.linear(X)
        out = torch.matmul(A, support)
        return out

class GNNIntegration(nn.Module):
    def __init__(self, num_features, hidden_dim, num_classes):
        super(GNNIntegration, self).__init__()
        self.gcn1 = SimpleGCNLayer(num_features, hidden_dim)
        self.gcn2 = SimpleGCNLayer(hidden_dim, hidden_dim)
        self.classifier = nn.Linear(hidden_dim, num_classes)
    def forward(self, X, A):
        x = self.gcn1(X, A)
        x = F.relu(x)
        x = self.gcn2(x, A)
        x = F.relu(x)
        # Global average pooling over nodes
        x = x.mean(dim=0)
        out = self.classifier(x)
        return out

if __name__ == "__main__":
    print("This module provides GNN layers/classes. Import and use within your training code.")


# ======================
# File: src/original/other/snapshot.R
# ======================

################################################################################
# snapshot.R
# Automated R script to tally IDAT/CSV files and estimate sample sizes
################################################################################

data_dir <- "/Volumes/T9/EpiMECoV/data"
output_file <- file.path(data_dir, "output_log.txt")

sink(output_file)

all_files <- list.files(path = data_dir, pattern = NULL, full.names = TRUE, recursive = TRUE)

idat_files <- all_files[grepl("\\.idat$", all_files, ignore.case = TRUE)]
gBase <- sub("_Grn\\.idat$", "", basename(idat_files), ignore.case = TRUE)
rBase <- sub("_Red\\.idat$", "", basename(idat_files), ignore.case = TRUE)
common_samples <- intersect(gBase, rBase)

cat("\nIDAT file check\n")
cat("==============================\n")
cat("Total IDAT files found:", length(idat_files), "\n")
cat("Estimated unique samples (assuming Grn/Red pairs):", length(common_samples), "\n")

sample_sheet_candidates <- all_files[grepl("samplesheet\\.csv$", all_files, ignore.case = TRUE)]
if(length(sample_sheet_candidates) > 0) {
  cat("\nPotential sample sheet(s):\n")
  print(sample_sheet_candidates)
  cat("\nReading first one:\n")
  sample_sheet <- read.csv(sample_sheet_candidates[1], header = TRUE, stringsAsFactors = FALSE)
  cat("\nSample sheet (head):\n")
  print(head(sample_sheet))
  cat("\nTotal rows in sample sheet:", nrow(sample_sheet), "\n")
} else {
  cat("\nNo obvious SampleSheet.csv found.\n")
}

cat("\nPlatform determination usually requires reading the IDAT data.\n")

csv_or_tsv_files <- all_files[grepl("\\.csv$|\\.tsv$|\\.txt$", all_files, ignore.case = TRUE)]
cat("\nCSV/TSV file check\n")
cat("==============================\n")
cat("Total CSV/TSV files found:", length(csv_or_tsv_files), "\n")

for(f in csv_or_tsv_files) {
  file_con <- file(f, "r")
  first_lines <- readLines(file_con, n = 5)
  close(file_con)
  suspicion <- FALSE
  if(any(grepl("beta", first_lines, ignore.case = TRUE))) suspicion <- TRUE
  if(any(grepl("Methylation", first_lines, ignore.case = TRUE))) suspicion <- TRUE
  if(any(grepl("^cg", first_lines))) suspicion <- TRUE
  
  if(suspicion) {
    cat("\nPotential methylation file:", basename(f), "\n")
    cat("First lines:\n")
    cat(paste(first_lines, collapse = "\n"), "\n")
  }
}

cat("\nDone.\n")
sink()

# ======================
# File: src/original/exploring/process_idat.R
# ======================

# process_idat.R
#
# Example script for loading IDAT files with minfi.
# Install needed packages:
# if (!requireNamespace("BiocManager", quietly=TRUE)) {
#     install.packages("BiocManager")
# }
# BiocManager::install(c(
#     "minfi", 
#     "IlluminaHumanMethylation450kmanifest", 
#     "IlluminaHumanMethylation450kanno.ilmn12.hg19"
# ))
#

library(minfi)
library(limma)

baseDir <- "/Volumes/T9/EpiMECoV/data/GSE93266_RAW"

RGset <- read.metharray.exp(base = baseDir)

qcReport(RGset, pdf="QC_Report.pdf")

MSet <- preprocessIllumina(RGset)
GSet <- mapToGenome(MSet)

plotMDS(getM(GSet), top=1000, gene.selection="common")

sampleSheet <- pData(RGset)
# sampleSheet$Phenotype <- c("Control","Patient","Control", ...)

design <- model.matrix(~ sampleSheet$Phenotype)
colnames(design) <- c("Intercept", "Patient")

fit <- lmFit(getBeta(GSet), design)
fit <- eBayes(fit)
top <- topTable(fit, coef="Patient", number=20)
write.csv(top, file="Differentially_Methylated_Probes.csv")

topProbe <- rownames(top)[1]
boxplot(getBeta(GSet)[topProbe,] ~ sampleSheet$Phenotype,
        main=paste("Methylation for Probe", topProbe),
        xlab="Phenotype", ylab="Beta")

# ======================
# File: src/original/exploring/view_merged.R
# ======================

###############################################################################
# fix_condition_only.R
#
# If Condition is missing or misaligned in your Beta CSV, this script merges
# them from your GenomicMethylSet's colData. 
###############################################################################

rm(list=ls())
cat("\014")

library(minfi)

merged_rds_path  <- "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds"
beta_csv_path    <- "Merged_450K_EPIC_BetaValues_with_Condition.csv"

cat("=== Loading merged GenomicMethylSet ===\n")
if (!file.exists(merged_rds_path)) {
  stop("Merged RDS file not found: ", merged_rds_path)
}
mergedSet <- readRDS(merged_rds_path)
cat("Loaded mergedSet. Dimension:", dim(mergedSet), "\n")

cat("Condition distribution:\n")
print(table(colData(mergedSet)$Condition))

cat("\n=== Loading Beta CSV ===\n")
if (!file.exists(beta_csv_path)) {
  stop("Beta CSV file not found: ", beta_csv_path)
}
beta_values <- read.csv(beta_csv_path, row.names=1, check.names=FALSE)
cat("Loaded Beta CSV. Dimension:", dim(beta_values), "\n")

if (!("Condition" %in% colnames(beta_values))) {
  cat("No Condition column found. Manually appending...\n")
  beta_values_t <- t(beta_values)
  cat("After transpose => dimension:", dim(beta_values_t), "\n")
  df <- as.data.frame(beta_values_t)
  df$Condition <- as.character(colData(mergedSet)$Condition)
  out_path <- "Merged_450K_EPIC_BetaValues_with_Condition_Updated.csv"
  write.csv(df, out_path, quote=FALSE)
  cat("Saved updated =>", out_path, "\n")
} else {
  cat("Condition column already present.\n")
}
cat("Done.\n")

# ======================
# File: src/original/exploring/dmap.R
# ======================

# dmap.R
#
# Minimal example for reading a DMAP matrix and summarizing top genes.

data <- read.csv("/Volumes/T9/EpiMECoV/data/GSE153667_DMAP_Matrix.csv")
significant <- subset(data, Pr < 0.05)

library(dplyr)
top_genes <- significant %>%
  group_by(GeneID) %>%
  summarise(count=n()) %>%
  arrange(desc(count))

print(head(top_genes, 10))

# ======================
# File: src/original/steps/02_differential_methylation_analysis.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# 02_differential_methylation_analysis.R
#
# Purpose:
#   Identify differentially methylated probes (DMPs) using 'limma'
#   among (ME, LC, Control).
#   1) Loads Beta_Transposed_with_Condition.csv (samples x probes + Condition).
#   2) Transposes => row=probes, col=samples for limma.
#   3) Fits a linear model with design matrix ~0+Condition.
#   4) Defines contrasts (ME-Control, LC-Control, ME-LC).
#   5) Applies filtering to keep only significant probes (adjusted P-value < 0.05 and |logFC| > 0.2).
#   6) Saves the filtered DMP results to the results directory.
#
# Usage:
#   Rscript 02_differential_methylation_analysis.R
###############################################################################

rm(list=ls())
cat("\014")  # Clear console

cat("=== Differential Methylation Analysis (limma) START ===\n\n")

# (A) Ensure required packages are available
if (!requireNamespace("limma", quietly = TRUE)) {
  install.packages("limma", repos = "https://cloud.r-project.org/")
}
suppressPackageStartupMessages(library(limma))

if (!requireNamespace("vroom", quietly = TRUE)) {
  install.packages("vroom", repos = "https://cloud.r-project.org/")
}
suppressPackageStartupMessages(library(vroom))

if (!requireNamespace("BiocParallel", quietly = TRUE)) {
  install.packages("BiocParallel", repos = "https://cloud.r-project.org/")
}
suppressPackageStartupMessages(library(BiocParallel))
# Use all available cores minus one for parallel processing in modeling.
num_cores <- max(1, parallel::detectCores() - 1)
BPPARAM <- MulticoreParam(workers = num_cores)
cat("[INFO] Using", num_cores, "cores for parallel processing.\n")

# (B) Define file paths
epi_root <- "/Volumes/T9/EpiMECoV"
beta_csv <- file.path(epi_root, "processed_data", "Beta_Transposed_with_Condition.csv")
results_dir <- file.path(epi_root, "results")
if (!dir.exists(results_dir)) {
  dir.create(results_dir, recursive = TRUE, showWarnings = FALSE)
}

cat("[INFO] Reading CSV =>", beta_csv, "\n")
if (!file.exists(beta_csv)) {
  stop("File not found: ", beta_csv)
}

# (C) Read the beta matrix using vroom::vroom for faster multi-threaded CSV reading
cat("[DATA] Reading beta matrix using vroom::vroom()...\n")
beta_df <- tryCatch({
  # vroom returns a tibble; convert it to a data.frame for consistency.
  as.data.frame(vroom(beta_csv, delim = ",", col_names = TRUE, progress = FALSE))
}, error = function(e) {
  cat("[WARN] vroom() failed with error:\n", conditionMessage(e), "\nFalling back to read.csv()...\n")
  read.csv(beta_csv, header = TRUE, sep = ",", check.names = FALSE)
})
cat("[INFO] Data reading complete; data converted to data.frame.\n")
gc()

cat("[INFO] Setting row names from the first column...\n")
rownames(beta_df) <- beta_df[[1]]
beta_df[[1]] <- NULL
cat("[INFO] Data dimensions (including Condition):", dim(beta_df), "\n")

# (D) Extract Condition and convert numeric data to a matrix
if (!("Condition" %in% colnames(beta_df))) {
  stop("[ERROR] 'Condition' column not found in the CSV. Cannot proceed.")
}
Condition <- as.factor(beta_df$Condition)
cat("[INFO] Converting beta values to numeric matrix (excluding 'Condition')...\n")
beta_numeric <- beta_df[, !colnames(beta_df) %in% "Condition", drop = FALSE]
beta_numeric <- as.matrix(sapply(beta_numeric, as.numeric))
cat("[INFO] Numeric beta matrix dimensions:", paste(dim(beta_numeric), collapse = " x "), "\n")
rm(beta_df)
gc()

# (E) Transpose the beta matrix for limma (row=probes, col=samples)
cat("\n[STEP] Transposing numeric matrix: now row=probes, col=samples.\n")
beta_t <- t(beta_numeric)
cat("[INFO] Transposed dimensions:", paste(dim(beta_t), collapse = " x "), "\n")
rm(beta_numeric)
gc()

# (F) Build design matrix
cat("\n[STEP] Building design matrix (model.matrix(~0 + Condition)).\n")
design <- model.matrix(~0 + Condition)
colnames(design) <- levels(Condition)
print(head(design))

# (G) Fit linear model and compute contrasts
cat("\n[STEP] Fitting linear model with limma (parallelized)...\n")
fit <- lmFit(beta_t, design, BPPARAM = BPPARAM)
contrast.matrix <- makeContrasts(
  ME_vs_Control = ME - Control,
  LC_vs_Control = LC - Control,
  ME_vs_LC      = ME - LC,
  levels = design
)
fit2 <- contrasts.fit(fit, contrast.matrix)
fit2 <- eBayes(fit2)

cat("[INFO] Extracting topTable for each contrast.\n\n")

# (H) Filter DMP results before saving
# Per paper: select CpGs with adjusted P-value < 0.01 (no additional logFC filter)
pval_thresh <- 0.01

top_ME <- topTable(fit2, coef = "ME_vs_Control", number = Inf, 
                   adjust.method = "BH", sort.by = "P")
sig_ME <- top_ME[top_ME$adj.P.Val < pval_thresh, ]
cat(" => ME vs Control: Found", nrow(sig_ME), "significant probes (of", nrow(top_ME), ").\n")
out_ME <- file.path(results_dir, "DMP_ME_vs_Control.csv")
write.csv(sig_ME, out_ME, row.names = TRUE, quote = FALSE)

top_LC <- topTable(fit2, coef = "LC_vs_Control", number = Inf,
                   adjust.method = "BH", sort.by = "P")
sig_LC <- top_LC[top_LC$adj.P.Val < pval_thresh, ]
cat(" => LC vs Control: Found", nrow(sig_LC), "significant probes (of", nrow(top_LC), ").\n")
out_LC <- file.path(results_dir, "DMP_LC_vs_Control.csv")
write.csv(sig_LC, out_LC, row.names = TRUE, quote = FALSE)

top_ME_LC <- topTable(fit2, coef = "ME_vs_LC", number = Inf,
                      adjust.method = "BH", sort.by = "P")
sig_ME_LC <- top_ME_LC[top_ME_LC$adj.P.Val < pval_thresh, ]
cat(" => ME vs LC: Found", nrow(sig_ME_LC), "significant probes (of", nrow(top_ME_LC), ").\n")
out_ME_LC <- file.path(results_dir, "DMP_ME_vs_LC.csv")
write.csv(sig_ME_LC, out_ME_LC, row.names = TRUE, quote = FALSE)

cat("\n=== Differential Methylation Analysis Complete. ===\n")
cat("Results saved in the 'results' folder.\n")

# Optionally, write a combined list of CpGs with adj.P.Val < 0.01 across any contrast (for downstream top-1280 selection)
try({
  sig_all <- unique(c(rownames(sig_ME), rownames(sig_LC), rownames(sig_ME_LC)))
  out_all <- file.path(results_dir, "DMP_all_adjP_lt_0.01.txt")
  write.table(sig_all, out_all, row.names = FALSE, col.names = FALSE, quote = FALSE)
  cat("[SAVED] =>", out_all, "\n")
}, silent = TRUE)

# ======================
# File: src/original/steps/14_visualize_results.py
# ======================

#!/usr/bin/env python3
"""
14_visualize_results.py

Purpose:
  Generate a suite of detailed visuals (PCA, t-SNE, UMAP, heatmaps, etc.)
  from the final data. This version has been optimized by using fast,
  vectorized imputation (handling NaN and infinite values), subsampling when
  datasets are huge, and safety checks for expensive computations.
  
Usage:
  python3 14_visualize_results.py
"""

import os
import sys
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
import scipy.cluster.hierarchy as sch
import networkx as nx

try:
    from upsetplot import UpSet
    HAVE_UPSETPLOT = True
except ImportError:
    HAVE_UPSETPLOT = False

try:
    import umap
    HAVE_UMAP = True
except ImportError:
    HAVE_UMAP = False

try:
    import plotly.graph_objects as go
    HAVE_PLOTLY = True
except ImportError:
    HAVE_PLOTLY = False

try:
    from PIL import Image
    HAVE_PIL = True
except ImportError:
    HAVE_PIL = False

# ----------------------------
# Helper: safe_call to catch exceptions
# ----------------------------
def safe_call(func, *args, **kwargs):
    """Wrapper to run a plotting function and catch any exceptions."""
    try:
        func(*args, **kwargs)
    except Exception as e:
        print(f"[ERROR] {func.__name__} failed: {e}")

# ----------------------------
# Optimized impute_and_scale using median fill (vectorized)
# ----------------------------
def impute_and_scale(df, feat_cols):
    """
    Imputes missing values in the specified feature columns using the median and
    applies StandardScaler. Replaces infinities and ensures no NaN remains.
    """
    if len(feat_cols) == 0:
        print("[ERROR] No numeric features. Skipping impute_and_scale.")
        return None
    feat_cols_in_df = [c for c in feat_cols if c in df.columns]
    if len(feat_cols_in_df) == 0:
        print("[ERROR] None of feat_cols exist in df.columns. Skipping.")
        return None

    X_orig = df[feat_cols_in_df]
    # Replace infinities with NaN and fill missing values with median
    X_orig = X_orig.replace([np.inf, -np.inf], np.nan)
    X_imputed = X_orig.fillna(X_orig.median())
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_imputed.values)
    # Ensure any remaining NaN or infinities are set to zero
    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)
    return X_scaled

# ----------------------------
# Plotting functions
# ----------------------------
def plot_volcano(dmp_csv, out_path):
    df = pd.read_csv(dmp_csv)
    if 'P.Value' not in df.columns or 'logFC' not in df.columns:
        raise ValueError("CSV file must contain 'P.Value' and 'logFC' columns.")
    df['-log10(P.Value)'] = -np.log10(df['P.Value'])

    plt.figure(figsize=(8,6))
    sns.scatterplot(data=df, x='logFC', y='-log10(P.Value)', hue='-log10(P.Value)',
                    palette="viridis", alpha=0.7)
    plt.title("Volcano Plot")
    plt.xlabel("Log Fold Change")
    plt.ylabel("-Log10(P.Value)")
    plt.axhline(y=-np.log10(0.05), color='red', linestyle='--', label='P=0.05')
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] Volcano plot => {out_path}")

def plot_scatter_box(df, scatter_out, box_out):
    # Reset index to ensure numeric positions for boxplot
    df = df.reset_index(drop=True)
    feat_cols = [col for col in df.columns if col != "Condition"]
    if not feat_cols:
        print("[INFO] No feature columns available for scatter/box plots.")
        return
    feature = feat_cols[0]

    plt.figure(figsize=(8,6))
    sns.scatterplot(x=df.index, y=df[feature], hue=df["Condition"], alpha=0.7)
    plt.title(f"Scatter Plot for {feature}")
    plt.xlabel("Sample Index")
    plt.ylabel(feature)
    plt.legend()
    plt.tight_layout()
    plt.savefig(scatter_out, dpi=300)
    plt.close()
    print(f"[SAVED] Scatter plot => {scatter_out}")

    plt.figure(figsize=(8,6))
    sns.boxplot(data=df, x="Condition", y=feature)
    plt.title(f"Box Plot for {feature}")
    plt.xlabel("Condition")
    plt.ylabel(feature)
    plt.tight_layout()
    plt.savefig(box_out, dpi=300)
    plt.close()
    print(f"[SAVED] Box plot => {box_out}")

def plot_age_distribution(df, out_path):
    if "Age" not in df.columns:
        print("[INFO] No 'Age' column found; skipping age distribution plot.")
        return
    plt.figure(figsize=(7,5))
    sns.histplot(df["Age"].dropna(), kde=True, bins=30, color="steelblue")
    plt.title("Age Distribution")
    plt.xlabel("Age")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] Age distribution => {out_path}")

def plot_sex_distribution(df, out_path):
    if "Sex" not in df.columns:
        print("[INFO] No 'Sex' column found; skipping sex distribution plot.")
        return
    plt.figure(figsize=(7,5))
    sns.countplot(x=df["Sex"], palette="pastel")
    plt.title("Sex Distribution")
    plt.xlabel("Sex")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] Sex distribution => {out_path}")

def plot_tsne(Xs, y, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for t-SNE.")
        return

    n_samples = Xs.shape[0]
    if n_samples > 1000:
        np.random.seed(42)
        indices = np.random.choice(n_samples, 1000, replace=False)
        Xs = Xs[indices, :]
        y = y[indices]

    # Ensure Xs is finite
    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    perplexity_value = max(1, min(Xs.shape[0] - 1, 30))
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value,
                n_iter=250, init='random')
    X2d = tsne.fit_transform(Xs)

    plt.figure(figsize=(7,5))
    for cond in np.unique(y):
        idx = (y == cond)
        plt.scatter(X2d[idx,0], X2d[idx,1], label=cond, alpha=0.7)
    plt.title("t-SNE on Final Data")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] t-SNE => {out_path}")

def plot_tsne_with_perplexity(Xs, y, perplexity, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for t-SNE perplexity test.")
        return

    n_samples = Xs.shape[0]
    if perplexity >= n_samples:
        perplexity = n_samples - 1
    if perplexity < 1:
        print("[WARN] Not enough samples for this perplexity.")
        return

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity,
                n_iter=250, init='random')
    X2d = tsne.fit_transform(Xs)

    plt.figure(figsize=(7,5))
    for cond in np.unique(y):
        idx = (y == cond)
        plt.scatter(X2d[idx,0], X2d[idx,1], label=cond, alpha=0.7)
    plt.title(f"t-SNE (Perplexity={perplexity})")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] t-SNE (p={perplexity}) => {out_path}")

def plot_pca(Xs, y, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for PCA.")
        return

    if Xs.shape[0] > 1000:
        np.random.seed(42)
        indices = np.random.choice(Xs.shape[0], 1000, replace=False)
        Xs = Xs[indices, :]
        y = y[indices]

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(Xs)

    plt.figure(figsize=(7,5))
    for cond in np.unique(y):
        idx = (y == cond)
        plt.scatter(X_pca[idx,0], X_pca[idx,1], label=cond, alpha=0.7)
    plt.title("PCA on Final Data")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] PCA => {out_path}")

def plot_pca_scree(Xs, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for PCA Scree.")
        return

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    pca = PCA(random_state=42)
    pca.fit(Xs)
    var_exp = pca.explained_variance_ratio_ * 100

    plt.figure(figsize=(8,5))
    plt.plot(range(1, len(var_exp)+1), var_exp, marker='o')
    plt.title("PCA Scree Plot")
    plt.xlabel("Principal Component")
    plt.ylabel("Variance Explained (%)")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] PCA scree => {out_path}")

def plot_pca_biplot(Xs, y, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for PCA biplot.")
        return

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(Xs)

    plt.figure(figsize=(8,6))
    sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette="viridis", alpha=0.7)
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

    for i in range(loadings.shape[0]):
        plt.arrow(0, 0, loadings[i,0], loadings[i,1], color='red', alpha=0.3)

    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title("PCA Biplot")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] PCA biplot => {out_path}")

def plot_umap_visualization(Xs, y, out_path):
    if not HAVE_UMAP:
        print("[INFO] UMAP not installed; skipping.")
        return
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for UMAP.")
        return

    if Xs.shape[0] > 1000:
        np.random.seed(42)
        indices = np.random.choice(Xs.shape[0], 1000, replace=False)
        Xs = Xs[indices, :]
        y = y[indices]

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    reducer = umap.UMAP(random_state=42)
    X_umap = reducer.fit_transform(Xs)

    plt.figure(figsize=(7,5))
    for cond in np.unique(y):
        idx = (y == cond)
        plt.scatter(X_umap[idx,0], X_umap[idx,1], label=cond, alpha=0.7)
    plt.title("UMAP on Final Data")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] UMAP => {out_path}")

def plot_correlation_heatmap(Xs, out_path):
    if Xs is None or Xs.shape[1] < 2:
        print("[WARN] Not enough features for correlation heatmap.")
        return

    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    corr = np.corrcoef(Xs, rowvar=False)
    if np.isnan(corr).all():
        print("[WARN] correlation is all NaN; skipping heatmap.")
        return

    plt.figure(figsize=(10,8))
    sns.heatmap(corr, cmap="coolwarm", center=0)
    plt.title("Correlation Heatmap (Features)")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] correlation_heatmap => {out_path}")

def plot_hierarchical_dendrogram(Xs, out_path):
    if Xs is None or Xs.shape[0] < 2 or Xs.shape[1] == 0:
        print("[WARN] Not enough data for dendrogram.")
        return

    max_samples = 500
    if Xs.shape[0] > max_samples:
        np.random.seed(42)
        indices = np.random.choice(Xs.shape[0], max_samples, replace=False)
        Xs_sample = Xs[indices, :]
    else:
        Xs_sample = Xs

    Xs_sample = np.nan_to_num(Xs_sample, nan=0.0, posinf=0.0, neginf=0.0)
    try:
        linkage = sch.linkage(Xs_sample, method='ward')
    except ValueError as e:
        print(f"[ERROR] Cannot cluster: {e}")
        return

    plt.figure(figsize=(10,7))
    sch.dendrogram(linkage)
    plt.title("Hierarchical Clustering Dendrogram")
    plt.xlabel("Samples")
    plt.ylabel("Distance")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] hierarchical_dendrogram => {out_path}")

def plot_box_feature(df, feature, out_path):
    if feature not in df.columns:
        print(f"[WARN] feature {feature} not in df; skipping box plot.")
        return

    plt.figure(figsize=(7,5))
    sns.boxplot(data=df, x="Condition", y=feature)
    plt.title(f"Boxplot of {feature}")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] boxplot_feature => {out_path}")

def plot_violin_feature(df, feature, out_path):
    if feature not in df.columns:
        print(f"[WARN] feature {feature} not in df; skipping violin plot.")
        return

    plt.figure(figsize=(7,5))
    sns.violinplot(data=df, x="Condition", y=feature)
    plt.title(f"Violin of {feature}")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] violin_feature => {out_path}")

def plot_pairplot(df, features, out_path):
    # Ensure all requested features are in df
    for feat in features:
        if feat not in df.columns:
            print(f"[WARN] feature {feat} not in df; skipping pairplot.")
            return

    sns_plot = sns.pairplot(df[features + ["Condition"]], hue="Condition")
    sns_plot.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] pairplot => {out_path}")

def plot_kde_feature(df, feature, out_path):
    if feature not in df.columns:
        print(f"[WARN] feature {feature} not found for KDE.")
        return

    plt.figure(figsize=(7,5))
    sns.kdeplot(data=df, x=feature, hue="Condition", fill=True)
    plt.title(f"KDE for {feature}")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] kde_plot => {out_path}")

def plot_condition_count(df, out_path):
    if "Condition" not in df.columns:
        print("[WARN] No Condition col for condition_count.")
        return

    plt.figure(figsize=(7,5))
    count_series = df["Condition"].value_counts()
    sns.barplot(x=count_series.index, y=count_series.values)
    plt.title("Sample Count per Condition")
    plt.xlabel("Condition")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] condition_count => {out_path}")

def plot_rf_feature_importance(feature_names, importances, top_n, out_path):
    idxs = np.argsort(importances)[::-1]
    top_idx = idxs[:top_n]

    plt.figure(figsize=(10,6))
    sns.barplot(x=importances[top_idx], y=np.array(feature_names)[top_idx], palette="viridis")
    plt.title("Top Feature Importances (RandomForest)")
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] rf_importance => {out_path}")

def plot_classifier_confusion_matrix(cm, classes, out_path):
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
    plt.title("Classifier Confusion Matrix")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] confusion_matrix => {out_path}")

def plot_roc_curve(y_true, y_score, out_path):
    if len(np.unique(y_true)) > 2:
        print("[WARN] ROC not valid for multi-class. Skipping.")
        return

    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(7,5))
    plt.plot(fpr, tpr, label=f"ROC (area={roc_auc:.2f})")
    plt.plot([0,1],[0,1],'--')
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title("ROC Curve")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] roc_curve => {out_path}")

def plot_precision_recall_curve(y_true, y_score, out_path):
    if len(np.unique(y_true)) > 2:
        print("[WARN] PR curve not valid for multi-class. Skipping.")
        return

    precision, recall, _ = precision_recall_curve(y_true, y_score)
    pr_auc = auc(recall, precision)
    plt.figure(figsize=(7,5))
    plt.plot(recall, precision, label=f"PR (area={pr_auc:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] pr_curve => {out_path}")

def plot_calibration_curve(y_true, y_prob, out_path):
    if len(np.unique(y_true)) > 2:
        print("[WARN] calibration curve not valid for multi-class.")
        return

    from sklearn.calibration import calibration_curve
    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)
    plt.figure(figsize=(7,5))
    plt.plot(prob_pred, prob_true, marker='o')
    plt.plot([0,1],[0,1],'--')
    plt.title("Calibration Curve")
    plt.xlabel("Mean Predicted Probability")
    plt.ylabel("Fraction of Positives")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] calibration_curve => {out_path}")

def plot_sankey_diagram(out_path):
    if not HAVE_PLOTLY:
        print("[INFO] Plotly not installed; skipping Sankey.")
        return
    try:
        import kaleido
    except ImportError:
        print("[ERROR] kaleido not installed => skipping Sankey.")
        return

    labels = ["Start","GroupA","GroupB","End"]
    source = [0,0,1,2]
    target = [1,2,3,3]
    value  = [8,4,2,6]
    fig = go.Figure(data=[go.Sankey(
        node=dict(label=labels, pad=15, thickness=20),
        link=dict(source=source, target=target, value=value)
    )])
    fig.update_layout(title_text="Sankey Example", font_size=10)

    try:
        fig.write_image(out_path)
        print(f"[SAVED] sankey_diagram => {out_path}")
    except Exception as e:
        print(f"[ERROR] plot_sankey_diagram => {e}")

def plot_pca_3d(Xs, y, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for 3D PCA.")
        return
    if Xs.shape[0] < 2:
        print("[INFO] Not enough samples for 3D PCA.")
        return

    from mpl_toolkits.mplot3d import Axes3D
    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    pca = PCA(n_components=3, random_state=42)
    X_3d = pca.fit_transform(Xs)

    fig = plt.figure(figsize=(8,6))
    ax = fig.add_subplot(111, projection='3d')
    for cond in np.unique(y):
        idx = (y == cond)
        ax.scatter(X_3d[idx,0], X_3d[idx,1], X_3d[idx,2], label=cond, alpha=0.7)
    ax.set_title("3D PCA Plot")
    ax.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] pca_3d_plot => {out_path}")

def plot_tsne_3d(Xs, y, out_path):
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] No valid features for 3D t-SNE.")
        return

    n_samples = Xs.shape[0]
    if n_samples < 3:
        print("[INFO] Not enough samples for 3D t-SNE.")
        return

    from mpl_toolkits.mplot3d import Axes3D
    Xs = np.nan_to_num(Xs, nan=0.0, posinf=0.0, neginf=0.0)
    perp = min(30, max(1, n_samples-1))
    tsne = TSNE(n_components=3, random_state=42, perplexity=perp, n_iter=250, init='random')
    X_3d = tsne.fit_transform(Xs)

    fig = plt.figure(figsize=(8,6))
    ax = fig.add_subplot(111, projection='3d')
    for cond in np.unique(y):
        idx = (y == cond)
        ax.scatter(X_3d[idx,0], X_3d[idx,1], X_3d[idx,2], label=cond, alpha=0.7)
    ax.set_title("3D t-SNE")
    ax.legend()
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] tsne_3d_plot => {out_path}")

def plot_multi_panel_summary(imputed_df, out_path):
    """
    Creates a multi-panel summary figure with PCA, t-SNE, UMAP, Scree plot, correlation heatmap,
    dendrogram, and box/violin plots for the first numeric feature, plus condition counts.
    """
    if imputed_df.shape[0] > 1000:
        np.random.seed(42)
        imputed_df = imputed_df.sample(n=1000, random_state=42)

    numeric_cols = [c for c in imputed_df.columns if c != "Condition"]
    if len(numeric_cols) == 0:
        print("[WARN] No numeric columns for multi-panel summary.")
        return

    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)

    n_samples = imputed_df.shape[0]
    if n_samples < 2:
        print("[WARN] Not enough samples for multi-panel summary.")
        return

    fig, axes = plt.subplots(3, 3, figsize=(15,15))

    # PCA (2D)
    X_mat = imputed_df[numeric_cols].values
    pca = PCA(n_components=2, random_state=42)
    try:
        X_pca = pca.fit_transform(X_mat)
    except Exception as e:
        print(f"[ERROR] PCA in multi-panel summary failed: {e}")
        return

    ax = axes[0,0]
    sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=imputed_df["Condition"], ax=ax)
    ax.set_title("PCA Plot")

    # t-SNE
    if n_samples > 2:
        tsne = TSNE(n_components=2, random_state=42,
                    perplexity=min(30, n_samples-1), n_iter=250, init='random')
        X_tsne = tsne.fit_transform(X_mat)
        ax = axes[0,1]
        sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=imputed_df["Condition"], ax=ax)
        ax.set_title("t-SNE")
    else:
        axes[0,1].text(0.5,0.5,"Not enough samples",ha='center')
        axes[0,1].set_title("t-SNE")

    # UMAP
    ax = axes[0,2]
    if HAVE_UMAP and n_samples > 2:
        import umap
        reducer = umap.UMAP(random_state=42)
        X_umap = reducer.fit_transform(X_mat)
        sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=imputed_df["Condition"], ax=ax)
        ax.set_title("UMAP")
    else:
        ax.text(0.5,0.5,"UMAP not installed or not enough samples",
                ha='center', va='center')
        ax.set_title("UMAP")

    # PCA Scree
    pca_full = PCA(random_state=42)
    pca_full.fit(X_mat)
    var_exp = pca_full.explained_variance_ratio_*100
    ax = axes[1,0]
    ax.plot(range(1,len(var_exp)+1), var_exp, marker='o')
    ax.set_title("PCA Scree")

    # Correlation heatmap (top 50 features)
    ax = axes[1,1]
    max_for_corr = min(50, len(numeric_cols))
    corr_data = X_mat[:, :max_for_corr]
    corr = np.corrcoef(corr_data, rowvar=False)
    if not np.isnan(corr).all():
        sns.heatmap(corr, ax=ax, cmap="coolwarm", cbar=False)
        ax.set_title("Corr Heatmap (top 50 features)")
    else:
        ax.text(0.5,0.5,"All NaN corr",ha='center')
        ax.set_title("Corr Heatmap")

    # Dendrogram
    ax = axes[1,2]
    try:
        link = sch.linkage(X_mat, method='ward')
        sch.dendrogram(link, ax=ax, no_labels=True)
        ax.set_title("Dendrogram")
    except Exception as e:
        ax.text(0.5,0.5,f"Dendrogram failed:\n{e}",ha='center')
        ax.set_title("Dendrogram")

    # Box + Violin for first numeric feature
    ax = axes[2,0]
    first_feat = numeric_cols[0]
    sns.boxplot(data=imputed_df, x="Condition", y=first_feat, ax=ax)
    ax.set_title(f"Box: {first_feat}")

    ax = axes[2,1]
    sns.violinplot(data=imputed_df, x="Condition", y=first_feat, ax=ax)
    ax.set_title(f"Violin: {first_feat}")

    # Condition count
    ax = axes[2,2]
    ccount = imputed_df["Condition"].value_counts()
    sns.barplot(x=ccount.index, y=ccount.values, ax=ax)
    ax.set_title("Condition Count")

    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] multi_panel_summary => {out_path}")

def plot_upset(dmp_files, out_path):
    """
    If upsetplot is available, we attempt to show an UpSet plot of overlapping DMP sets.
    DMP CSVs must have at least one column with CpG IDs (named 'CpG' or else we use first col).
    """
    if not HAVE_UPSETPLOT:
        print("[INFO] upsetplot not installed; skipping UpSet.")
        return

    sets = {}
    for key, path in dmp_files.items():
        if not os.path.exists(path):
            print(f"[WARN] {path} missing => skipping in UpSet.")
            continue
        df = pd.read_csv(path)
        if 'CpG' in df.columns:
            cpgs = df['CpG'].dropna().astype(str).values
        else:
            cpgs = df.iloc[:,0].dropna().astype(str).values
        sets[key] = set(cpgs)

    if len(sets)==0:
        print("[WARN] No DMP sets for UpSet.")
        return

    all_union = set().union(*sets.values())
    if len(all_union)==0:
        print("[WARN] No CpGs in union => skip upset.")
        return

    data = {}
    for key, s in sets.items():
        data[key] = [1 if x in s else 0 for x in all_union]

    upset_df = pd.DataFrame(data, index=list(all_union))
    from upsetplot import from_indicators, plot as upset_plot
    upset_data = from_indicators(upset_df)

    fig, ax = plt.subplots(figsize=(8,6))
    upset_plot(upset_data, ax=ax)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] intersection_plot => {out_path}")

def plot_network(dmp_csv, out_path):
    """
    Build a simple co-annotation network over top CpGs based on shared gene symbols
    if available in the DMP file (expects a 'Gene' or 'UCSC_RefGene_Name' column).
    Falls back to no-op if annotations are missing.
    """
    if not os.path.exists(dmp_csv):
        raise FileNotFoundError(dmp_csv)

    df = pd.read_csv(dmp_csv)
    cpg_col = 'CpG' if 'CpG' in df.columns else df.columns[0]
    gene_col = None
    for col in ['Gene', 'UCSC_RefGene_Name', 'gene', 'GeneSymbol']:
        if col in df.columns:
            gene_col = col
            break

    top = df.head(200).copy()
    top_cpgs = top[cpg_col].astype(str).tolist()

    if gene_col is None:
        print("[INFO] No gene annotation column found; skipping network diagram.")
        return

    # Split multiple genes per CpG if present
    def split_genes(val):
        if pd.isna(val):
            return []
        s = str(val)
        if ';' in s:
            return [g.strip() for g in s.split(';') if g.strip()]
        if ',' in s:
            return [g.strip() for g in s.split(',') if g.strip()]
        return [s.strip()] if s.strip() else []

    top['__genes__'] = top[gene_col].apply(split_genes)
    # Build edges between CpGs that share at least one gene
    G = nx.Graph()
    for cpg in top_cpgs:
        G.add_node(cpg)

    gene_to_cpgs = {}
    for _, row in top.iterrows():
        cpg = str(row[cpg_col])
        for g in row['__genes__']:
            gene_to_cpgs.setdefault(g, []).append(cpg)

    for cpg_list in gene_to_cpgs.values():
        if len(cpg_list) > 1:
            # Connect all pairs within this gene bucket
            for i in range(len(cpg_list)):
                for j in range(i+1, len(cpg_list)):
                    G.add_edge(cpg_list[i], cpg_list[j])

    if G.number_of_edges() == 0:
        print("[INFO] No shared-gene edges detected; skipping network diagram.")
        return

    plt.figure(figsize=(10,8))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(
        G, pos,
        with_labels=False,
        node_size=80,
        node_color="steelblue",
        edge_color="lightgray"
    )
    plt.title("Co-annotation Network (Top DMPs)")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[SAVED] network_diagram => {out_path}")

# ----------------------------
# Main function
# ----------------------------
def main():
    # Resolve repository root dynamically
    current_dir = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.abspath(os.path.join(current_dir, "..", "..", "..", ".."))
    results_dir = os.path.join(repo_root, "results")
    os.makedirs(results_dir, exist_ok=True)

    data_csv = os.path.join(repo_root, "processed_data", "transformed_data.csv")
    if not os.path.exists(data_csv):
        print("[WARN] No transformed_data.csv. Falling back to cleaned_data.csv.")
        data_csv = os.path.join(repo_root, "processed_data", "cleaned_data.csv")
    if not os.path.exists(data_csv):
        print("[WARN] No cleaned_data.csv. Exiting visualization early.")
        return

    df = pd.read_csv(data_csv)
    if "Condition" not in df.columns:
        print("[ERROR] Condition missing in data. Exiting.")
        return

    # Normalize labels: merge any 'Noel ME' into 'ME'
    print("Unique conditions BEFORE fix:", df['Condition'].unique())
    df.loc[df['Condition'] == "Noel ME", "Condition"] = "ME"
    print("Unique conditions AFTER fix:", df['Condition'].unique())

    feat_cols = [c for c in df.columns if c != "Condition"]

    # If extremely high dimensional, do a quick PCA -> 64D
    if len(feat_cols) > 1000:
        print("[INFO] High-dimensional data detected => applying PCA to reduce to 64 features.")
        X_orig = df[feat_cols].values
        pca_reducer = PCA(n_components=64, random_state=42)
        X_reduced = pca_reducer.fit_transform(X_orig)
        latent_cols = [f"AE_{i}" for i in range(64)]
        df_new = pd.DataFrame(X_reduced, columns=latent_cols)
        df_new["Condition"] = df["Condition"].values
        df = df_new
        feat_cols = latent_cols

    # Impute + scale
    Xs = impute_and_scale(df, feat_cols)
    if Xs is None or Xs.shape[1] == 0:
        print("[WARN] impute_and_scale failed; skipping visualizations that require numeric matrix.")
        return

    y = df["Condition"].values
    imputed_df = pd.DataFrame(Xs, columns=[f"Imputed_{i}" for i in range(Xs.shape[1])])
    imputed_df["Condition"] = y

    # Train a small random forest to inspect feature importances
    # (Optional: skip if you do not want an internal model.)
    from sklearn.ensemble import RandomForestClassifier
    top_rf_cols = [c for c in feat_cols[:100] if c in df.columns]
    if len(top_rf_cols) > 0:
        X_for_rf = df[top_rf_cols].values
        y_rf = np.array([str(x) for x in df["Condition"].values])
        clf = RandomForestClassifier(n_estimators=200, random_state=42)
        clf.fit(X_for_rf, y_rf)
        preds_rf = clf.predict(X_for_rf)
        cm_rf = confusion_matrix(y_rf, preds_rf)
    else:
        X_for_rf = None
        cm_rf = None

    # DMP CSVs if they exist
    dmp_files = {
        "ME_vs_Control": os.path.join(results_dir, "DMP_ME_vs_Control.csv"),
        "LC_vs_Control": os.path.join(results_dir, "DMP_LC_vs_Control.csv"),
        "ME_vs_LC":      os.path.join(results_dir, "DMP_ME_vs_LC.csv")
    }

    # Generate visuals
    safe_call(plot_tsne, Xs, y, os.path.join(results_dir, "tsne_plot.png"))
    safe_call(plot_pca, Xs, y, os.path.join(results_dir, "pca_plot.png"))
    safe_call(plot_age_distribution, df, os.path.join(results_dir, "age_distribution.png"))
    safe_call(plot_sex_distribution, df, os.path.join(results_dir, "sex_distribution.png"))
    safe_call(plot_scatter_box, df,
              os.path.join(results_dir, "scatter_cpg.png"),
              os.path.join(results_dir, "box_cpg.png"))
    safe_call(plot_volcano,
              os.path.join(results_dir, "DMP_ME_vs_Control.csv"),
              os.path.join(results_dir, "volcano_plot.png"))
    safe_call(plot_upset, dmp_files, os.path.join(results_dir, "intersection_plot.png"))
    safe_call(plot_network,
              os.path.join(results_dir, "DMP_ME_vs_Control.csv"),
              os.path.join(results_dir, "network_diagram.png"))
    safe_call(plot_pca_scree, Xs, os.path.join(results_dir, "pca_scree_plot.png"))
    safe_call(plot_pca_biplot, Xs, y, os.path.join(results_dir, "pca_biplot.png"))
    safe_call(plot_tsne_with_perplexity, Xs, y, 10,
              os.path.join(results_dir, "tsne_plot_p10.png"))
    safe_call(plot_tsne_with_perplexity, Xs, y, 20,
              os.path.join(results_dir, "tsne_plot_p20.png"))
    safe_call(plot_umap_visualization, Xs, y,
              os.path.join(results_dir, "umap_plot.png"))
    safe_call(plot_correlation_heatmap, Xs,
              os.path.join(results_dir, "correlation_heatmap.png"))
    safe_call(plot_hierarchical_dendrogram, Xs,
              os.path.join(results_dir, "hierarchical_dendrogram.png"))

    # Box & violin for the first feature, if any
    first_feat = feat_cols[0] if len(feat_cols) > 0 else None
    if first_feat is not None:
        safe_call(plot_box_feature, df, first_feat,
                  os.path.join(results_dir, "boxplot_feature.png"))
        safe_call(plot_violin_feature, df, first_feat,
                  os.path.join(results_dir, "violin_feature.png"))

    # Pairplot & KDE for up to first 5 features
    n_plot_feats = min(5, len(feat_cols))
    subset_feats = [c for c in feat_cols[:n_plot_feats] if c in df.columns]
    if len(subset_feats) > 0 and "Condition" in df.columns:
        safe_call(plot_pairplot, df, subset_feats,
                  os.path.join(results_dir, "pairplot.png"))
        safe_call(plot_kde_feature, df, subset_feats[0],
                  os.path.join(results_dir, "kde_plot.png"))

    # Condition count bar
    safe_call(plot_condition_count, df,
              os.path.join(results_dir, "condition_count.png"))

    # If we have a small random forest trained above, show feature importances & confusion
    if X_for_rf is not None and cm_rf is not None:
        safe_call(plot_rf_feature_importance, top_rf_cols, clf.feature_importances_, 20,
                  os.path.join(results_dir, "rf_feature_importances.png"))
        safe_call(plot_classifier_confusion_matrix, cm_rf, np.unique(df["Condition"].values),
                  os.path.join(results_dir, "confusion_matrix.png"))

    # Optional Sankey diagram (requires plotly + kaleido)
    safe_call(plot_sankey_diagram,
              os.path.join(results_dir, "sankey_diagram.png"))

    # 3D PCA / t-SNE
    safe_call(plot_pca_3d, Xs, y,
              os.path.join(results_dir, "pca_3d_plot.png"))
    safe_call(plot_tsne_3d, Xs, y,
              os.path.join(results_dir, "tsne_3d_plot.png"))

    # Multi-panel summary
    safe_call(plot_multi_panel_summary, imputed_df,
              os.path.join(results_dir, "multi_panel_summary.png"))

    # Optionally do a smaller pairplot subset for feats [5:10] if we have >10 feats
    if len(feat_cols) > 10:
        subset2 = [c for c in feat_cols[5:10] if c in df.columns]
        if len(subset2) > 0:
            safe_call(plot_pairplot, df, subset2,
                      os.path.join(results_dir, "pairplot_subset.png"))

    print("=== All visualization steps completed. ===")


if __name__ == "__main__":
    main()


# ======================
# File: src/original/steps/05_preprocessing.py
# ======================

#!/usr/bin/env python3
"""
05_preprocessing.py

Purpose:
  1) Load a CSV that typically has (row=sample) x (feature-columns) plus a "Condition" column.
  2) Remove duplicates.
  3) Check that "Condition" is present.
  4) Convert non-numeric columns (except "Condition") to numeric (coercing errors), cast numeric columns to float32,
     and fill numeric NaNs using column medians.
  5) Drop any columns that remain entirely NaN after median fill, but do NOT drop all columns if that leads to zero.
  6) Save cleaned CSV.

Usage example:
  python 05_preprocessing.py --csv /path/to/filtered_biomarker_matrix.csv \
                             --out /path/to/cleaned_data.csv \
                             --method auto --chunksize 100000
"""

import argparse
import logging
import os
import sys
import time
from typing import List

import numpy as np
import pandas as pd

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

def read_csv_auto(csv_path: str, chunksize: int = 100000) -> pd.DataFrame:
    logging.info("Reading header to count columns...")
    try:
        with open(csv_path, 'r') as f:
            header = f.readline().strip().split(',')
    except Exception as e:
        logging.error(f"Error reading header: {e}")
        sys.exit(1)
    num_cols = len(header)
    logging.info(f"Detected {num_cols} columns.")

    if num_cols > 10000:
        try:
            import polars as pl
            logging.info("Detected wide CSV; using Polars for fast reading.")
            df = pl.read_csv(csv_path).to_pandas()
        except ImportError:
            logging.warning("Polars not installed; falling back to pandas with low_memory=False.")
            df = pd.read_csv(csv_path, low_memory=False)
    else:
        logging.info(f"Using chunked pandas read (chunksize={chunksize}).")
        chunks = []
        for chunk in pd.read_csv(csv_path, chunksize=chunksize):
            chunk = chunk.drop_duplicates()
            chunks.append(chunk)
        df = pd.concat(chunks, ignore_index=True)
    return df

def force_numeric_conversion(df: pd.DataFrame, exclude_cols: List[str]) -> pd.DataFrame:
    for col in df.columns:
        if col in exclude_cols:
            continue
        if not pd.api.types.is_numeric_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], errors='coerce')
            logging.info(f"Converted column '{col}' to numeric (non-numeric => NaN).")
    return df

def fill_numeric_medians(df: pd.DataFrame, condition_col: str = "Condition") -> pd.DataFrame:
    logging.info("Extracting numeric columns for median imputation...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) == 0:
        logging.warning("No numeric columns found!")
        return df

    # Convert to float32
    df[numeric_cols] = df[numeric_cols].astype(np.float32)
    logging.info(f"Numeric data shape => {df[numeric_cols].shape}")

    # Compute column medians
    numeric_data = df[numeric_cols].values
    medians = np.nanmedian(numeric_data, axis=0)
    median_series = pd.Series(medians, index=numeric_cols)
    df[numeric_cols] = df[numeric_cols].fillna(median_series)

    # Now drop columns that remain all-NaN
    all_na_cols = df[numeric_cols].columns[df[numeric_cols].isna().all()]
    if len(all_na_cols) > 0:
        # But ensure we do not drop *all* numeric columns
        if len(all_na_cols) == len(numeric_cols):
            logging.warning("All numeric columns are fully NaN. Preserving them to avoid zero columns.")
        else:
            df.drop(columns=all_na_cols, inplace=True)
            logging.info(f"Dropped {len(all_na_cols)} columns that were all-NaN after median fill.")

    return df

def main() -> None:
    parser = argparse.ArgumentParser(description="Preprocessing (fill NA, drop duplicates).")
    parser.add_argument("--csv", required=True,
                        help="Input CSV with row=sample, columns=features, last col=Condition.")
    parser.add_argument("--out", required=True,
                        help="Output CSV path for cleaned data.")
    parser.add_argument("--method", default="auto", choices=["auto", "chunked", "polars"],
                        help="Reading method. 'auto' tries best approach automatically.")
    parser.add_argument("--chunksize", type=int, default=100000,
                        help="Chunk size for reading CSV if chunked approach is used.")
    args = parser.parse_args()

    start_time = time.time()

    if not os.path.exists(args.csv):
        logging.error(f"CSV not found: {args.csv}")
        sys.exit(1)

    if args.method == "auto":
        df = read_csv_auto(args.csv, chunksize=args.chunksize)
    elif args.method == "polars":
        try:
            import polars as pl
            logging.info("Using Polars to read CSV with default threading.")
            df_pl = pl.read_csv(args.csv)
            df = df_pl.to_pandas()
            df = df.drop_duplicates()
        except ImportError:
            logging.warning("Polars not installed; falling back to auto mode.")
            df = read_csv_auto(args.csv, chunksize=args.chunksize)
    else:  # chunked
        chunks = []
        logging.info(f"Using chunked pandas read (chunksize={args.chunksize}).")
        for chunk in pd.read_csv(args.csv, chunksize=args.chunksize):
            chunk = chunk.drop_duplicates()
            chunks.append(chunk)
        df = pd.concat(chunks, ignore_index=True)
    logging.info(f"After reading, shape = {df.shape}")

    # Drop any duplicate rows
    before = len(df)
    df = df.drop_duplicates()
    after = len(df)
    logging.info(f"Dropped {before - after} duplicate rows; new shape = {df.shape}")

    if "Condition" not in df.columns:
        logging.error("'Condition' column not found; aborting.")
        sys.exit(1)
    df["Condition"] = df["Condition"].astype(str)

    # Convert everything except Condition => numeric
    df = force_numeric_conversion(df, exclude_cols=["Condition"])
    df = fill_numeric_medians(df, "Condition")

    out_dir = os.path.dirname(args.out)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)
        logging.info(f"Created directory {out_dir}.")

    logging.info(f"Saving cleaned CSV to {args.out} with final shape {df.shape}")
    df.to_csv(args.out, index=False)

    elapsed_time = time.time() - start_time
    logging.info(f"Preprocessing complete in {elapsed_time:.2f} seconds.")
    logging.info("=== Done Preprocessing. ===")

if __name__ == "__main__":
    main()

# ======================
# File: src/original/steps/12_evaluate_results.py
# ======================

#!/usr/bin/env python3
"""
12_evaluate_results.py

Purpose:
  Loads the saved advanced transformer model and evaluates it on the dataset.
  Prints the confusion matrix and classification report.
  
Usage:
  python 12_evaluate_results.py
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix, classification_report

from transformer_classifier import (
    MethylationChunkedDataset,
    methylation_collate_fn,
    TransformerClassifier,
    CHUNK_SIZE
)

def evaluate_transformer(model_path, csv_path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dataset = MethylationChunkedDataset(csv_path)
    loader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=methylation_collate_fn)
    seq_len_init = max(len(chunks) for chunks in dataset.data_chunks)
    num_classes = len(dataset.classes)
    model = TransformerClassifier(seq_len=seq_len_init, num_classes=num_classes).to(device)
    sd = torch.load(model_path, map_location=device)
    model.load_state_dict(sd)
    model.eval()
    preds_all = []
    labels_all = []
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            out = model(X_batch)
            _, predicted = torch.max(out, dim=1)
            preds_all.append(predicted.cpu().numpy())
            labels_all.append(y_batch.cpu().numpy())
    preds_all = np.concatenate(preds_all)
    labels_all = np.concatenate(labels_all)
    cm = confusion_matrix(labels_all, preds_all)
    print("[CONFUSION MATRIX]\n", cm)
    print("\n[CLASSIFICATION REPORT]\n",
          classification_report(labels_all, preds_all, digits=4))

def main():
    model_path = "/Volumes/T9/EpiMECoV/results/transformer_model.pth"
    data_csv = "/Volumes/T9/EpiMECoV/processed_data/filtered_biomarker_matrix.csv"
    evaluate_transformer(model_path, data_csv)

if __name__ == "__main__":
    main()

# ======================
# File: src/original/steps/11_report_shared_distinct_dmps.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# report_shared_distinct_dmps.R
#
# Purpose:
#   - Reads your three DMP CSVs (ME vs Control, LC vs Control, ME vs LC).
#   - Identifies:
#       * set of ME_CpGs
#       * set of LC_CpGs
#       * set of ME_LC_CpGs
#     and prints:
#       * Shared in all three
#       * Distinct to ME vs Ctrl only
#       * Distinct to LC vs Ctrl only
#       * Distinct to ME vs LC only
#   - Saves them in separate text files for your reference.
#
# Usage:
#   Rscript report_shared_distinct_dmps.R
###############################################################################
library(data.table)

epi_root <- "/Volumes/T9/EpiMECoV"
res_dir  <- file.path(epi_root, "results")

dmp_me_ctrl <- file.path(res_dir, "DMP_ME_vs_Control.csv")
dmp_lc_ctrl <- file.path(res_dir, "DMP_LC_vs_Control.csv")
dmp_me_lc   <- file.path(res_dir, "DMP_ME_vs_LC.csv")

if (!all(file.exists(c(dmp_me_ctrl, dmp_lc_ctrl, dmp_me_lc)))) {
  stop("One or more DMP CSVs are missing from results/ folder.")
}

df_me_ctrl <- fread(dmp_me_ctrl)
df_lc_ctrl <- fread(dmp_lc_ctrl)
df_me_lc   <- fread(dmp_me_lc)

# We assume the first column is the CpG or rownames
cpg_me_ctrl <- df_me_ctrl[[1]]
cpg_lc_ctrl <- df_lc_ctrl[[1]]
cpg_me_lc   <- df_me_lc[[1]]

set_me_ctrl <- unique(cpg_me_ctrl)
set_lc_ctrl <- unique(cpg_lc_ctrl)
set_me_lc   <- unique(cpg_me_lc)

# Intersection / union
shared_all   <- Reduce(intersect, list(set_me_ctrl, set_lc_ctrl, set_me_lc))
distinct_me_ctrl <- set_me_ctrl[ !(set_me_ctrl %in% c(set_lc_ctrl, set_me_lc)) ]
distinct_lc_ctrl <- set_lc_ctrl[ !(set_lc_ctrl %in% c(set_me_ctrl, set_me_lc)) ]
distinct_me_lc   <- set_me_lc[   !(set_me_lc   %in% c(set_me_ctrl, set_lc_ctrl)) ]

cat("\n=== Shared in all three comparisons:", length(shared_all), "CpGs ===\n")
cat("Some examples:\n")
print(head(shared_all, 10))

cat("\nME vs Ctrl only (distinct):", length(distinct_me_ctrl), "\n")
cat("LC vs Ctrl only (distinct):", length(distinct_lc_ctrl), "\n")
cat("ME vs LC only (distinct):",   length(distinct_me_lc),   "\n")

fwrite(data.table(CpG=shared_all), file.path(res_dir, "Shared_in_all_three.txt"))
fwrite(data.table(CpG=distinct_me_ctrl), file.path(res_dir, "Distinct_ME_Ctrl_only.txt"))
fwrite(data.table(CpG=distinct_lc_ctrl), file.path(res_dir, "Distinct_LC_Ctrl_only.txt"))
fwrite(data.table(CpG=distinct_me_lc),   file.path(res_dir, "Distinct_ME_LC_only.txt"))

cat("\n=== Shared/Distinct DMP lists saved in 'results/' directory. ===\n")

# ======================
# File: src/original/steps/06_feature_engineer.py
# ======================

#!/usr/bin/env python3
"""
06_feature_engineer.py

Purpose:
  - Reads a cleaned CSV (row=sample, columns=features, plus Condition).
  - Trains a Variational Autoencoder (VAE) for dimensionality reduction instead of PCA.
  - During training, records the reconstruction loss and KL divergence.
  - After training, prints the dimensions of the latent features produced.
  - It saves:
      • The VAE loss convergence curves as "vae_loss_curve.png"
      • The transformed data CSV ("transformed_data.csv")
      • A “head” file ("transformed_data_head.csv") showing the first few rows of latent features.
      
Usage example:
  python 06_feature_engineer.py \
    --csv /Volumes/T9/EpiMECoV/processed_data/cleaned_data.csv \
    --out /Volumes/T9/EpiMECoV/processed_data/transformed_data.csv \
    --latent_dim 64 \
    --epochs 20 \
    --batch_size 64 \
    --lr 0.001 \
    --dropout 0.1 \
    --use_scale True
"""

import argparse
import os
import time

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

try:
    from sklearn.preprocessing import StandardScaler
    HAVE_SKLEARN = True
except ImportError:
    HAVE_SKLEARN = False
import matplotlib.pyplot as plt

class BetaDataset(Dataset):
    """Simple PyTorch Dataset that extracts numeric columns as X, ignoring 'Condition'."""
    def __init__(self, df, condition_col="Condition"):
        self.condition_col = condition_col
        self.features = df.drop(columns=[condition_col]).values.astype(np.float32)
        self.labels = df[condition_col].values  # not used for VAE target
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return self.features[idx]

class VAE(nn.Module):
    """
    Variational Autoencoder with one hidden layer.
    """
    def __init__(self, input_dim, latent_dim=64, dropout_p=0.0):
        super(VAE, self).__init__()
        hidden_dim = 256
        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        self.dropout = nn.Dropout(dropout_p)
        # Decoder
        self.fc2 = nn.Linear(latent_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, input_dim)
    
    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        h1 = self.dropout(h1)
        mu = self.fc_mu(h1)
        logvar = self.fc_logvar(h1)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h2 = torch.relu(self.fc2(z))
        h2 = self.dropout(h2)
        return self.fc3(h2)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar

def loss_function(recon_x, x, mu, logvar):
    # Reconstruction loss: MSE
    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')
    # KL divergence
    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss, kl_loss

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", required=True,
                        help="Path to cleaned CSV (row=sample, columns=features+Condition).")
    parser.add_argument("--out", required=True,
                        help="Output CSV path for transformed data (latent features + Condition).")
    parser.add_argument("--latent_dim", type=int, default=64)
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--dropout", type=float, default=0.0)
    parser.add_argument("--use_scale", type=bool, default=False,
                        help="Whether to standard-scale the numeric columns.")
    parser.add_argument("--condition_col", default="Condition")
    args = parser.parse_args()

    print("=== Feature Engineering (VAE) ===")
    if not os.path.exists(args.csv):
        raise FileNotFoundError(f"Could not find CSV => {args.csv}")

    # 1) Load DataFrame
    df = pd.read_csv(args.csv)
    print(f"Input shape = {df.shape}")

    if args.condition_col not in df.columns:
        raise ValueError(f"No '{args.condition_col}' column found in DataFrame.")

    # Remove duplicate columns if any
    df = df.loc[:, ~df.columns.duplicated()].copy()

    # 2) Separate numeric vs. Condition
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if args.condition_col in numeric_cols:
        numeric_cols.remove(args.condition_col)
    keep_cols = numeric_cols + [args.condition_col]
    df = df[keep_cols]
    print(f"Using numeric columns + {args.condition_col}, shape = {df.shape}")

    # 3) Optional scaling
    if args.use_scale:
        if not HAVE_SKLEARN:
            raise ImportError("scikit-learn is not installed => cannot use --use_scale.")
        sc = StandardScaler()
        df_num = sc.fit_transform(df[numeric_cols].values)
        df[numeric_cols] = df_num
        print("[INFO] Applied standard scaling to numeric columns.")

    # 4) Create Dataset and determine input dimension
    dataset = BetaDataset(df, condition_col=args.condition_col)
    input_dim = dataset.features.shape[1]
    print(f"[INFO] VAE input dimension = {input_dim}")

    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    # 5) Build VAE, optimizer, and loss function
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = VAE(input_dim, latent_dim=args.latent_dim, dropout_p=args.dropout).to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Record training losses for convergence curves
    recon_losses = []
    kl_losses = []
    total_losses = []

    print(f"Training VAE for {args.epochs} epochs on {len(dataset)} samples (batch size = {args.batch_size}).")
    for ep in range(1, args.epochs + 1):
        model.train()
        total_loss_ep = 0.0
        total_recon_ep = 0.0
        total_kl_ep = 0.0
        for X_batch in dataloader:
            X_batch = X_batch.to(device)
            optimizer.zero_grad()
            recon, mu, logvar = model(X_batch)
            recon_loss, kl_loss = loss_function(recon, X_batch, mu, logvar)
            loss = recon_loss + kl_loss
            loss.backward()
            optimizer.step()
            total_loss_ep += loss.item() * X_batch.size(0)
            total_recon_ep += recon_loss.item() * X_batch.size(0)
            total_kl_ep += kl_loss.item() * X_batch.size(0)
        avg_loss = total_loss_ep / len(dataset)
        avg_recon = total_recon_ep / len(dataset)
        avg_kl = total_kl_ep / len(dataset)
        total_losses.append(avg_loss)
        recon_losses.append(avg_recon)
        kl_losses.append(avg_kl)
        print(f"[Epoch {ep}/{args.epochs}] Total Loss = {avg_loss:.6f}, Recon Loss = {avg_recon:.6f}, KL Loss = {avg_kl:.6f}")

    # Plot and save the loss curves
    plt.figure(figsize=(8, 6))
    plt.plot(range(1, args.epochs + 1), total_losses, marker='o', label="Total Loss")
    plt.plot(range(1, args.epochs + 1), recon_losses, marker='s', label="Reconstruction Loss")
    plt.plot(range(1, args.epochs + 1), kl_losses, marker='^', label="KL Divergence")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("VAE Training Loss Convergence")
    plt.legend()
    plt.grid(True)
    loss_curve_path = os.path.join(os.path.dirname(args.out), "vae_loss_curve.png")
    plt.savefig(loss_curve_path, dpi=300)
    plt.close()
    print(f"[INFO] Loss convergence curves saved to {loss_curve_path}")

    # 6) Generate latent features for all samples (using the mean, mu)
    model.eval()
    with torch.no_grad():
        X_all = torch.from_numpy(dataset.features).to(device)
        _, mu_all, _ = model(X_all)
        latents = mu_all.cpu().numpy()
    print(f"[INFO] Latent features shape: {latents.shape}")

    latent_df = pd.DataFrame(latents, columns=[f"VAE_{i}" for i in range(args.latent_dim)])
    latent_df[args.condition_col] = df[args.condition_col].values
    head_df = latent_df.head(10)
    head_out = os.path.join(os.path.dirname(args.out), "transformed_data_head.csv")
    head_df.to_csv(head_out, index=False)
    print(f"[INFO] Head of latent features saved to {head_out}")

    latent_df.to_csv(args.out, index=False)
    print(f"[INFO] Transformed data saved to {args.out}")

    print("=== Done feature engineering with VAE. ===")


if __name__ == "__main__":
    main()

# ======================
# File: src/original/steps/01_unify_IDATs_And_Transpose.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# unify_IDATs_And_Transpose.R
#
# PURPOSE:
#   A single, combined script that:
#   (1) Installs + loads necessary Bioconductor and CRAN packages.
#   (2) Detects and merges IDAT files from subfolders (ME, LC, controls).
#   (3) Filters by detection p-values, performs Noob normalization, maps to genome.
#   (4) Removes cross-reactive + SNP-affected probes (if desired).
#   (5) Merges 450K and EPIC sets into a single GenomicMethylSet.
#   (6) Verifies Condition labeling, then saves the merged set + Beta matrix.
#   (7) Transposes that Beta matrix into “samples x probes,” appends Condition 
#       as the last column.
#   (8) **New:** Applies BMIQ to correct type II probe bias and then uses RUVM 
#       (via RUVfit/RUVadj) to adjust for batch effects.
#
# USAGE:
#   Rscript unify_IDATs_And_Transpose.R
###############################################################################

cat("=== unify_IDATs_And_Transpose START ===\n", as.character(Sys.time()), "\n")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (1) Install and Load Packages
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[1] Installing/loading required packages...\n")

if (!requireNamespace("remotes", quietly = TRUE)) {
  install.packages("remotes")
}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}

# Personal R library path (optional). Adjust as you wish:
personal_lib <- "~/R/library"
if (!dir.exists(personal_lib)) {
  dir.create(personal_lib, recursive = TRUE, showWarnings = FALSE)
}
.libPaths(c(personal_lib, .libPaths()))

pkg_list <- c(
  "minfi",
  "IlluminaHumanMethylation450kmanifest",
  "IlluminaHumanMethylationEPICmanifest",
  "BiocParallel",
  "minfiData",
  "IlluminaHumanMethylation450kanno.ilmn12.hg19",
  "maxprobes",   # to handle cross-reactive or SNP-affected probes if desired
  "wateRmelon",  # for BMIQ normalization
  "sva"          # for ComBat batch-effect correction
)

install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    if (pkg %in% c("maxprobes")) {
      # 'maxprobes' is not on CRAN/Bioc => install from GitHub
      remotes::install_github("markgene/maxprobes",
                              lib = personal_lib,
                              upgrade = "never")
    } else {
      BiocManager::install(pkg, ask = FALSE, lib = personal_lib)
    }
  }
  library(pkg, character.only = TRUE, lib.loc = personal_lib)
}

for (p in pkg_list) {
  install_if_missing(p)
}

library(BiocParallel)
num_cores <- max(1, parallel::detectCores() - 1)
register(MulticoreParam(num_cores))
cat("[INFO] Using", num_cores, "cores.\n")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (2) Define Data and Output Directories + max_idats
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# You can control how many IDATs to sample with this variable:
# - If set to -1, it uses ALL IDATs found (forces balanced sampling)
# - Otherwise it picks exactly that many IDATs in a balanced way across conditions
max_idats <- 6   # <--- changed here

epi_root <- "/Volumes/T9/EpiMECoV"
data_root_dir <- file.path(epi_root, "data")

# Condition subfolders: "ME", "LC", "controls"
cond_names <- c("ME", "LC", "Control")
dir_names  <- c("ME", "LC", "controls") 
condition_dirs <- file.path(data_root_dir, dir_names)
names(condition_dirs) <- cond_names

for (cn in names(condition_dirs)) {
  if (!dir.exists(condition_dirs[cn])) {
    stop(paste("No folder for condition", cn, "=>", condition_dirs[cn]))
  }
  cat("[OK]", cn, "found at:", condition_dirs[cn], "\n")
}

processed_dir <- file.path(epi_root, "processed_data")
if (!dir.exists(processed_dir)) {
  dir.create(processed_dir, recursive = TRUE, showWarnings = FALSE)
  cat("[INFO] Created 'processed_data' folder =>", processed_dir, "\n")
} else {
  cat("[INFO] Using existing output dir =>", processed_dir, "\n")
}

# Helper function to pick a balanced subset across conditions
balancedSubset <- function(sheet, desired_total) {
  conds <- unique(sheet$Condition)
  k <- length(conds)
  if (k <= 1) {
    cat("[INFO] Only one condition => returning entire sheet.\n")
    return(sheet)
  }
  
  each <- floor(desired_total / k)
  out_list <- list()
  
  # pick an equal number from each condition
  for (cn in conds) {
    subdf <- sheet[sheet$Condition == cn, ]
    n_pick <- min(nrow(subdf), each)
    if (n_pick > 0) {
      idx <- sample(seq_len(nrow(subdf)), n_pick)
      out_list[[cn]] <- subdf[idx, ]
    } else {
      out_list[[cn]] <- subdf[0, ]
    }
  }
  
  out_df <- do.call(rbind, out_list)
  used_ct <- nrow(out_df)
  leftover <- desired_total - used_ct
  
  # If leftover > 0, pick random from entire set that wasn't used yet
  if (leftover > 0) {
    used_names <- out_df$Sample_Name
    remain <- sheet[!(sheet$Sample_Name %in% used_names), ]
    if (nrow(remain) < leftover) leftover <- nrow(remain)
    if (leftover > 0) {
      idx2 <- sample(seq_len(nrow(remain)), leftover)
      out_df <- rbind(out_df, remain[idx2, ])
    }
  }
  
  return(out_df)
}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (3) IDAT Detection & Sample Sheet
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[3] Scanning IDAT files for each condition...\n")

findIDATsForCondition <- function(dirPath, condName) {
  # Recursively locate all *.idat
  all_idats <- list.files(dirPath, pattern = "\\.idat$", 
                          full.names = TRUE, recursive = TRUE)
  gPaths <- all_idats[grepl("Grn\\.idat$", all_idats, ignore.case = TRUE)]
  rPaths <- all_idats[grepl("Red\\.idat$", all_idats, ignore.case = TRUE)]
  gBase <- sub("_Grn\\.idat$", "", basename(gPaths), ignore.case = TRUE)
  rBase <- sub("_Red\\.idat$", "", basename(rPaths), ignore.case = TRUE)
  common <- intersect(gBase, rBase)
  if (length(common) == 0) {
    cat("[WARNING] No matching Grn/Red for condition:", condName, "\n")
    return(NULL)
  }
  df <- data.frame(
    Sample_Name = common,
    Basename    = rep("", length(common)),
    Condition   = rep(condName, length(common)),
    stringsAsFactors = FALSE
  )
  for (i in seq_along(common)) {
    sid <- common[i]
    # Locate matching *Grn.idat
    gMatch <- gPaths[basename(gPaths) == paste0(sid, "_Grn.idat")]
    if (length(gMatch) == 1) {
      basePath <- sub("_Grn\\.idat$", "", gMatch, ignore.case = TRUE)
      df$Basename[i] <- basePath
    } else {
      df$Basename[i] <- NA
    }
  }
  df <- df[!is.na(df$Basename), ]
  return(df)
}

all_df_list <- lapply(names(condition_dirs), function(cn) {
  findIDATsForCondition(condition_dirs[cn], cn)
})
sample_sheet <- do.call(rbind, all_df_list)
sample_sheet <- sample_sheet[!is.na(sample_sheet$Basename), ]
cat("Total samples across all conditions:", nrow(sample_sheet), "\n")

cat("[INFO] Picking a balanced subset across conditions.\n")
set.seed(123)
# Force balanced sampling even when max_idats is -1.
if (max_idats < 1) {
  counts <- table(sample_sheet$Condition)
  min_count <- min(counts)
  balanced_total <- min_count * length(counts)
  actual_idats <- balanced_total
  cat("[INFO] max_idats = -1, so using", min_count, "samples per condition (total =", balanced_total, ").\n")
} else {
  actual_idats <- max_idats
}
sample_sheet <- balancedSubset(sample_sheet, actual_idats)
cat("Now using", nrow(sample_sheet), "samples.\n")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (4) Detect 450K vs EPIC array
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[4] Checking array platform for each sample...\n")

detectPlatform <- function(bPath) {
  rg <- tryCatch(
    read.metharray(bPath, verbose = FALSE, force = TRUE),
    error = function(e) { cat("[ERR] Could not read:", bPath, "\n"); return(NULL) }
  )
  if (is.null(rg)) {
    return("Unknown")
  }
  arr <- annotation(rg)[["array"]]
  if (is.null(arr) || arr == "") arr <- "Unknown"
  return(arr)
}

library(parallel)  # or use BiocParallel
platforms <- bplapply(sample_sheet$Basename, detectPlatform)
sample_sheet$Platform <- unlist(platforms)

tbl_platform <- table(sample_sheet$Platform, sample_sheet$Condition)
cat("Platform counts:\n")
print(tbl_platform)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (5) Build RGChannelSets for each platform in parallel
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[5] Building RGChannelSets (450K, EPIC) in parallel...\n")

sheet_450k <- sample_sheet[sample_sheet$Platform == "IlluminaHumanMethylation450k", ]
sheet_epic <- sample_sheet[sample_sheet$Platform == "IlluminaHumanMethylationEPIC", ]

buildRGChannelSet <- function(sheet, arrayname="450K") {
  if (nrow(sheet) == 0) {
    cat("[INFO]", arrayname, " => no samples.\n")
    return(NULL)
  }
  cat("[INFO] Reading", nrow(sheet), "samples =>", arrayname, "\n")
  rgSet <- read.metharray.exp(targets = sheet, force = TRUE, verbose = FALSE)
  # Store Condition in pData
  pData(rgSet)$Condition <- sheet$Condition
  cat("[INFO]", arrayname, " => read", sum(!is.null(rgSet)), "non-null.\n")
  return(rgSet)
}

rgSet_450k <- buildRGChannelSet(sheet_450k, arrayname="450K")
if (!is.null(rgSet_450k)) {
  out_rds_450k <- file.path(processed_dir, "450K_Combined_RGChannelSet.rds")
  saveRDS(rgSet_450k, out_rds_450k)
  cat("[SAVED]", out_rds_450k, "\n")
}

rgSet_epic <- buildRGChannelSet(sheet_epic, arrayname="EPIC")
if (!is.null(rgSet_epic)) {
  out_rds_epic <- file.path(processed_dir, "EPIC_Combined_RGChannelSet.rds")
  saveRDS(rgSet_epic, out_rds_epic)
  cat("[SAVED]", out_rds_epic, "\n")
}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (6) Preprocess each platform (detectionP, Noob, mapToGenome, BMIQ, RUVM)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[6] Preprocessing each platform (detectionP, Noob, mapToGenome, BMIQ, RUVM)...\n")

preprocessPlatform <- function(rgSet, array_label="450K") {
  if (is.null(rgSet)) {
    return(NULL)
  }
  cat("[INFO]", array_label, "=> detection p-value filter...\n")
  detP <- detectionP(rgSet)
  keep <- colMeans(detP < 0.01) > 0.98
  rgSet <- rgSet[, keep, drop=FALSE]
  
  cat("[INFO]", array_label, "=> preprocessNoob (background correction) ...\n")
  gmSet <- preprocessNoob(rgSet)
  
  # Optional quantile normalization if desired per paper phrasing
  cat("[INFO]", array_label, "=> preprocessQuantile (quantile normalization) ...\n")
  gmSet <- preprocessQuantile(gmSet)
  
  cat("[INFO]", array_label, "=> mapToGenome...\n")
  gmSet <- mapToGenome(gmSet)
  
  # --- Apply BMIQ Normalization ---
  cat("[INFO] Applying BMIQ normalization...\n")
  betas <- getBeta(gmSet)
  # Get probe design information from annotation
  anno <- getAnnotation(gmSet)
  # Convert design type ("I" or "II") to numeric (1 for type I, 2 for type II)
  design <- ifelse(anno$Type == "I", 1, 2)
  betas_bmiq <- betas  # initialize matrix for BMIQ-adjusted betas
  for (i in 1:ncol(betas)) {
    sample_beta <- betas[, i]
    res <- BMIQ(beta.v = sample_beta, design.v = design, nfit = 5000,
                th1.v = c(0.2, 0.75), th2.v = NULL, niter = 5, tol = 0.001,
                plots = FALSE, pri = FALSE)
    betas_bmiq[, i] <- res$nbeta
    cat("[INFO] BMIQ normalization applied to sample", i, "\n")
  }
  # Update the GenomicMethylSet with BMIQ-normalized beta values
  gmSet_bmiq <- gmSet
  assay(gmSet_bmiq, "Beta") <- betas_bmiq
  outFileCSV_BMIQ <- file.path(processed_dir, paste0(array_label, "_BetaValues_BMIQ.csv"))
  write.csv(betas_bmiq, outFileCSV_BMIQ, quote = FALSE)
  cat("[SAVED]", outFileCSV_BMIQ, "\n")
  
  # --- Apply ComBat Batch Correction (sva) ---
  cat("[INFO] Applying ComBat batch-effect correction (sva)...\n")
  betas_bmiq <- assay(gmSet_bmiq, "Beta")
  # Build batch vector: if 'Slide' info exists in pData, use it; otherwise use 'Array'
  pd <- pData(gmSet_bmiq)
  batch <- NULL
  if (!is.null(pd$Slide)) {
    batch <- as.factor(pd$Slide)
  } else if (!is.null(pd$Array)) {
    batch <- as.factor(pd$Array)
  } else {
    # fallback: use Condition as batch proxy (not ideal, but avoids failure)
    batch <- as.factor(pd$Condition)
  }
  # ComBat expects genes x samples matrix (features x samples)
  require(sva)
  combat_corrected <- ComBat(dat = betas_bmiq, batch = batch, par.prior = TRUE, prior.plots = FALSE)
  
  # Update GenomicMethylSet with ComBat-corrected betas
  gmSet_corrected <- gmSet_bmiq
  assay(gmSet_corrected, "Beta") <- combat_corrected
  
  outFileRDS_corrected <- file.path(processed_dir, paste0(array_label, "_Final_GenomicMethylSet_ComBat.rds"))
  saveRDS(gmSet_corrected, outFileRDS_corrected)
  cat("[SAVED]", outFileRDS_corrected, "\n")
  
  outFileCSV_corrected <- file.path(processed_dir, paste0(array_label, "_BetaValues_ComBat.csv"))
  write.csv(combat_corrected, outFileCSV_corrected, quote = FALSE)
  cat("[SAVED]", outFileCSV_corrected, "\n")
  
  return(gmSet_corrected)
}

gmSet_450k <- preprocessPlatform(rgSet_450k, "450K")
gmSet_epic <- preprocessPlatform(rgSet_epic, "EPIC")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (7) Removes cross-reactive + SNP-affected probes (if desired).
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Example usage (commented out):
# if (!is.null(gmSet_450k)) {
#   gmSet_450k <- removeCrossReactive(gmSet_450k)
# }
# if (!is.null(gmSet_epic)) {
#   gmSet_epic <- removeCrossReactive(gmSet_epic)
# }

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (8) Merge 450K & EPIC if both exist
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("\n[8] Merging 450K & EPIC if both exist...\n")
if (!is.null(gmSet_450k) && !is.null(gmSet_epic)) {
  cat("[convertArray] Casting EPIC to IlluminaHumanMethylation450k\n")
  gmSet_epic_450k <- convertArray(gmSet_epic,
                                  outType = "IlluminaHumanMethylation450k",
                                  verbose = FALSE)
  combined <- combineArrays(gmSet_450k, gmSet_epic_450k)
  
  cond_450k <- pData(gmSet_450k)$Condition
  cond_epic <- pData(gmSet_epic_450k)$Condition
  new_cond <- c(cond_450k, cond_epic)
  if (length(new_cond) == ncol(combined)) {
    pData(combined)$Condition <- new_cond
  }
  
  out_merged_rds <- file.path(processed_dir, "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds")
  saveRDS(combined, out_merged_rds)
  cat("[SAVED]", out_merged_rds, "\n")
  
  combined_betas <- getBeta(combined)
  out_merged_csv <- file.path(processed_dir, "Merged_450K_EPIC_BetaValues_with_Condition.csv")
  write.csv(combined_betas, out_merged_csv, quote = FALSE)
  cat("[SAVED]", out_merged_csv, "\n")
  
  cat("\nNow => generating transposed Beta with Condition as final column.\n")
  cond_vec <- pData(combined)$Condition
  if (is.null(cond_vec)) cond_vec <- rep("Unknown", ncol(combined_betas))
  
  beta_t <- t(combined_betas)
  beta_df <- as.data.frame(beta_t)
  beta_df$Condition <- cond_vec
  
  outFileTransposed <- file.path(processed_dir, "Beta_Transposed_with_Condition.csv")
  write.csv(beta_df, outFileTransposed, row.names = TRUE, quote = FALSE)
  cat("[INFO] Saving =>", outFileTransposed, "\n")
  
} else if (!is.null(gmSet_450k)) {
  cat("[INFO] Only 450K => saving Beta_Transposed_with_Condition for that alone.\n")
  combined <- gmSet_450k
  combined_betas <- getBeta(combined)
  
  out_merged_rds <- file.path(processed_dir, "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds")
  saveRDS(combined, out_merged_rds)
  
  out_merged_csv <- file.path(processed_dir, "Merged_450K_EPIC_BetaValues_with_Condition.csv")
  write.csv(combined_betas, out_merged_csv, quote = FALSE)
  
  cond_vec <- pData(combined)$Condition
  beta_t <- t(combined_betas)
  beta_df <- as.data.frame(beta_t)
  beta_df$Condition <- cond_vec
  
  outFileTransposed <- file.path(processed_dir, "Beta_Transposed_with_Condition.csv")
  write.csv(beta_df, outFileTransposed, row.names = TRUE, quote = FALSE)
  cat("[INFO] Saving =>", outFileTransposed, "\n")
  
} else if (!is.null(gmSet_epic)) {
  cat("[INFO] Only EPIC => saving Beta_Transposed_with_Condition for that alone.\n")
  combined <- gmSet_epic
  combined_betas <- getBeta(combined)
  
  out_merged_rds <- file.path(processed_dir, "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds")
  saveRDS(combined, out_merged_rds)
  
  out_merged_csv <- file.path(processed_dir, "Merged_450K_EPIC_BetaValues_with_Condition.csv")
  write.csv(combined_betas, out_merged_csv, quote = FALSE)
  
  cond_vec <- pData(combined)$Condition
  beta_t <- t(combined_betas)
  beta_df <- as.data.frame(beta_t)
  beta_df$Condition <- cond_vec
  
  outFileTransposed <- file.path(processed_dir, "Beta_Transposed_with_Condition.csv")
  write.csv(beta_df, outFileTransposed, row.names = TRUE, quote = FALSE)
  cat("[INFO] Saving =>", outFileTransposed, "\n")
  
} else {
  cat("[ERROR] No GMsets created => no data.\n")
}

cat("\n=== unify_IDATs_And_Transpose DONE ===\n", as.character(Sys.time()), "\n")

# ======================
# File: src/original/steps/transformer_classifier.py
# ======================

#!/usr/bin/env python3
"""
epigenomic_transformer.py

Piyush Acharya, Derek Jacoby
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import logging
import matplotlib.pyplot as plt
import wandb  # Added wandb import

# Global hyperparameters
# Per paper defaults
CHUNK_SIZE = 320
BATCH_SIZE = 16
PRETRAIN_EPOCHS = 30
FINETUNE_EPOCHS = 50
LEARNING_RATE = 5e-5

MODEL_DIM = 64
FF_DIM    = 256
N_LAYERS  = 6
NUM_HEADS = 4
DROPOUT   = 0.2
MOE_EXPERTS = 4
LORA_R     = 8

# ----------------------------
# Dataset
# ----------------------------
class MethylationChunkedDataset(Dataset):
    """
    Loads a CSV with row=sample, columns=features + Condition.
    Splits each sample's feature vector into chunked tokens of size CHUNK_SIZE.
    """
    def __init__(self, csv_path):
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV not found: {csv_path}")
        df = pd.read_csv(csv_path, index_col=0)
        if "Condition" not in df.columns:
            raise ValueError("No 'Condition' column found.")
        self.classes = sorted(df["Condition"].unique())
        self.class_map = {c: i for i, c in enumerate(self.classes)}
        self.labels = np.array([self.class_map[c] for c in df["Condition"].values], dtype=np.int64)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if not len(numeric_cols):
            raise ValueError("No numeric features found in CSV.")
        self.data = df[numeric_cols].values.astype(np.float32)
        self.num_features = self.data.shape[1]
        self.data_chunks = []
        for row in self.data:
            tokens = []
            for i in range(0, self.num_features, CHUNK_SIZE):
                tokens.append(row[i:i+CHUNK_SIZE])
            self.data_chunks.append(tokens)
    
    def __len__(self):
        return len(self.data_chunks)
    
    def __getitem__(self, idx):
        return self.data_chunks[idx], self.labels[idx]

def methylation_collate_fn(batch):
    sequences, labels = zip(*batch)
    seq_lengths = [len(seq) for seq in sequences]
    max_seq = max(seq_lengths)
    padded = []
    for seq in sequences:
        tokens = []
        for t in seq:
            t = torch.tensor(t, dtype=torch.float32)
            if len(t) < CHUNK_SIZE:
                pad_len = CHUNK_SIZE - len(t)
                t = torch.cat([t, torch.zeros(pad_len, dtype=torch.float32)], dim=0)
            tokens.append(t)
        while len(tokens) < max_seq:
            tokens.append(torch.zeros(CHUNK_SIZE))
        padded.append(torch.stack(tokens))
    X = torch.stack(padded)
    y = torch.tensor(labels, dtype=torch.long)
    return X, y

# ----------------------------
# LoRA Mixin
# ----------------------------
class LoRALinear(nn.Module):
    """
    A linear layer with LoRA adaptation.
    """
    def __init__(self, in_features, out_features, r=LORA_R, alpha=1.0):
        super(LoRALinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.r = r
        self.alpha = alpha
        
        self.weight = nn.Parameter(torch.randn(out_features, in_features)*0.02)
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        self.A = nn.Parameter(torch.zeros(r, in_features))
        self.B = nn.Parameter(torch.zeros(out_features, r))
        
        nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))
        nn.init.kaiming_uniform_(self.A, a=np.sqrt(5))
        nn.init.kaiming_uniform_(self.B, a=np.sqrt(5))

    def forward(self, x):
        base_out = torch.matmul(x, self.weight.T) + self.bias
        lora_out = torch.matmul(x, self.A.T)
        lora_out = torch.matmul(lora_out, self.B.T) * (self.alpha / self.r)
        return base_out + lora_out

# ----------------------------
# ALiBi Multihead Attention with ReZero
# ----------------------------
def alibi_slopes(n_heads):
    slopes = []
    base = 1.0
    for i in range(n_heads):
        slopes.append(base)
        base *= 0.5
    return torch.tensor(slopes)

class ALiBiMultiheadAttention(nn.Module):
    """Multihead attention with ALiBi bias & ReZero scaling."""
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = nn.Dropout(dropout)
        self.head_dim = embed_dim // num_heads
        self.q_proj = LoRALinear(embed_dim, embed_dim)
        self.k_proj = LoRALinear(embed_dim, embed_dim)
        self.v_proj = LoRALinear(embed_dim, embed_dim)
        self.out_proj = LoRALinear(embed_dim, embed_dim)
        self.register_buffer("slopes", alibi_slopes(num_heads), persistent=False)
        self.rezero = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        bsz, seq_len, _ = x.size()
        Q = self.q_proj(x).view(bsz, seq_len, self.num_heads, self.head_dim)
        K = self.k_proj(x).view(bsz, seq_len, self.num_heads, self.head_dim)
        V = self.v_proj(x).view(bsz, seq_len, self.num_heads, self.head_dim)
        Q = Q.permute(0,2,1,3)
        K = K.permute(0,2,1,3)
        V = V.permute(0,2,1,3)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        pos_ids = torch.arange(seq_len, device=x.device).view(1,1,seq_len)
        pos_j = pos_ids - pos_ids.transpose(-1,-2)
        slopes_2d = self.slopes.view(self.num_heads, 1, 1).to(x.device)
        alibi = slopes_2d * pos_j
        alibi = alibi.unsqueeze(0)
        scores = scores + alibi
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = torch.matmul(attn, V)
        out = out.permute(0,2,1,3).contiguous().view(bsz, seq_len, self.embed_dim)
        out = self.out_proj(out)
        return x + self.rezero * out

class ALiBiTransformerBlock(nn.Module):
    """
    A single encoder block with ALiBi attention and ReZero-scaled feedforward.
    """
    def __init__(self, embed_dim=MODEL_DIM, ff_dim=FF_DIM, num_heads=NUM_HEADS, dropout=DROPOUT):
        super().__init__()
        self.mha = ALiBiMultiheadAttention(embed_dim, num_heads, dropout)
        self.ln1 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            LoRALinear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            LoRALinear(ff_dim, embed_dim)
        )
        self.ln2 = nn.LayerNorm(embed_dim)
        self.rezero_ffn = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        attn_out = self.ln1(self.mha(x))
        ff_out = self.ffn(attn_out)
        out = attn_out + self.rezero_ffn * ff_out
        out = self.ln2(out)
        return out

# ----------------------------
# RNN Integration and MoE Blocks
# ----------------------------
class RNNIntegrationBlock(nn.Module):
    """Bidirectional GRU integration block"""
    def __init__(self, model_dim, dropout):
        super().__init__()
        self.gru = nn.GRU(model_dim, model_dim//2, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.proj = nn.Linear(model_dim, model_dim)
    def forward(self, x):
        out, _ = self.gru(x)
        out = self.dropout(out)
        out = self.proj(out)
        return out

class MoEBlock(nn.Module):
    """Mixture-of-Experts gating block"""
    def __init__(self, model_dim, moe_experts=MOE_EXPERTS):
        super().__init__()
        self.gate = nn.Linear(model_dim, moe_experts)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(model_dim, model_dim),
                nn.GELU(),
                nn.Linear(model_dim, model_dim)
            ) for _ in range(moe_experts)
        ])

    def forward(self, x):
        gate_logits = self.gate(x)
        gate_w = torch.softmax(gate_logits, dim=-1)
        expert_outs = [expert(x) for expert in self.experts]
        E = torch.stack(expert_outs, dim=-1)
        gate_w = gate_w.unsqueeze(2)
        out = (E * gate_w).sum(dim=-1)
        return out

# ----------------------------
# Final SOTA Model with Logging for Weight/Bias Convergence
# ----------------------------
class SOTATransformer(nn.Module):
    """
    Full pipeline:
      1) Input projection from CHUNK_SIZE -> MODEL_DIM using LoRA
      2) N_LAYERS of ALiBiTransformerBlock
      3) RNNIntegrationBlock
      4) MoEBlock
      5) Global average pooling
      6) Classifier MLP
    """
    def __init__(self, seq_len, num_classes):
        super().__init__()
        self.seq_len = seq_len
        self.model_dim = MODEL_DIM
        self.input_proj = LoRALinear(CHUNK_SIZE, MODEL_DIM)
        self.blocks = nn.ModuleList([ALiBiTransformerBlock(MODEL_DIM, FF_DIM, NUM_HEADS, DROPOUT) for _ in range(N_LAYERS)])
        self.rnn_block = RNNIntegrationBlock(MODEL_DIM, DROPOUT)
        self.moe_block = MoEBlock(MODEL_DIM, MOE_EXPERTS)
        self.classifier = nn.Sequential(
            nn.LayerNorm(MODEL_DIM),
            nn.Linear(MODEL_DIM, MODEL_DIM//2),
            nn.GELU(),
            nn.Dropout(DROPOUT),
            nn.Linear(MODEL_DIM//2, num_classes)
        )
    
    def forward(self, x):
        # x: [bsz, seq_len, CHUNK_SIZE]
        bsz, seq_len, _ = x.size()
        x = self.input_proj(x)
        for block in self.blocks:
            x = block(x)
        x = self.rnn_block(x)
        x = self.moe_block(x)
        x = x.mean(dim=1)
        out = self.classifier(x)
        return out

# ----------------------------
# Functions for Convergence Logging
# ----------------------------
def log_weight_bias_norms(model):
    norms = {}
    for name, param in model.named_parameters():
        if "weight" in name or "bias" in name:
            norms[name] = param.data.norm().item()
    return norms

def plot_convergence(norms_history, out_path):
    keys = list(norms_history[0].keys())
    epochs = range(1, len(norms_history) + 1)
    plt.figure(figsize=(10, 6))
    for key in keys:
        values = [epoch_dict[key] for epoch_dict in norms_history]
        plt.plot(epochs, values, marker='o', label=key)
    plt.xlabel("Epoch")
    plt.ylabel("Parameter Norm")
    plt.title("Convergence of Transformer Weights and Biases")
    plt.legend(fontsize=8)
    plt.grid(True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"[INFO] Transformer weight/bias convergence graph saved to {out_path}")

# ----------------------------
# Self-Supervised Pretraining with Convergence Logging
# ----------------------------
def self_supervised_pretraining(model, loader, device, epochs=PRETRAIN_EPOCHS, mask_prob=0.15):
    recon_head = LoRALinear(MODEL_DIM, CHUNK_SIZE).to(device)
    optimizer = optim.Adam(list(model.parameters()) + list(recon_head.parameters()), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    model.train()
    pretrain_loss_history = []
    weight_norms_history = []
    out_dir = "/scratch/derekja/EpiMECoV/results"
    for ep in range(1, epochs+1):
        total_loss = 0.0
        for Xb, _ in loader:
            Xb = Xb.to(device)
            bsz, seq_len, _ = Xb.size()
            mask = (torch.rand(bsz, seq_len, device=device) < mask_prob).float().unsqueeze(-1)
            X_masked = Xb * (1 - mask)
            optimizer.zero_grad()
            emb = model.input_proj(X_masked)
            for blk in model.blocks:
                emb = blk(emb)
            emb = model.rnn_block(emb)
            emb = model.moe_block(emb)
            recon = recon_head(emb)
            loss = criterion(recon * mask, Xb * mask)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        pretrain_loss_history.append(avg_loss)
        norms = log_weight_bias_norms(model)
        weight_norms_history.append(norms)
        
        # Log to wandb
        wandb.log({
            "pretrain_loss": avg_loss,
            "pretrain_epoch": ep,
            **{f"pretrain_norm_{k}": v for k, v in norms.items()}
        })
        
        logging.info(f"[Pretrain] Epoch {ep}/{epochs}, Loss = {avg_loss:.4f}")
    
    plt.figure(figsize=(8,6))
    plt.plot(range(1, epochs+1), pretrain_loss_history, marker='o', label="Pretrain Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Self-Supervised Pretraining Loss")
    plt.legend()
    plt.savefig(os.path.join(out_dir, "pretrain_loss_curve.png"), dpi=300)
    plt.close()
    plot_convergence(weight_norms_history, os.path.join(out_dir, "transformer_weights_convergence.png"))
    logging.info("Self-supervised pretraining complete.")

# ----------------------------
# Finetuning with Convergence Logging
# ----------------------------
def finetune_classifier(model, loader_train, loader_val, device, epochs=FINETUNE_EPOCHS):
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    train_loss_history = []
    val_acc_history = []
    out_dir = "/scratch/derekja/EpiMECoV/results"
    weight_norms_history = []
    for ep in range(1, epochs+1):
        total_loss = 0.0
        for Xb, Yb in loader_train:
            Xb, Yb = Xb.to(device), Yb.to(device)
            optimizer.zero_grad()
            out = model(Xb)
            loss = criterion(out, Yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader_train)
        train_loss_history.append(avg_loss)
        
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for Xv, Yv in loader_val:
                Xv, Yv = Xv.to(device), Yv.to(device)
                logits = model(Xv)
                _, preds = torch.max(logits, 1)
                correct += (preds == Yv).sum().item()
                total += Yv.size(0)
        acc = correct / total
        val_acc_history.append(acc)
        model.train()
        norms = log_weight_bias_norms(model)
        weight_norms_history.append(norms)
        
        # Log to wandb
        wandb.log({
            "train_loss": avg_loss,
            "val_accuracy": acc,
            "finetune_epoch": ep,
            **{f"finetune_norm_{k}": v for k, v in norms.items()}
        })
        
        logging.info(f"[Finetune] Epoch {ep}/{epochs} => Train Loss = {avg_loss:.4f}, Val Acc = {acc:.4f}")
        plt.figure(figsize=(8,6))
        plt.plot(range(1, ep+1), train_loss_history, marker='o', label="Train Loss")
        plt.plot(range(1, ep+1), val_acc_history, marker='s', label="Val Accuracy")
        plt.xlabel("Epoch")
        plt.ylabel("Metric")
        plt.title("Finetuning Progress")
        plt.legend()
        plt.savefig(os.path.join(out_dir, "finetune_progress.png"), dpi=300)
        plt.close()
    plot_convergence(weight_norms_history, os.path.join(out_dir, "transformer_weights_convergence_finetune.png"))
    logging.info("Finetuning complete.")

# ----------------------------
# Main Training Pipeline
# ----------------------------
def main():
    logging.basicConfig(filename='results/transformer_classifier.log',level=logging.INFO, format='[%(levelname)s] %(message)s')
    logging.info("entered main")
    # Initialize wandb
    wandb.init(
        project="epimecov-transformer",
        config={
            "chunk_size": CHUNK_SIZE,
            "batch_size": BATCH_SIZE,
            "pretrain_epochs": PRETRAIN_EPOCHS,
            "finetune_epochs": FINETUNE_EPOCHS,
            "learning_rate": LEARNING_RATE,
            "model_dim": MODEL_DIM,
            "ff_dim": FF_DIM,
            "n_layers": N_LAYERS,
            "num_heads": NUM_HEADS,
            "dropout": DROPOUT,
            "moe_experts": MOE_EXPERTS,
            "lora_r": LORA_R,
            "architecture": "SOTA Transformer with ALiBi + LoRA + MoE"
        }
    )
    logging.info("wandb init")
    
    tmpdir = os.environ['SLURM_TMPDIR']
    logging.info(tmpdir)
    csv_path = tmpdir + "/filtered_biomarker_matrix.csv"
    if not os.path.exists(csv_path):
        logging.error(f"No CSV found: {csv_path}")
        sys.exit(1)
    logging.info("csv")
    dataset = MethylationChunkedDataset(csv_path)
    total = len(dataset)
    logging.info(f"total: {total}")
    train_len = int(0.8 * total)
    val_len = total - train_len
    logging.info(f"val_len: {val_len}")
    train_ds, val_ds = random_split(dataset, [train_len, val_len])
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=methylation_collate_fn)
    logging.info("train loader")
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=methylation_collate_fn)
    logging.info("val_loader")

    seq_len_init = max(len(tokens) for tokens in dataset.data_chunks)
    n_classes = len(dataset.classes)
    
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        logging.info("Using MPS device for training.")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        logging.info("Using CUDA device for training.")
    else:
        device = torch.device("cpu")
        logging.info("Using CPU for training.")
    
    logging.info(f"Building SOTA model: seq_len = {seq_len_init}, num_classes = {n_classes}")
    model = SOTATransformer(seq_len_init, n_classes).to(device)
    
    # Log model architecture to wandb
    wandb.watch(model, log="all")
    
    logging.info("==> Starting self-supervised pretraining <==")
    self_supervised_pretraining(model, train_loader, device, epochs=PRETRAIN_EPOCHS, mask_prob=0.15)
    
    logging.info("==> Fine-tuning for classification <==")
    finetune_classifier(model, train_loader, val_loader, device, epochs=FINETUNE_EPOCHS)
    
    out_path = "/scratch/derekja/EpiMECoV/results/transformer_model.pth"
    torch.save(model.state_dict(), out_path)
    logging.info(f"Model saved => {out_path}")
    
    # Close wandb run
    wandb.finish()

if __name__ == "__main__":
    main()

# Added alias for evaluation compatibility.
TransformerClassifier = SOTATransformer


# ======================
# File: src/original/steps/03_verify.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# 03_verify.R
#
# Purpose:
#   Verify the final Beta CSV (row=samples, columns=probes + Condition)
#   matches the merged GenomicMethylSet in dimension, sample order, and Condition.
#
# Usage:
#   Rscript 03_verify.R
###############################################################################

rm(list=ls())
cat("\014")

library(minfi)

epi_root <- "/Volumes/T9/EpiMECoV"
genomic_rds_path <- file.path(epi_root,"processed_data","Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds")
beta_csv_updated <- file.path(epi_root,"processed_data","Beta_Transposed_with_Condition.csv")

cat("=== Checking merged GenomicMethylSet vs. Beta CSV ===\n\n")

if (!file.exists(genomic_rds_path)) {
  stop("[ERROR] RDS file not found => ", genomic_rds_path)
}
if (!file.exists(beta_csv_updated)) {
  stop("[ERROR] CSV file not found => ", beta_csv_updated)
}

mergedSet <- readRDS(genomic_rds_path)
cat("[INFO] mergedSet dimension =>", dim(mergedSet), "\n")
cat("Condition distribution =>\n")
print(table(colData(mergedSet)$Condition))

num_samples <- ncol(mergedSet)
num_probes  <- nrow(mergedSet)

beta_df <- read.csv(beta_csv_updated, row.names=1, check.names=FALSE)
cat("\n[INFO] Beta CSV dimension =>", dim(beta_df), "\n")

if (!("Condition" %in% colnames(beta_df))) {
  stop("[ERROR] No 'Condition' col found in Beta CSV.")
}

# Check #samples
if (nrow(beta_df) != num_samples) {
  warning("Mismatch in sample count: CSV has", nrow(beta_df),
          "where mergedSet has", num_samples)
} else {
  cat("[OK] #samples match.\n")
}
# Check #probes
if ((ncol(beta_df)-1) != num_probes) {
  warning("Mismatch in probe count: CSV has", (ncol(beta_df)-1),
          "where mergedSet has", num_probes)
} else {
  cat("[OK] #probes match (accounting for Condition as last col).\n")
}

# Check Condition alignment
csvCond <- as.character(beta_df$Condition)
setCond <- as.character(colData(mergedSet)$Condition)
if (length(csvCond) == length(setCond) && all(csvCond == setCond)) {
  cat("[OK] Condition vectors match exactly.\n")
} else {
  cat("[WARNING] Condition mismatch or partial mismatch.\n")
  idx <- which(csvCond != setCond)
  if (length(idx)>0) {
    cat("First mismatch at index =>", idx[1], 
        "Beta CSV cond=", csvCond[idx[1]], 
        "mergedSet cond=", setCond[idx[1]], "\n")
  }
}

cat("\n=== 03_verify done. ===\n")

# ======================
# File: src/original/steps/07_ensemble_ml.py
# ======================

#!/usr/bin/env python3
"""
07_ensemble_ml.py

Manual Hard-Vote Ensemble:

Steps:
  1) Loads 'transformed_data.csv' (or whichever CSV from the autoencoder).
  2) Splits into train/test.
  3) Trains XGB, LGBM, CatBoost individually.
  4) Hard-votes to get final predictions.
  5) Prints confusion matrix & classification report.

Optional: Ray Tune for XGB hyperparam search if --tune is used.
"""

import argparse
import os

import numpy as np
import pandas as pd
import ray
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from ray import tune
from ray.air import session
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier


def xgb_tune(config, X_train, y_train, X_val, y_val):
    """Helper for Ray Tune to evaluate XGB hyperparams."""
    model = XGBClassifier(
        n_estimators=int(config["n_estimators"]),
        learning_rate=config["learning_rate"],
        eval_metric="mlogloss",
        use_label_encoder=False,
        random_state=42,
    )
    model.fit(X_train, y_train)
    preds_val = model.predict(X_val)
    acc = accuracy_score(y_val, preds_val)
    session.report({"accuracy": acc})


def tune_xgb(X_train, y_train, X_test, y_test):
    """Runs random search via Ray Tune for XGB, then returns best model."""
    X_sub, X_val, y_sub, y_val = train_test_split(
        X_train, y_train, test_size=0.25, random_state=999, stratify=y_train
    )
    space = {
        "n_estimators": tune.randint(50, 300),
        "learning_rate": tune.loguniform(1e-3, 1e-1),
    }
    tuner = tune.run(
        tune.with_parameters(
            xgb_tune, X_train=X_sub, y_train=y_sub, X_val=X_val, y_val=y_val
        ),
        metric="accuracy",
        mode="max",
        num_samples=8,
        verbose=1,
    )
    best_trial = tuner.get_best_trial("accuracy", mode="max")
    best_cfg = best_trial.config
    print("[HYPERPARAM] Best config:", best_cfg)
    final_xgb = XGBClassifier(
        n_estimators=int(best_cfg["n_estimators"]),
        learning_rate=best_cfg["learning_rate"],
        eval_metric="mlogloss",
        use_label_encoder=False,
        random_state=42,
    )
    final_xgb.fit(X_train, y_train)
    preds = final_xgb.predict(X_test)
    acc_test = accuracy_score(y_test, preds)
    print(f"[HYPERPARAM] Final XGB => test acc: {acc_test:.4f}")
    return final_xgb


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--csv",
        default="/Volumes/T9/EpiMECoV/processed_data/transformed_data.csv",
        help="Input CSV with features + Condition.",
    )
    parser.add_argument("--test_size", type=float, default=0.2)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--tune", action="store_true", help="Tune XGB hyperparams with Ray.")
    args = parser.parse_args()

    if not os.path.exists(args.csv):
        raise FileNotFoundError(f"No CSV => {args.csv}")

    df = pd.read_csv(args.csv)
    if "Condition" not in df.columns:
        raise ValueError("Need Condition col in CSV.")

    # X,y
    feat_cols = [c for c in df.columns if c != "Condition"]
    X = df[feat_cols].values
    conds = df["Condition"].unique()
    cond_map = {c: i for i, c in enumerate(conds)}
    y = np.array([cond_map[v] for v in df["Condition"].values])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_size, random_state=args.seed, stratify=y
    )

    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)

    # XGB
    if args.tune:
        model_xgb = tune_xgb(X_train, y_train, X_test, y_test)
    else:
        model_xgb = XGBClassifier(use_label_encoder=False, eval_metric="mlogloss", random_state=42)
        model_xgb.fit(X_train, y_train)

    # LightGBM
    model_lgb = LGBMClassifier(random_state=42)
    model_lgb.fit(X_train, y_train)

    # CatBoost
    model_cat = CatBoostClassifier(verbose=0, random_state=42)
    model_cat.fit(X_train, y_train)

    preds_xgb = model_xgb.predict(X_test)
    preds_lgb = model_lgb.predict(X_test)
    preds_cat = model_cat.predict(X_test)

    # Hard-vote ensemble
    ensemble_preds = []
    for i in range(len(y_test)):
        votes = [preds_xgb[i], preds_lgb[i], preds_cat[i]]
        voted = np.bincount(votes).argmax()
        ensemble_preds.append(voted)
    ensemble_preds = np.array(ensemble_preds, dtype=int)

    # Evaluate
    cm = confusion_matrix(y_test, ensemble_preds)
    acc = accuracy_score(y_test, ensemble_preds)
    print("\nEnsemble Confusion Matrix:")
    print(cm)
    print("\nClassification Report:")
    inv_map = {v:k for k,v in cond_map.items()}
    label_names = [inv_map[i] for i in sorted(inv_map.keys())]
    print(classification_report(y_test, ensemble_preds, target_names=label_names))
    print("Ensemble Accuracy:", acc)


if __name__ == "__main__":
    main()

# ======================
# File: src/original/steps/08_prepare_final_data.R
# ======================

#!/usr/bin/env Rscript
############################################################
# 08_prepare_final_data.R
#
# This script:
# 1) Loads Beta_Transposed_with_Condition.csv (row=samples, columns=probes + Condition)
# 2) Removes rows with too many NAs or extremely low variance
#    (but uses a less strict filter than before).
# 3) Saves "filtered_biomarker_matrix.csv"
#
# Usage:
#   Rscript 08_prepare_final_data.R
############################################################

library(data.table)
library(dplyr)

cat("=== Step 8) Additional data filtering (less aggressive) ===\n")

epi_root <- "/Volumes/T9/EpiMECoV"
beta_path <- file.path(epi_root, "processed_data", "Beta_Transposed_with_Condition.csv")
if (!file.exists(beta_path)) {
  stop("Beta CSV not found => ", beta_path)
}

# FIX: Use read.csv with row.names=1 so that the sample IDs (rownames) are preserved.
df <- read.csv(beta_path, row.names = 1, check.names = FALSE)
cat("[INFO] dimension =>", dim(df), "\n")

if (!("Condition" %in% colnames(df))) {
  cat("[WARN] 'Condition' column not found. We will proceed but the pipeline may need it.\n")
} else {
  cat("[INFO] 'Condition' column found.\n")
}

cond_vector <- NULL
if ("Condition" %in% colnames(df)) {
  cond_vector <- df$Condition
}

# Remove the Condition column to work on numeric data only.
df_noCond <- df[, !colnames(df) %in% "Condition", drop = FALSE]
cat("[INFO] dimension without Condition =>", dim(df_noCond), "\n")

# Filtering: allow up to 70% missing values per sample (row).
threshold_na <- 0.9
row_na_frac <- apply(df_noCond, 1, function(x) mean(is.na(x)))
keep_na <- which(row_na_frac < threshold_na)
df_filtered <- df_noCond[keep_na, ]

# Filter rows (samples) with extremely low variance.
threshold_var <- 1e-9
rowvars <- apply(df_filtered, 1, var, na.rm = TRUE)
keep_var <- which(rowvars >= threshold_var)
df_filtered <- df_filtered[keep_var, ]

cat("[INFO] After filtering =>", dim(df_filtered), "\n")

# Re-append the Condition vector.
if (!is.null(cond_vector)) {
  final_indices <- keep_na[keep_var]
  df_filtered$Condition <- cond_vector[final_indices]
}

out_path <- file.path(epi_root, "processed_data", "filtered_biomarker_matrix.csv")
# Write with row names so that later scripts can recover the sample IDs.
write.csv(df_filtered, out_path, row.names = TRUE, quote = FALSE)
cat("[SAVED] =>", out_path, "\n")
cat("=== Done preparing final data with a less strict filter. ===\n")

# ======================
# File: src/original/steps/04_qa.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# 04_qa_FAST.R (a faster drop-in replacement for 04_qa.R)
#
# PURPOSE:
#   Minimal QA: verifies that the merged GenomicMethylSet loads correctly,
#   and then does a quick read of Beta_Transposed_with_Condition.csv to
#   confirm dimension and distribution of Condition. 
#
#   This version uses data.table::fread to handle extremely wide CSVs quickly
#   (the user had 3 x ~415K columns, which can be painfully slow with base R).
#
# USAGE:
#   Rscript 04_qa_FAST.R
#
#   (Or rename it back to 04_qa.R if you prefer the same file name.)
###############################################################################

# Clear workspace and console
rm(list=ls())
cat("\014")

cat("=== Minimal QA Only Script Start ===\n", as.character(Sys.time()), "\n\n")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (1) Load needed packages
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if (!requireNamespace("data.table", quietly = TRUE)) {
  install.packages("data.table", repos="https://cloud.r-project.org/")
}
if (!requireNamespace("minfi", quietly = TRUE)) {
  if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager", repos="https://cloud.r-project.org/")
  }
  BiocManager::install("minfi")
}
library(data.table)
library(minfi)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (2) Define your paths
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
epi_root <- "/Volumes/T9/EpiMECoV"
processed_dir <- file.path(epi_root, "processed_data")

gmset_path <- file.path(processed_dir, "Merged_450K_EPIC_GenomicMethylSet_with_Condition.rds")
beta_csv   <- file.path(processed_dir, "Beta_Transposed_with_Condition.csv")

cat("[STEP] Loading RDS =>", gmset_path, "\n")
if (!file.exists(gmset_path)) {
  stop("Could not find GenomicMethylSet RDS => ", gmset_path)
}

gmset <- readRDS(gmset_path)
cat("[INFO] Dimension =>", nrow(gmset), "x", ncol(gmset), "\n")
cat("[INFO] Condition distribution =>\n\n")
print(table(colData(gmset)$Condition))
cat("\n")

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# (3) Read the huge Beta CSV using data.table::fread (multi-threaded)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cat("[STEP] Loading Beta CSV =>", beta_csv, "\n\n")
if (!file.exists(beta_csv)) {
  stop("Beta CSV not found => ", beta_csv)
}

# Adjust nThread= to match your Mac’s core count (e.g., 8, 10, 16).
# data.table automatically uses a default that is often good enough, 
# but you can override it explicitly:
nCores <- 16  # or something that matches your Apple M4 Max
cat("[INFO] Using data.table::fread with nThread =", nCores, "\n")

# Because your CSV is 3 rows × ~415,000 columns, the main challenge 
# is the extremely wide dimension. data.table's fread is highly optimized 
# for this scenario.
DT <- fread(
  input       = beta_csv,
  sep         = ",",
  header      = TRUE,
  nThread     = nCores,
  verbose     = FALSE,
  showProgress= TRUE
)

cat("[INFO] Dimension =>", nrow(DT), "x", ncol(DT), "\n\n")

# If 'Condition' is your last column, let’s confirm distribution:
if (!("Condition" %in% names(DT))) {
  cat("[WARNING] No 'Condition' col found in CSV.\n")
} else {
  cond_vec <- DT[["Condition"]]
  cat("[INFO] Condition distribution =>\n")
  print(table(cond_vec))
  cat("\n")
}

cat("=== Minimal QA is complete. ===\n\n")

# ======================
# File: src/original/steps/10_baseline_classification.py
# ======================

#!/usr/bin/env python3
"""
10_baseline_classification.py

Purpose:
  - Loads a CSV with row=sample, columns=selected features, last col=Condition.
  - Removes classes with fewer than 2 samples.
  - Splits data (train/test) and runs a quick RandomForest.
  - Prints confusion matrix & classification report.
"""

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split

def main():
    csv_path = "./processed_data/filtered_biomarker_matrix.csv"
    if not os.path.exists(csv_path):
        print(f"[ERROR] No CSV at {csv_path}. Did you run '09_feature_selection.R'?")
        return

    df = pd.read_csv(csv_path, index_col=0)
    if "Condition" not in df.columns:
        print("[ERROR] 'Condition' is missing in the final CSV.")
        return

    # Remove duplicates in columns or rows if any
    df = df.loc[:, ~df.columns.duplicated()].copy()
    df = df[~df.index.duplicated()].copy()

    # X,y
    X = df.drop("Condition", axis=1).values
    y_str = df["Condition"].values.astype(str)

    # Remove classes with <2 samples
    class_counts = pd.Series(y_str).value_counts()
    valid_classes = class_counts[class_counts >= 2].index
    mask_valid = pd.Series(y_str).isin(valid_classes)
    X = X[mask_valid]
    y_str = y_str[mask_valid]

    if len(np.unique(y_str)) < 2:
        print("[ERROR] After removing rare classes, not enough classes to classify.")
        return

    classes = sorted(np.unique(y_str))
    c2i = {c: i for i, c in enumerate(classes)}
    y = np.array([c2i[val] for val in y_str], dtype=int)

    n_samples = len(X)
    n_classes = len(classes)
    test_size = 0.3

    # Ensure we have enough test samples for all classes:
    # => test_size * n_samples >= n_classes
    if int(round(test_size * n_samples)) < n_classes:
        # pick the smallest fraction that yields at least n_classes in test
        recommended = float(n_classes + 1) / n_samples
        # clamp recommended to 0.5 max, so we keep some train
        recommended = min(recommended, 0.5)
        print(f"[INFO] Adjusting test_size from {test_size} to {recommended:.2f} to accommodate {n_classes} classes.")
        test_size = recommended

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )

    clf = RandomForestClassifier(n_estimators=200, random_state=42)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:\n", cm)
    print("\nClassification Report:\n",
          classification_report(y_test, y_pred, target_names=classes, digits=4))

    if len(classes) == 2:
        y_proba = clf.predict_proba(X_test)[:, 1]
        auc_val = roc_auc_score(y_test, y_proba)
        print("[Binary] ROC AUC =>", auc_val)

    print("\nBaseline classification done.")

if __name__ == "__main__":
    main()

# ======================
# File: src/original/steps/09_debug_feature_selection.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# 09_fix_step_10.R
#
# Optional debugging script to show overlaps between DMP probe IDs and
# your Beta matrix. Not used in the normal pipeline, but helpful if 
# you get "no overlap" or "Condition mismatch" errors.
###############################################################################

library(data.table)
library(dplyr)

cat("=== 09_fix_step_10 debugging script ===\n")

epi_root <- "/Volumes/T9/EpiMECoV"

# 1) load filtered beta (row=probes, columns=samples, possibly with a 'CpG' col)
beta_file <- file.path(epi_root, "processed_data", "filtered_biomarker_matrix.csv")
beta_data <- fread(beta_file)

if ("V1" %in% colnames(beta_data)) {
  setnames(beta_data, old="V1", new="CpG", skip_absent=TRUE)
}
cat("Beta data dimension =>", dim(beta_data), "\n")
cat("Beta data first row =>\n")
print(head(beta_data,1))

# 2) load DMP
results_dir <- file.path(epi_root,"results")
dmp_me_ctrl_file <- file.path(results_dir,"DMP_ME_vs_Control.csv")
dmp_lc_ctrl_file <- file.path(results_dir,"DMP_LC_vs_Control.csv")
dmp_me_lc_file   <- file.path(results_dir,"DMP_ME_vs_LC.csv")

dmp_me_ctrl <- fread(dmp_me_ctrl_file)
dmp_lc_ctrl <- fread(dmp_lc_ctrl_file)
dmp_me_lc   <- fread(dmp_me_lc_file)

for (dmp_df in list(dmp_me_ctrl, dmp_lc_ctrl, dmp_me_lc)) {
  if ("V1" %in% colnames(dmp_df)) {
    setnames(dmp_df, old="V1", new="CpG", skip_absent=TRUE)
  }
}

# top 2k
topN <- 2000
top_me_ctrl <- head(dmp_me_ctrl[order(dmp_me_ctrl$P.Value)], topN)
top_lc_ctrl <- head(dmp_lc_ctrl[order(dmp_lc_ctrl$P.Value)], topN)
top_me_lc   <- head(dmp_me_lc[order(dmp_me_lc$P.Value)],   topN)

all_cpgs <- unique(c(top_me_ctrl$CpG, top_lc_ctrl$CpG, top_me_lc$CpG))
cat("Unique CpGs from top hits =>", length(all_cpgs), "\n")

if (!"CpG" %in% colnames(beta_data)) {
  cat("[ERROR] Beta data lacks 'CpG' col => can't do overlap.\n")
} else {
  overlap <- intersect(all_cpgs, beta_data$CpG)
  cat("Overlap =>", length(overlap), "CpGs in Beta matrix.\n")
  cat("Example of overlap:\n")
  print(head(overlap, 10))
}

cat("=== Debug script done. ===\n")

# ======================
# File: src/original/steps/09_feature_selection.R
# ======================

#!/usr/bin/env Rscript
###############################################################################
# 09_feature_selection.R
#
# Step 9) Feature Selection + Condition Restoration
#
# This script:
#   (1) Reads the "filtered_biomarker_matrix.csv" (from step 8) – a large matrix
#       with row=probes, columns=samples, plus a "Condition" column repeated in
#       each row.
#   (2) Reads the top DMPs from the three contrasts (DMP_ME_vs_Control.csv, etc.)
#   (3) Combines (union) the top CpG IDs across these DMP results
#   (4) Subsets the big matrix to those CpGs only
#   (5) Re-appends the Condition vector as the final column
#   (6) Writes out "feature_selected_matrix.csv"
#
#   NEW: Also writes out a separate "all_cpg_matrix.csv" that keeps ALL CpGs
#
# USAGE:
#   Rscript 09_feature_selection.R
###############################################################################

suppressPackageStartupMessages({
  library(dplyr)
  # Note: We now use base R’s read.csv() instead of data.table::fread()
})

cat("=== Step 9) Feature Selection + Condition Restoration ===\n\n")

# 1) Define paths
epi_root <- "/Volumes/T9/EpiMECoV"
processed_dir <- file.path(epi_root, "processed_data")
results_dir   <- file.path(epi_root, "results")

filtered_csv <- file.path(processed_dir, "filtered_biomarker_matrix.csv")
if (!file.exists(filtered_csv)) {
  stop("[ERROR] Could not find filtered_biomarker_matrix.csv =>", filtered_csv)
}

# 2) Read the big Beta matrix using read.csv (instead of fread)
cat("[INFO] Reading Beta data using read.csv ...\n")
beta_big <- read.csv(filtered_csv, row.names = 1, stringsAsFactors = FALSE)
cat("[INFO] Beta data dimension:", dim(beta_big), "\n")

# === NEW PART: also save a copy with ALL CpGs (no subsetting) ===
all_cpg_out <- file.path(processed_dir, "all_cpg_matrix.csv")
write.csv(beta_big, all_cpg_out, quote = FALSE)
cat("[INFO] Also wrote out full CpG data to =>", all_cpg_out, "\n")

# 3) Read the top DMP CpG IDs from the three contrast files and take their union,
#    then select exactly the top 1,280 CpGs by adjusted P-value as per paper.
dmp_me_ctrl_file <- file.path(results_dir, "DMP_ME_vs_Control.csv")
dmp_lc_ctrl_file <- file.path(results_dir, "DMP_LC_vs_Control.csv")
dmp_me_lc_file   <- file.path(results_dir, "DMP_ME_vs_LC.csv")

cpg_union <- c()
collect_dmp <- function(path) {
  if (!file.exists(path)) return(NULL)
  df <- read.csv(path, row.names = 1, stringsAsFactors = FALSE)
  # Expect BH-adjusted P-value column names commonly used by limma
  pcols <- intersect(colnames(df), c("adj.P.Val", "adj.P.Val.", "adjP", "adj_p"))
  pcol <- if (length(pcols) > 0) pcols[1] else NA
  if (is.na(pcol)) {
    # fallback: try P.Value if adj not present
    pcol <- if ("P.Value" %in% colnames(df)) "P.Value" else NULL
  }
  if (is.null(pcol)) return(NULL)
  data.frame(CpG = rownames(df), adjP = df[[pcol]], stringsAsFactors = FALSE)
}

d1 <- collect_dmp(dmp_me_ctrl_file)
d2 <- collect_dmp(dmp_lc_ctrl_file)
d3 <- collect_dmp(dmp_me_lc_file)
d_all <- do.call(rbind, Filter(Negate(is.null), list(d1, d2, d3)))

if (is.null(d_all) || nrow(d_all) == 0) {
  cat("[WARN] No DMPs found => skipping feature selection.\n")
  out_path <- file.path(processed_dir, "feature_selected_matrix.csv")
  write.csv(beta_big, out_path, quote = FALSE)
  cat("[SAVED] =>", out_path, "\n")
  quit(status = 0)
}

# Aggregate best adjP per CpG across contrasts, then pick top 1280 by adjP
agg <- aggregate(adjP ~ CpG, data = d_all, FUN = function(x) min(as.numeric(x), na.rm = TRUE))
agg <- agg[order(agg$adjP, decreasing = FALSE), ]
top_n <- min(1280, nrow(agg))
selected_cpgs <- agg$CpG[seq_len(top_n)]
cat("[INFO] Selected top", top_n, "CpGs (target=1280) by adjusted P-value.\n")

if (length(cpg_union) < 1) {
  cat("[WARN] No DMPs found => skipping feature selection.\n")
  out_path <- file.path(processed_dir, "feature_selected_matrix.csv")
  write.csv(beta_big, out_path, quote = FALSE)
  cat("[SAVED] =>", out_path, "\n")
  quit(status = 0)
}

# 4) Subset and order by genomic coordinates to create contiguous blocks per token
probe_ids <- rownames(beta_big)
if (length(probe_ids) == 0) {
  stop("[ERROR] Could not identify the probe IDs in the row names.")
}

keep_idx <- which(probe_ids %in% selected_cpgs)
subset_mat <- beta_big[keep_idx, , drop = FALSE]
cat("[INFO] Post-subset dimension:", dim(subset_mat), "\n")

# Try to order CpGs by genomic coordinate if annotation available via minfi
suppressWarnings(suppressMessages({
  try({
    library(minfi)
    anno <- getAnnotation(IlluminaHumanMethylation450kanno.ilmn12.hg19)
    # match and order
    common <- intersect(rownames(anno), rownames(subset_mat))
    anno_sub <- anno[common, c("chr", "pos")]
    ord <- order(anno_sub$chr, anno_sub$pos, na.last = TRUE)
    ordered_ids <- rownames(anno_sub)[ord]
    subset_mat <- subset_mat[ordered_ids, , drop = FALSE]
    cat("[INFO] Ordered selected CpGs by genomic position.", "\n")
  }, silent = TRUE)
}))

# 5) Re-append the Condition vector.
# (Assuming the original beta_big has a "Condition" column)
cond_vec <- beta_big$Condition
final_df <- subset_mat
final_df$Condition <- cond_vec[keep_idx]

# 6) Write out the final feature-selected matrix.
out_file <- file.path(processed_dir, "feature_selected_matrix.csv")
write.csv(final_df, out_file, quote = FALSE)
cat("[SAVED] =>", out_file, "\n")

cat("\n=== Feature Selection step complete. ===\n")


# ======================
# File: src/original/steps/13_summarize_findings.R
# ======================

#!/usr/bin/env Rscript
############################################################
# 13_summarize_findings.R
#
# Summarize final pipeline results:
#  - If row=sample and there's a Condition col, do a quick PCA
#    on the numeric columns (excluding Condition).
#  - Save "pca_final.png" in the results folder if feasible.
#
# Usage:
#   Rscript 13_summarize_findings.R
############################################################

suppressMessages({
  library(data.table)
  library(ggplot2)
  library(dplyr)
})

cat("=== Summarize Findings Script ===\n")

epi_root <- "/Volumes/T9/EpiMECoV"
results_dir <- file.path(epi_root, "results")

final_csv_1 <- file.path(epi_root, "processed_data", "transformed_data.csv")
final_csv_2 <- file.path(epi_root, "processed_data", "filtered_biomarker_matrix.csv")

use_file <- NA
if (file.exists(final_csv_1)) {
  use_file <- final_csv_1
} else if (file.exists(final_csv_2)) {
  use_file <- final_csv_2
} else {
  stop("[ERROR] No final CSV found (neither transformed_data.csv nor filtered_biomarker_matrix.csv).")
}

cat("[INFO] Using =>", use_file, "\n")

df <- fread(use_file)
cat("[INFO] dimension =>", dim(df), "\n")

if (!("Condition" %in% colnames(df))) {
  cat("[ERROR] no Condition col => cannot do PCA by group.\n")
  quit(status=1)
}

# The typical pipeline has row=sample, so #rows = ~678, #cols = 64 (or 4470).
if (nrow(df) < ncol(df)) {
  cat("[INFO] row=sample => attempting PCA.\n")
  
  # Exclude Condition + any non-numeric columns:
  numeric_cols <- df %>%
    select(where(is.numeric)) %>%
    colnames()

  # if Condition was numeric for some reason, remove:
  if ("Condition" %in% numeric_cols) {
    numeric_cols <- numeric_cols[numeric_cols != "Condition"]
  }
  
  if (length(numeric_cols) < 2) {
    cat("[WARN] Not enough numeric columns for PCA.\n")
    quit(status=0)
  }
  
  mat <- as.matrix(df[, ..numeric_cols])

  # Must ensure the row count > col count for typical prcomp usage:
  # If you have more columns than rows, you can still do prcomp, but let's proceed:
  pca_res <- prcomp(mat, scale. = TRUE)
  pc_df <- as.data.frame(pca_res$x[,1:2])
  colnames(pc_df) <- c("PC1","PC2")

  pc_df$Condition <- df$Condition

  p <- ggplot(pc_df, aes(x=PC1, y=PC2, color=Condition)) +
    geom_point(alpha=0.7) +
    theme_minimal() +
    labs(title="PCA of Final Data", x="PC1", y="PC2")

  out_png <- file.path(results_dir, "pca_final.png")
  ggsave(out_png, p, width=6, height=5)
  cat("[SAVED PCA plot]", out_png, "\n")
  
} else {
  cat("[INFO] Data suggests row=probes => skipping PCA.\n")
}

cat("=== Summarize Findings Done. ===\n")

# ======================
# File: src/original/steps/450k/450k_transformer.py
# ======================

#!/usr/bin/env python3
"""
Epigenomic Transformer Pipeline with MoE + ACT + Masked Pretraining
====================================================================

Usage (quick local test):
  python epigenomic_transformer.py --csv /Users/username/Downloads/output.txt \
                                   --quick_test 1

Usage (full HPC run):
  python epigenomic_transformer.py --csv /path/to/filtered_biomarker_matrix.csv \
                                   --quick_test 0

Key Steps:
 - Loads CSV with shape [Samples x Features], last column = 'Condition'
 - (Optional) transpose if your data is [Features x Samples], see the commented line
 - Masked Pretraining (randomly mask ~15% of CpG values)
 - Fine-tune on classification with mixture-of-experts feed-forward + ACT gating
 - Logs more often so you can see partial progress
 - If 'quick_test=1', it uses fewer epochs, fewer hidden dims, smaller chunk, etc.
 - Else, it uses recommended settings for large-scale HPC training.

"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score
import random

###############################################################################
# DataSet + Collate
###############################################################################
class MethylationCSVDataset(Dataset):
    """
    A dataset that loads the entire CSV into memory. Expects:
        - 'Condition' as the last column
        - The rest columns as numeric features
    If the user sets quick_test=1, we further subsample rows & features to run quickly.
    """
    def __init__(self, csv_path, quick_test=False, logger=print):
        self.logger = logger
        self.logger(f"[INFO] Loading CSV => {csv_path}")
        df = pd.read_csv(csv_path)
        self.logger(f"[INFO] Raw shape from CSV: {df.shape}")

        if "Condition" not in df.columns:
            raise ValueError("No 'Condition' column found in the CSV. Please ensure last col is Condition.")

        # If your CSV has shape [Features x Samples], you may need to transpose.
        # We avoid automatic transpose here to prevent accidental mishandling.

        # Condition as the last column
        cond_values = df["Condition"].values.astype(str)
        self.classes = sorted(np.unique(cond_values))
        self.class2idx = {c: i for i, c in enumerate(self.classes)}
        self.y = np.array([self.class2idx[c] for c in cond_values], dtype=np.int64)

        df_feat = df.drop(columns=["Condition"])
        # Convert to float
        Xmat = df_feat.values.astype(np.float32)
        # Per paper: z-score normalize each sample's beta-values
        eps = 1e-8
        row_mean = Xmat.mean(axis=1, keepdims=True)
        row_std = Xmat.std(axis=1, keepdims=True) + eps
        Xmat = (Xmat - row_mean) / row_std
        self.logger(f"[INFO] Feature matrix shape before quick_test: {Xmat.shape}")

        if quick_test:
            # Subsample rows & cols drastically for a quick run
            # e.g. keep first 200 samples, first 500 features
            n_rows = min(200, Xmat.shape[0])
            n_cols = min(500, Xmat.shape[1])
            Xmat = Xmat[:n_rows, :n_cols]
            self.y = self.y[:n_rows]
            self.logger("[INFO] Quick test => subsample to shape: "
                        f"{n_rows} x {n_cols}")

        # Convert to torch float
        self.X = torch.from_numpy(Xmat).float()
        self.logger(f"[INFO] Final dataset shape => {self.X.shape}, Y => {self.y.shape}")

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

###############################################################################
# Masked Pretraining Collate
###############################################################################
def masked_collate_fn(batch, mask_prob=0.15, logger=print):
    """
    For self-supervised pretraining:
     - We randomly mask a portion of the features for each sample
     - Return (masked_X, original_X) so we can compute MSE
     - Y is not used for pretraining, but let's just store zeros or ignore it
    """
    Xs, Ys = zip(*batch)
    Xs = torch.stack(Xs, dim=0)  # [B, F]
    # Mask
    mask = (torch.rand_like(Xs) < mask_prob)  # boolean of same shape
    # We'll set masked positions to 0, you might choose some sentinel
    masked_X = Xs.clone()
    masked_X[mask] = 0.0
    return masked_X, Xs, mask  # we'll compute MSE on masked positions

###############################################################################
# Fine-Tuning Collate
###############################################################################
def classification_collate_fn(batch):
    Xs, Ys = zip(*batch)
    return torch.stack(Xs, dim=0), torch.tensor(Ys, dtype=torch.long)


###############################################################################
# The Transformer with:
#  - chunking
#  - mixture-of-experts (MoE)
#  - adaptive computation time (ACT)
#  - masked pretraining
###############################################################################

class DynamicLinear(nn.Module):
    """Linear layer wrapper kept for compatibility; not a placeholder."""
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
    def forward(self, x):
        return self.linear(x)

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=8, alpha=1.0):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)
        self.A = nn.Parameter(torch.zeros(r, in_features))
        self.B = nn.Parameter(torch.zeros(out_features, r))
        nn.init.kaiming_uniform_(self.A, a=5 ** 0.5)
        nn.init.kaiming_uniform_(self.B, a=5 ** 0.5)
        self.scaling = alpha / max(1, r)

    def forward(self, x):
        base = x @ self.weight.t() + self.bias
        lora = (x @ self.A.t()) @ self.B.t() * self.scaling
        return base + lora

def alibi_slopes(n_heads):
    slopes = []
    base = 1.0
    for _ in range(n_heads):
        slopes.append(base)
        base *= 0.5
    return torch.tensor(slopes)

class ALiBiMultiheadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.q_proj = LoRALinear(embed_dim, embed_dim)
        self.k_proj = LoRALinear(embed_dim, embed_dim)
        self.v_proj = LoRALinear(embed_dim, embed_dim)
        self.o_proj = LoRALinear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer("slopes", alibi_slopes(num_heads), persistent=False)
        self.rezero = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        B, L, _ = x.shape
        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim)
        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim)
        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim)
        Q = Q.permute(0, 2, 1, 3)
        K = K.permute(0, 2, 1, 3)
        V = V.permute(0, 2, 1, 3)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        pos_ids = torch.arange(L, device=x.device).view(1, 1, L)
        rel = pos_ids - pos_ids.transpose(-1, -2)
        slopes_2d = self.slopes.view(self.num_heads, 1, 1).to(x.device)
        alibi = slopes_2d * rel
        alibi = alibi.unsqueeze(0)
        scores = scores + alibi
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = torch.matmul(attn, V)
        out = out.permute(0, 2, 1, 3).contiguous().view(B, L, self.embed_dim)
        out = self.o_proj(out)
        return x + self.rezero * out

class MoEFeedForward(nn.Module):
    """
    Mixture-of-Experts feed-forward network with E experts. Gate is a small linear -> softmax.
    Each expert is a 2-layer MLP (GELU).
    """
    def __init__(self, d_model, d_ff, E=4, dropout=0.1, top_k=2):
        super().__init__()
        self.E = E
        self.top_k = min(max(1, top_k), E)
        self.gate = nn.Linear(d_model, E)
        self.experts = nn.ModuleList()
        self.dropout = nn.Dropout(dropout)
        hidden_dim = d_ff
        for _ in range(E):
            # each expert is linear -> GELU -> dropout -> linear
            expert = nn.Sequential(
                nn.Linear(d_model, hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, d_model),
                nn.Dropout(dropout)
            )
            self.experts.append(expert)

    def forward(self, x):
        # x: [B, L, d_model]
        B, L, D = x.shape
        # gating
        gate_logits = self.gate(x)  # [B, L, E]
        gate_scores = torch.softmax(gate_logits, dim=-1)  # [B, L, E]
        # sparsify: keep top_k experts per token
        if self.top_k < self.E:
            topk_vals, topk_idx = torch.topk(gate_scores, k=self.top_k, dim=-1)
            mask = torch.zeros_like(gate_scores)
            mask.scatter_(-1, topk_idx, 1.0)
            gate_scores = gate_scores * mask
            # renormalize
            denom = gate_scores.sum(dim=-1, keepdim=True) + 1e-9
            gate_scores = gate_scores / denom

        # for each expert, we compute that expert's output
        # then sum up with the gating as weights
        # shape: each expert output => [B, L, d_model]
        out_stack = []
        for e_idx in range(self.E):
            e_out = self.experts[e_idx](x)  # [B, L, d_model]
            # weighting
            w = gate_scores[:,:,e_idx].unsqueeze(-1)  # [B, L, 1]
            e_weighted = e_out * w
            out_stack.append(e_weighted)

        out = sum(out_stack)  # [B, L, d_model]
        return out

class TransformerBlock(nn.Module):
    """
    One encoder block with:
      - multihead attn
      - residual+norm
      - MoE feedforward
      - residual+norm
    """
    def __init__(self, d_model, num_heads, d_ff, E=4, dropout=0.1):
        super().__init__()
        self.mha = ALiBiMultiheadAttention(d_model, num_heads, dropout)
        self.ln1 = nn.LayerNorm(d_model)
        self.moe_ff = MoEFeedForward(d_model, d_ff, E=E, dropout=dropout)
        self.ln2 = nn.LayerNorm(d_model)
        self.rezero_ffn = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        # x: [B, L, d_model]
        attn_out = self.ln1(self.mha(x))
        ff_out = self.moe_ff(attn_out)
        x = attn_out + self.rezero_ffn * ff_out
        x = self.ln2(x)
        return x

class AdaptiveComputationTime(nn.Module):
    """
    Wrap the entire stack of blocks. We'll do multiple passes if needed.
    p = sigma(w^T * x_mean)
    If p < threshold, do next pass, up to max_passes.
    We'll keep it simpler: we compute a single pass, then we do p if we do a second pass, etc.
    """
    def __init__(self, d_model, max_passes=3, act_threshold=0.99):
        super().__init__()
        self.halt_linear = nn.Linear(d_model, 1)
        self.max_passes = max_passes
        self.act_threshold = act_threshold

    def forward(self, x, blocks):
        """
        Per-sample ACT with simple halting:
        - After each full pass, compute p_i for each sample
        - Samples with p_i >= threshold are considered halted and keep their last state
        - Continue up to max_passes for remaining samples
        Also computes a differentiable expected pass count for regularization.
        """
        B = x.size(0)
        active = torch.ones(B, 1, device=x.device)
        remainders = torch.ones(B, 1, device=x.device)  # product of (1-p)
        expected_passes = 0.0
        x_current = x
        for t in range(self.max_passes):
            x_next = x_current
            for block in blocks:
                x_next = block(x_next)
            x_mean = x_next.mean(dim=1)
            p = torch.sigmoid(self.halt_linear(x_mean)).unsqueeze(-1)  # [B,1,1] after unsqueeze
            p = p.squeeze(-1)  # [B,1]
            # Expected pass contribution: (t+1) * p * remainders
            expected_passes = expected_passes + (t + 1) * (p * remainders)
            # Determine which samples halt this step
            halt_mask = (p >= self.act_threshold).float()  # [B,1]
            # Update x only for still-active samples
            active_mask = (active > 0.5).float()
            update_mask = (active_mask * (1.0 - halt_mask)).view(B, 1, 1)
            keep_mask = (1.0 - update_mask)
            x_current = x_next * update_mask + x_current * keep_mask
            # Update active and remainders
            active = active * (1.0 - halt_mask)
            remainders = remainders * (1.0 - p)
            # If all halted, stop early
            if active.sum().item() == 0:
                break
        # Save differentiable scalar regularizer
        self.last_expected_passes_tensor = expected_passes.mean()
        return x_current

class AdvancedTransformerClassifier(nn.Module):
    """
    Full pipeline:
     - chunking (embedding)
     - N= e.g. 4 or 6 blocks with multihead attn + MoE
     - ACT gating
     - classification head
    plus a separate 'mask_recon_head' for pretraining
    """
    def __init__(self, input_dim=1280,
                 chunk_len=320,
                 d_model=64,
                 num_heads=4,
                 d_ff=256,
                 E=4,
                 dropout=0.2,
                 n_layers=6,
                 n_classes=3,
                 max_passes=3,
                 act_threshold=0.99,
                 quick_test=False,
                 logger=print):
        super().__init__()
        self.logger = logger
        self.input_dim = input_dim
        self.chunk_len = chunk_len
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.n_layers = n_layers
        self.n_classes = n_classes
        self.quick_test = quick_test

        # possibly scale down if quick_test
        if self.quick_test:
            # keep the same dims per paper, but reduce layers for speed in quick mode
            self.n_layers = 2
            self.logger("[INFO] quick_test => n_layers=2 (dims fixed to match paper)")

        # compute how many chunks
        self.num_chunks = (self.input_dim + self.chunk_len - 1)// self.chunk_len

        # input proj
        # We'll do a simple linear that goes from chunk_len -> d_model
        # We'll store it as self.embedding
        self.embedding = LoRALinear(self.chunk_len, self.d_model)

        # Build blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(self.d_model, self.num_heads,
                             self.d_ff, E=E, dropout=dropout)
            for _ in range(self.n_layers)
        ])

        # ACT
        self.act = AdaptiveComputationTime(self.d_model, max_passes, act_threshold)

        # RNN integration block per paper table
        self.rnn = nn.GRU(self.d_model, self.d_model // 2, batch_first=True, bidirectional=True)
        self.rnn_proj = nn.Linear(self.d_model, self.d_model)

        # classifier
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, self.d_model//2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(self.d_model//2, n_classes)
        )

        # for masked pretraining, a recon head
        # we want to reconstruct chunk_len dims per chunk
        self.mask_recon_head = nn.Linear(self.d_model, self.chunk_len)

    def forward(self, x):
        """
        x shape: [B, F] (F = input_dim). We'll chunk it => [B, num_chunks, chunk_len].
        Then embed => [B, num_chunks, d_model].
        Then pass through ACT + blocks => final => classifier
        We'll do training mode or something. We'll see how to handle. Typically we won't do partial for pretraining. We'll do a separate function for that.
        """
        B, F = x.shape
        # chunk
        # pad if needed
        if F < self.num_chunks*self.chunk_len:
            pad_len = self.num_chunks*self.chunk_len - F
            x = torch.cat([x, torch.zeros(B, pad_len, device=x.device)], dim=1)
        # reshape
        x = x.view(B, self.num_chunks, self.chunk_len)
        # embed each chunk => [B, num_chunks, d_model]
        x = self.embedding(x)
        x = self.act(x, self.blocks)
        # RNN integration
        rnn_out, _ = self.rnn(x)
        x = self.rnn_proj(rnn_out)
        # mean pool across chunks
        x_mean = x.mean(dim=1) # [B, d_model]
        logits = self.classifier(x_mean)
        return logits

    def reconstruct_masked(self, x, mask):
        """
        For pretraining: x shape: [B, F], mask shape: [B, F]
         - we chunk x, embed => pass through blocks => no classification, but we apply recon head chunk by chunk
         - We'll compute MSE on the masked positions
        """
        B, F = x.shape
        # pad if needed
        if F < self.num_chunks*self.chunk_len:
            pad_len = self.num_chunks*self.chunk_len - F
            x = torch.cat([x, torch.zeros(B, pad_len, device=x.device)], dim=1)
            mask = torch.cat([mask, torch.zeros(B, pad_len, device=mask.device, dtype=mask.dtype)], dim=1)

        x_c = x.view(B, self.num_chunks, self.chunk_len)
        emb = self.embedding(x_c)
        # pass through blocks, but let's skip the ACT to keep it simpler in pretraining
        for block in self.blocks:
            emb = block(emb)
        # now apply recon head chunk by chunk
        recon_c = self.mask_recon_head(emb) # [B, num_chunks, chunk_len]
        recon = recon_c.view(B, self.num_chunks*self.chunk_len)
        # if we padded, slice back
        recon = recon[:, :F]
        mask = mask[:, :F]
        return recon, mask


###############################################################################
# Full Train Script
###############################################################################
def train_transformer(args):
    logger = print
    logger("[INFO] Starting train_transformer with args:")
    logger(args)

    # 1) Load dataset
    dataset = MethylationCSVDataset(args.csv, quick_test=args.quick_test, logger=logger)
    n_samples, n_features = dataset.X.shape
    classes = dataset.classes
    logger(f"[DATA] #samples={n_samples}, #features={n_features}, #classes={len(classes)} => {classes}")

    # Per paper: 20% independent hold-out, plus 10-fold stratified CV on train
    indices = np.arange(len(dataset))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=dataset.y)
    train_data = torch.utils.data.Subset(dataset, train_idx)
    test_data  = torch.utils.data.Subset(dataset, test_idx)

    # Another split from train_data => for pretraining vs not? Actually we do pretraining on the entire train_data. Then we do fine-tuning on the same data. Or we can do partial.
    # We'll keep it simple: pretrain on entire train_data. Then fine-tune.

    # 2) Build model
    # if quick_test => chunk_len=200 or something smaller
    chunk_len = args.chunk_len
    if args.quick_test:
        chunk_len = min(256, chunk_len)
    model = AdvancedTransformerClassifier(input_dim=n_features,
                                          chunk_len=chunk_len,
                                          d_model=args.d_model,
                                          num_heads=args.num_heads,
                                          d_ff=args.d_ff,
                                          E=args.num_experts,
                                          dropout=args.dropout,
                                          n_layers=args.n_layers,
                                          n_classes=len(classes),
                                          max_passes=args.max_passes,
                                          act_threshold=args.act_threshold,
                                          quick_test=args.quick_test,
                                          logger=logger)
    logger("[INFO] Model created. Param count = "
           f"{sum(p.numel() for p in model.parameters())}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # 3) Pretraining
    if args.enable_pretraining:
        logger("[PRETRAIN] Starting masked pretraining stage ...")
        pt_dataset = torch.utils.data.Subset(dataset, train_idx)  # or entire dataset if you prefer
        pt_loader = DataLoader(pt_dataset, batch_size=args.batch_size, shuffle=True,
                               collate_fn=lambda b: masked_collate_fn(b, mask_prob=0.15, logger=logger))
        pt_opt = optim.Adam(model.parameters(), lr=args.pretrain_lr, weight_decay=args.weight_decay)
        # We'll do a small # epochs for quick test
        pretrain_epochs = args.pretrain_epochs if not args.quick_test else 2
        for ep in range(1, pretrain_epochs+1):
            model.train()
            epoch_loss = 0.0
            for i, (masked_x, orig_x, mask) in enumerate(pt_loader):
                masked_x = masked_x.to(device)
                orig_x   = orig_x.to(device)
                mask     = mask.to(device)
                pt_opt.zero_grad()
                recon, m = model.reconstruct_masked(masked_x, mask)
                # MSE on masked positions
                # We'll only compute on mask=1
                diff = recon - orig_x
                diff2 = diff * diff
                # mask out
                diff2 = diff2 * mask
                loss = diff2.sum() / (mask.sum() + 1e-7)
                loss.backward()
                pt_opt.step()
                epoch_loss += loss.item()
                if (i+1) % 10 == 0:
                    logger(f"[PRETRAIN] ep={ep} iter={i+1}/{len(pt_loader)} partial_loss={loss.item():.6f}")
            epoch_loss /= len(pt_loader)
            logger(f"[PRETRAIN] Ep {ep}/{pretrain_epochs}, avg_loss={epoch_loss:.6f}")
        logger("[PRETRAIN] Done. Saving pretrained weights as 'pretrain_model.pth' ...")
        torch.save(model.state_dict(), "pretrain_model.pth")

    # 4) Fine-tuning
    logger("[TRAIN] Starting fine-tune classification ...")
    # reload pretrained if we want
    if args.enable_pretraining and os.path.exists("pretrain_model.pth"):
        logger("[TRAIN] Loading pretrain_model.pth for fine-tuning ...")
        sd = torch.load("pretrain_model.pth", map_location=device)
        model.load_state_dict(sd, strict=True)

    ft_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,
                           collate_fn=classification_collate_fn)
    # 10-fold CV on train split (re-train per fold, using pretrained weights if available)
    from sklearn.model_selection import StratifiedKFold
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    X_train = dataset.X[train_idx]
    y_train = dataset.y[train_idx]
    cv_f1s = []
    for fold, (tr, va) in enumerate(skf.split(X_train, y_train), 1):
        tr_indices = train_idx[tr]
        va_indices = train_idx[va]
        tr_subset = torch.utils.data.Subset(dataset, tr_indices)
        va_subset = torch.utils.data.Subset(dataset, va_indices)
        tr_loader = DataLoader(tr_subset, batch_size=args.batch_size, shuffle=True, collate_fn=classification_collate_fn)
        va_loader = DataLoader(va_subset, batch_size=args.batch_size, shuffle=False, collate_fn=classification_collate_fn)

        # fresh model per fold
        model_cv = AdvancedTransformerClassifier(input_dim=n_features,
                                                 chunk_len=chunk_len,
                                                 d_model=args.d_model,
                                                 num_heads=args.num_heads,
                                                 d_ff=args.d_ff,
                                                 E=args.num_experts,
                                                 dropout=args.dropout,
                                                 n_layers=args.n_layers,
                                                 n_classes=len(classes),
                                                 max_passes=args.max_passes,
                                                 act_threshold=args.act_threshold,
                                                 quick_test=args.quick_test,
                                                 logger=logger).to(device)
        if args.enable_pretraining and os.path.exists("pretrain_model.pth"):
            try:
                model_cv.load_state_dict(torch.load("pretrain_model.pth", map_location=device), strict=False)
            except Exception as e:
                logger(f"[CV fold {fold}] Warning: could not load pretrained weights: {e}")
        opt_cv = optim.Adam(model_cv.parameters(), lr=args.finetune_lr, weight_decay=args.weight_decay)
        total_steps_cv = max(1, len(tr_loader) * max(1, (min(10, args.finetune_epochs) if not args.quick_test else 2)))
        warmup_steps_cv = int(0.1 * total_steps_cv)
        def lr_lambda_cv(step):
            if step < warmup_steps_cv:
                return float(step) / float(max(1, warmup_steps_cv))
            progress = float(step - warmup_steps_cv) / float(max(1, total_steps_cv - warmup_steps_cv))
            return 0.5 * (1.0 + np.cos(np.pi * progress))
        sched_cv = torch.optim.lr_scheduler.LambdaLR(opt_cv, lr_lambda=lr_lambda_cv)
        crit_cv = nn.CrossEntropyLoss()

        model_cv.train()
        step_ct = 0
        for ep in range(1, (min(10, args.finetune_epochs) if not args.quick_test else 2) + 1):
            for Xb, Yb in tr_loader:
                Xb = Xb.to(device)
                Yb = Yb.to(device)
                opt_cv.zero_grad()
                logits = model_cv(Xb)
                act_penalty = getattr(model_cv.act, 'last_expected_passes_tensor', None)
                if act_penalty is None:
                    loss_cv = crit_cv(logits, Yb)
                else:
                    loss_cv = crit_cv(logits, Yb) + 1e-3 * act_penalty
                loss_cv.backward()
                opt_cv.step()
                sched_cv.step()
                step_ct += 1

        # evaluate fold
        model_cv.eval()
        all_preds, all_trues = [], []
        with torch.no_grad():
            for Xb, Yb in va_loader:
                Xb = Xb.to(device)
                Yb = Yb.to(device)
                logits = model_cv(Xb)
                pred = torch.argmax(logits, dim=1)
                all_preds.append(pred.cpu().numpy())
                all_trues.append(Yb.cpu().numpy())
        all_preds = np.concatenate(all_preds) if len(all_preds) else np.array([])
        all_trues = np.concatenate(all_trues) if len(all_trues) else np.array([])
        if all_preds.size and all_trues.size:
            f1v = f1_score(all_trues, all_preds, average='macro')
            cv_f1s.append(f1v)
            logger(f"[CV fold {fold}] Macro-F1={f1v:.4f}")
    if cv_f1s:
        logger(f"[CV] 10-fold Macro-F1 mean={np.mean(cv_f1s):.4f} ± {np.std(cv_f1s):.4f}")
    val_loader = None
    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False,
                             collate_fn=classification_collate_fn)

    ft_opt = optim.Adam(model.parameters(), lr=args.finetune_lr, weight_decay=args.weight_decay)
    # Warmup + cosine scheduler
    total_steps = max(1, len(ft_loader) * max(1, (args.finetune_epochs if not args.quick_test else 3)))
    warmup_steps = int(0.1 * total_steps)
    def lr_lambda(step):
        if step < warmup_steps:
            return float(step) / float(max(1, warmup_steps))
        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return 0.5 * (1.0 + np.cos(np.pi * progress))
    scheduler = torch.optim.lr_scheduler.LambdaLR(ft_opt, lr_lambda=lr_lambda)
    criterion = nn.CrossEntropyLoss()

    max_epochs = args.finetune_epochs if not args.quick_test else 3
    best_f1 = 0.0
    for ep in range(1, max_epochs+1):
        model.train()
        epoch_loss = 0.0
        for i, (Xb, Yb) in enumerate(ft_loader):
            Xb = Xb.to(device)
            Yb = Yb.to(device)
            ft_opt.zero_grad()
            logits = model(Xb)
            # ACT regularization: expected passes term
            act_penalty = getattr(model.act, 'last_expected_passes_tensor', None)
            if act_penalty is None:
                loss = criterion(logits, Yb)
            else:
                loss = criterion(logits, Yb) + 1e-3 * act_penalty
            loss.backward()
            ft_opt.step()
            scheduler.step()
            epoch_loss += loss.item()
            if (i+1) % 10 == 0:
                logger(f"[TRAIN] ep={ep} iter={i+1}/{len(ft_loader)} partial_loss={loss.item():.6f}")

        epoch_loss /= len(ft_loader)
        # Evaluate quickly on train subset or small subset
        # We do a simple measure: macro-F1 on train
        model.eval()
        all_preds = []
        all_trues = []
        with torch.no_grad():
            for Xb, Yb in ft_loader:
                Xb = Xb.to(device)
                Yb = Yb.to(device)
                logit = model(Xb)
                pred = torch.argmax(logit, dim=1)
                all_preds.append(pred.cpu().numpy())
                all_trues.append(Yb.cpu().numpy())
        all_preds = np.concatenate(all_preds)
        all_trues = np.concatenate(all_trues)
        train_f1 = f1_score(all_trues, all_preds, average='macro')
        logger(f"[TRAIN] Ep {ep}/{max_epochs}, loss={epoch_loss:.6f}, train_F1={train_f1:.4f}")

        # If we had val_loader, we would compute val_f1; here we track train_f1
        if train_f1 > best_f1:
            best_f1 = train_f1
            # save
            torch.save(model.state_dict(), "best_model.pth")
            logger(f"[TRAIN] Ep {ep} => new best F1={train_f1:.4f}, saved model.")
        if (train_f1 > 0.99 and not args.quick_test):
            # assume we've basically converged
            logger(f"[TRAIN] Early stopping, train_f1 > 0.99.")
            break

    logger("[TRAIN] Done fine-tuning. Loading best_model.pth for final test evaluation ...")
    best_sd = torch.load("best_model.pth", map_location=device)
    model.load_state_dict(best_sd)

    # Evaluate on test
    logger("[TEST] Evaluating on hold-out test set ...")
    model.eval()
    all_preds = []
    all_trues = []
    all_probs = []
    with torch.no_grad():
        for Xb, Yb in test_loader:
            Xb = Xb.to(device)
            Yb = Yb.to(device)
            logit = model(Xb)
            prob = torch.softmax(logit, dim=1)
            pred = torch.argmax(logit, dim=1)
            all_preds.append(pred.cpu().numpy())
            all_probs.append(prob.cpu().numpy())
            all_trues.append(Yb.cpu().numpy())

    all_preds = np.concatenate(all_preds)
    all_probs = np.concatenate(all_probs)
    all_trues = np.concatenate(all_trues)
    test_f1 = f1_score(all_trues, all_preds, average='macro')

    # confusion
    cm = confusion_matrix(all_trues, all_preds)
    # macro auc
    # if #classes=3, we do a one-vs-rest approach
    try:
        # one-hot
        from sklearn.preprocessing import label_binarize
        Yb_oh = label_binarize(all_trues, classes=range(len(classes)))
        auc_ovr = roc_auc_score(Yb_oh, all_probs, average='macro', multi_class='ovr')
    except:
        auc_ovr = float('nan')
    logger("[TEST] Confusion Matrix:\n" + str(cm))
    acc = (all_preds==all_trues).mean()*100.0
    logger(f"[TEST] Accuracy={acc:.2f}%, Macro-F1={test_f1:.4f}, Macro-AUC={auc_ovr:.4f}")

    # Permutation testing: 100 shuffles of labels on test set
    logger("[TEST] Permutation testing (100 shuffles of TRAIN labels) ...")
    from sklearn.utils import shuffle as skshuffle
    perm_acc = []
    for _ in range(100):
        # shuffle training labels
        y_perm = skshuffle(dataset.y[train_idx], random_state=None)
        perm_subset = torch.utils.data.TensorDataset(dataset.X[train_idx], torch.from_numpy(y_perm))
        perm_loader = DataLoader(perm_subset, batch_size=args.batch_size, shuffle=True)
        # fresh small model for speed
        model_perm = AdvancedTransformerClassifier(input_dim=n_features,
                                                   chunk_len=chunk_len,
                                                   d_model=args.d_model,
                                                   num_heads=args.num_heads,
                                                   d_ff=args.d_ff,
                                                   E=args.num_experts,
                                                   dropout=args.dropout,
                                                   n_layers=max(2, args.n_layers//2),
                                                   n_classes=len(classes),
                                                   max_passes=args.max_passes,
                                                   act_threshold=args.act_threshold,
                                                   quick_test=True,
                                                   logger=lambda *a, **k: None).to(device)
        opt_perm = optim.Adam(model_perm.parameters(), lr=args.finetune_lr, weight_decay=args.weight_decay)
        crit_perm = nn.CrossEntropyLoss()
        model_perm.train()
        for ep in range(2):
            for Xb, Yb in perm_loader:
                Xb = Xb.to(device)
                Yb = Yb.to(device)
                opt_perm.zero_grad()
                logits = model_perm(Xb)
                loss_p = crit_perm(logits, Yb)
                loss_p.backward()
                opt_perm.step()
        # evaluate on same test set
        model_perm.eval()
        preds_perm = []
        with torch.no_grad():
            for Xb, Yb in test_loader:
                Xb = Xb.to(device)
                logit = model_perm(Xb)
                preds_perm.append(torch.argmax(logit, dim=1).cpu().numpy())
        preds_perm = np.concatenate(preds_perm)
        perm_acc.append((preds_perm == all_trues).mean() * 100.0)
    logger(f"[TEST] Permutation test mean accuracy (chance level estimate): {np.mean(perm_acc):.2f}% ± {np.std(perm_acc):.2f}")

    # We are done
    return

###############################################################################
# Main
###############################################################################
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", type=str, required=True, help="Path to the CSV with shape [samples x features], last col=Condition.")
    parser.add_argument("--quick_test", type=int, default=0, help="1=small ephemeral run for local debugging, 0=full run.")
    parser.add_argument("--enable_pretraining", action='store_true', help="Whether to do masked pretraining first.")
    parser.add_argument("--pretrain_epochs", type=int, default=30, help="Number of epochs for masked pretraining.")
    parser.add_argument("--finetune_epochs", type=int, default=50, help="Number of epochs for classification.")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--pretrain_lr", type=float, default=1e-4)
    parser.add_argument("--finetune_lr", type=float, default=5e-5)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--chunk_len", type=int, default=320, help="Number of features per chunk.")
    parser.add_argument("--d_model", type=int, default=64)
    parser.add_argument("--num_heads", type=int, default=4)
    parser.add_argument("--d_ff", type=int, default=256)
    parser.add_argument("--num_experts", type=int, default=4, help="Number of MoE experts")
    parser.add_argument("--n_layers", type=int, default=6)
    parser.add_argument("--max_passes", type=int, default=3, help="ACT max passes.")
    parser.add_argument("--act_threshold", type=float, default=0.99)
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout rate for transformer blocks and classifier.")

    args = parser.parse_args()
    train_transformer(args)

if __name__ == "__main__":
    main()


# ======================
# File: src/original/steps/450k/extract_quarter.py
# ======================

import os
import re

# File paths
input_path = os.path.join('/Users/verisimilitude/Downloads', 'output.txt')
output_path = os.path.join('/Users/verisimilitude/Downloads', 'output_small.txt')

print(f"Reading file from: {input_path}")

try:
    # Read the entire file
    with open(input_path, 'r', encoding='utf-8') as file:
        data = file.read()
    
    # Count occurrences - using regex with word boundaries to ensure we're counting whole words
    me_count = len(re.findall(r'\bME\b', data))
    lc_count = len(re.findall(r'\bLC\b', data))
    controls_count = len(re.findall(r'\bcontrols\b', data))
    
    # Print occurrence counts
    print(f"Occurrences of 'ME': {me_count}")
    print(f"Occurrences of 'LC': {lc_count}")
    print(f"Occurrences of 'controls': {controls_count}")
    
    # Split into rows
    rows = data.split('\n')
    total_rows = len(rows)
    print(f"Total rows: {total_rows}")
    
    # Calculate 25% of rows
    rows_to_keep = int(total_rows * 0.25)
    if rows_to_keep < 1:
        rows_to_keep = 1  # Keep at least one row
    print(f"Keeping first {rows_to_keep} rows (25%)")
    
    # Extract first quarter of rows
    first_quarter = rows[:rows_to_keep]
    
    # Join back into a string
    new_content = '\n'.join(first_quarter)
    
    # Count occurrences in the reduced file as well
    me_count_reduced = len(re.findall(r'\bME\b', new_content))
    lc_count_reduced = len(re.findall(r'\bLC\b', new_content))
    controls_count_reduced = len(re.findall(r'\bcontrols\b', new_content))
    
    print(f"In reduced file:")
    print(f"Occurrences of 'ME': {me_count_reduced}")
    print(f"Occurrences of 'LC': {lc_count_reduced}")
    print(f"Occurrences of 'controls': {controls_count_reduced}")
    
    # Write to new file
    with open(output_path, 'w', encoding='utf-8') as output_file:
        output_file.write(new_content)
    
    print(f"Successfully wrote {rows_to_keep} rows to: {output_path}")
    print(f"Original size: {len(data)} characters")
    print(f"New file size: {len(new_content)} characters")
    print(f"Size reduction: {((len(data) - len(new_content)) / len(data) * 100):.2f}%")
    
except Exception as e:
    print(f"Error: {str(e)}")

# ======================
# File: src/original/steps/450k/view.py
# ======================

import pandas as pd
import os
import sys
import time
from tqdm import tqdm

# File paths
big_csv = "/Volumes/T9/EpiMECoV/processed_data/filtered_biomarker_matrix.csv"
out_csv = "/Users/verisimilitude/Downloads/output_small.csv"

def get_file_size(file_path):
    """Get file size in MB"""
    return os.path.getsize(file_path) / (1024 * 1024)

class LineCounter:
    """Count lines in a file with progress updates"""
    def __init__(self, filename):
        self.filename = filename
        self.file_size = get_file_size(filename)
        
    def count_lines(self):
        """Count lines in a file with progress updates"""
        print(f"Estimating total lines in file ({self.file_size:.2f} MB)...")
        line_count = 0
        
        # Create a progress bar for line counting
        with tqdm(total=100, desc="Scanning file", unit="%") as pbar:
            with open(self.filename, 'rb') as f:
                # Use binary mode for better performance
                last_percent = 0
                
                while True:
                    # Read 1MB at a time
                    buffer = f.read(1024 * 1024)
                    if not buffer:
                        break
                    
                    # Count newlines in this buffer
                    line_count += buffer.count(b'\n')
                    
                    # Update progress
                    current_pos = f.tell() / (1024 * 1024)
                    percent_complete = min(99, int(current_pos / self.file_size * 100))
                    
                    if percent_complete > last_percent:
                        pbar.update(percent_complete - last_percent)
                        last_percent = percent_complete
                        
        # Add 1 for the last line if file doesn't end with newline
        return line_count + 1

class CSVProcessor:
    def __init__(self, input_file, output_file, small_chunksize=10000):
        self.input_file = input_file
        self.output_file = output_file
        self.file_size = get_file_size(input_file)
        self.chunksize = small_chunksize  # Smaller chunks for more frequent updates
        
    def process_with_live_updates(self):
        """Process CSV with live updates to show progress"""
        print(f"File size: {self.file_size:.2f} MB")
        
        # Get an estimate of total lines for better progress tracking
        counter = LineCounter(self.input_file)
        estimated_lines = counter.count_lines()
        print(f"Estimated total lines: {estimated_lines:,}")
        
        # Track header separately
        header = None
        
        # Track if we've processed at least the first chunk
        first_chunk_processed = False
        
        # Process in chunks with progress updates
        chunks_processed = 0
        rows_processed = 0
        start_time = time.time()
        
        print("\nReading CSV data with progress monitoring...")
        with tqdm(total=estimated_lines, desc="Reading rows", unit="rows") as pbar:
            # Initial progress message
            print("Starting to read the first chunk (this may take a while for large files)...")
            
            # Read the header first (just the first line)
            try:
                header_df = pd.read_csv(self.input_file, nrows=0)
                header = header_df.columns.tolist()
                print(f"Successfully read header with {len(header)} columns")
                
                # Update progress for header
                pbar.update(1)
                rows_processed += 1
            except Exception as e:
                print(f"Error reading header: {e}")
                return None
            
            # Process rest of file in chunks
            try:
                # Initialize empty list to store processed chunks for small subset
                first_chunks = []
                
                # Create iterator for chunks
                chunk_iterator = pd.read_csv(self.input_file, chunksize=self.chunksize, 
                                           skiprows=1, header=None)
                
                # Process chunks
                for i, chunk in enumerate(chunk_iterator):
                    chunks_processed += 1
                    
                    # Assign header
                    chunk.columns = header
                    
                    # Keep track of the first chunks for small subset
                    if len(first_chunks) < 1 and rows_processed < 5:
                        first_chunks.append(chunk.head(5 - rows_processed))
                    
                    # Update progress
                    chunk_rows = len(chunk)
                    rows_processed += chunk_rows
                    pbar.update(chunk_rows)
                    
                    # Print live status every few chunks
                    if chunks_processed % 5 == 0:
                        elapsed = time.time() - start_time
                        rate = rows_processed / elapsed if elapsed > 0 else 0
                        print(f"Status: Processed {chunks_processed} chunks, {rows_processed:,} rows "
                              f"({rate:.2f} rows/sec), elapsed time: {elapsed:.1f} sec")
                    
                    # Mark first chunk processed
                    if not first_chunk_processed:
                        print(f"First chunk successfully processed! Processing continues...")
                        first_chunk_processed = True
                
                # Create final small dataframe
                small_df = pd.concat(first_chunks) if first_chunks else None
                
                return header, small_df, rows_processed
                
            except Exception as e:
                print(f"\nError during chunk processing: {e}")
                
                # If we processed at least the first chunk, we might be able to continue
                if first_chunk_processed and first_chunks:
                    print("Attempting to create output with partial data...")
                    small_df = pd.concat(first_chunks)
                    return header, small_df, rows_processed
                else:
                    print("Failed to process even the first chunk.")
                    return None
    
    def create_small_subset(self, header, small_df):
        """Create a small subset of the data"""
        if header is None or small_df is None or small_df.empty:
            print("Not enough data processed to create a subset.")
            return
        
        try:
            # Identify the last column name (which should be "Condition")
            condition_col = header[-1]
            
            # Let's pick the first 10 feature columns, plus the Condition column
            feature_cols = header[:10]  # first 10 columns
            subset_cols = feature_cols + [condition_col]
            
            # Keep only the subset of columns
            df_small = small_df[subset_cols].head(5)
            
            # Write this small subset to a new file
            print(f"Writing subset to {self.output_file}...")
            df_small.to_csv(self.output_file, index=False)
            print(f"Created a small CSV with shape {df_small.shape} at {self.output_file}")
            
        except Exception as e:
            print(f"Error creating subset: {e}")

# Main execution
try:
    print(f"Starting CSV processing with incremental progress feedback...")
    
    # Create processor
    processor = CSVProcessor(big_csv, out_csv, small_chunksize=5000)
    
    # Process file with live updates
    result = processor.process_with_live_updates()
    
    if result:
        header, small_df, rows_processed = result
        processor.create_small_subset(header, small_df)
        print(f"Processing completed successfully. Total rows processed: {rows_processed:,}")
    else:
        print("Processing failed.")
    
except KeyboardInterrupt:
    print("\nOperation cancelled by user.")
except Exception as e:
    print(f"Unexpected error: {e}")

# ======================
# File: src/original/create_controls/filter.py
# ======================

import re

file_in = "/Users/verisimilitude/Downloads/GSE42861_series_matrix.txt"
file_out = "/Volumes/T9/EpiMECoV/control_gsms.txt"

with open(file_in,"r") as fin:
    lines = fin.readlines()

control_gsms = []
for i, line in enumerate(lines):
    if "Normal genomic DNA" in line:
        gsm_matches = re.findall(r"GSM\d+", line)
        if not gsm_matches:
            next_line = lines[i+1] if i+1<len(lines) else ""
            gsm_matches = re.findall(r"GSM\d+", next_line)
        control_gsms.extend(gsm_matches)

with open(file_out,"w") as fout:
    fout.write("\n".join(control_gsms))

print("Control GSM IDs =>", len(control_gsms))

# ======================
# File: src/original/create_controls/delete_patients.py
# ======================

import os

raw_data_dir = "/Volumes/T9/EpiMECoV/data/controls/GSE42861/GSE42861_RAW"
gsm_file = "/Volumes/T9/EpiMECoV/src/create_controls/control_gsms.txt"

with open(gsm_file, "r") as f:
    control_gsms = set(line.strip() for line in f if line.strip())

deleted_files_count = 0
retained_files_count = 0
unmatched_gsm_ids = []

for file_name in os.listdir(raw_data_dir):
    file_path = os.path.join(raw_data_dir, file_name)
    if file_name.startswith(".") or os.path.isdir(file_path):
        continue
    gsm_id = file_name.split("_")[0].strip()
    if gsm_id not in control_gsms:
        unmatched_gsm_ids.append(gsm_id)
        try:
            os.remove(file_path)
            deleted_files_count += 1
            print(f"Deleted: {file_path}")
        except Exception as e:
            print(f"Error deleting {file_path}: {e}")
    else:
        retained_files_count += 1

print(f"\nCleanup done. Deleted={deleted_files_count}, Retained={retained_files_count}.")
if unmatched_gsm_ids:
    print("Sample of unmatched GSM IDs:", unmatched_gsm_ids[:10])